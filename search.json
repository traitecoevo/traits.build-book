[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "",
    "text": "1 About"
  },
  {
    "objectID": "index.html#brief-overview",
    "href": "index.html#brief-overview",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "1.1 Brief overview",
    "text": "1.1 Brief overview\nImagine you wanted to build a database of traits. You might start by compiling data from existing datasets, but you’d quickly find that there are many different ways to name and measure the same trait, that different studies use different units, or use an outdated name for a species or taxon.\ntraits.build is a data standard, R package, and workflow that is desgined to help you build a harmonised, relational database from disparate datasets. The package was first developed to create austraits.build an, open-source database of Australian plant traits. The code has been transformed into a standalone package allowing anyone to build a relational, tabular database for any taxonomic group and any collection of traits."
  },
  {
    "objectID": "index.html#about-this-manual",
    "href": "index.html#about-this-manual",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "1.2 About this manual",
    "text": "1.2 About this manual\nThis manual is a step-by-step user guide to the traits.build standard, R package and workflow manual. Alongside this manual, you may find the following resources useful:\n\ninstallation instructions for the traits.build package\na reference page with all user-side functions for the traits.build package.\nlinks to example projects"
  },
  {
    "objectID": "index.html#a-simple-example",
    "href": "index.html#a-simple-example",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "1.3 A simple example",
    "text": "1.3 A simple example\nTo get you started, we’ve provided a template example compilation which you can clone and modify. This provides the basic steps for building a traits.build compilation. Follow the instructions in Tutorials chapter for a step-by-step guide to building a database from scratch."
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "1.4 Getting help",
    "text": "1.4 Getting help\nAlong the way, users may encounter a number of errors and warnings. Some of these are designed to help you build a robust database, others may constitue an error in your data, file structure, or the package. If you encounter an error or warning, please read the message carefully and follow the instructions for getting help."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "1.5 Contributing",
    "text": "1.5 Contributing\nXXX"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "The {traits.build} data standard, R package, and workflow",
    "section": "1.6 Acknowledgements",
    "text": "1.6 Acknowledgements\nThis manual arose from code within the AusTraits project at https://github.com/traitecoevo/austraits.build. The AusTraits project received investment (https://doi.org/10.47486/DP720) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS)."
  },
  {
    "objectID": "motivation.html#the-problem",
    "href": "motivation.html#the-problem",
    "title": "2  Motivation",
    "section": "2.1 The problem",
    "text": "2.1 The problem\nEcological data are often collected in disparate formats, making it difficult to combine datasets for analysis. This is particularly true for trait data, which are often collected in a variety of formats and are rarely stored in a relational database. This makes it difficult to combine trait data with other ecological data, such as abundance or biomass, for analysis.\nMoreover, trait data are often collected in fragments, corrpesonding to scientific papers. Different traits may be collected for different species, or different traits are collected for the same species at different times or at different locations.\nTo create a large harmonised dataset, we need to combine these different datasets into a unfied whole, with common names, units, values for categorical traits, and so on. This is a time-consuming process.\n\nWe are not the first group to tackle this problem. Our field (plant ecology) has many trait datasets, most compiled from diverse sources. Most start small, with a few soruces that can be handled within a spreadhseet. But as thery grow, they ecnouter issues. The largest is TRY, with &gt; 15 million records from over 300,000 plant taxa. However, each group has had to tackle the compilation challenge anew, as there are no common pathways for creating a harmonised dataset."
  },
  {
    "objectID": "motivation.html#the-solution",
    "href": "motivation.html#the-solution",
    "title": "2  Motivation",
    "section": "2.2 The solution",
    "text": "2.2 The solution\nThe traits.build data standard, R package, and workflow offer a solution to this problem, with a set of open-source tools that enable users to create open-source, harmonised, reproducible databases from disparate datasets, underpinned by a sophisticated ontology able to handle the complexities inherent to ecological data.\nWe developed these tools for the to create AusTraits, an, open-source database of Australian plant traits. We figured others may want to replicate these efforts, so we the code and workflow was transformed into a standalone package allowing anyone to build a trait database for their own region or taxa.\nAlong the way, we hdeveloped a suite of tools:\n\ntraits.build data standard: a relational database structure that fully documents the contextual data essential to interpreting ecological data.\ntraits.build R package: a set of functions that enable users to create a harmonised database from disparate datasets.\nAPD: The AusTraits Plant Dictionary, with detailed descriptions for more than 500 plant trait concepts.\n\nAPCalign: an R package to align and update Australian plant taxon name strings with the Australian Plant Census.\n\nThe tools are open-source, so that users can apply them to suit their needs and without cost."
  },
  {
    "objectID": "workflow.html#core-principles",
    "href": "workflow.html#core-principles",
    "title": "3  Workflow",
    "section": "3.1 Core principles",
    "text": "3.1 Core principles\nThe project’s guiding principles are to:\n\nWherever possible solve problems in a general way, that enables others to leverage our efforts to solve their own problems.\nEnable users to create open-source, harmonised, reproducible databases from disparate datasets.\nProvide a fully transparent workflow, where all decisions on how to handle the data are exposed and can be.\nOffer a relational database structure that fully documents the contextual data essential to interpreting ecological data.\nOffer a straightforward, robust template for building a trait dictionary.\nOffer a database structure that is flexible enough to accommodate the complexities inherent to ecological data.\nOffer a database structure that is underlain by a documented ontology, ensuring each database field is interpretable and interoperable with other databases and data structures.\nHave no dependencies on proprietary software or costs to setup and maintain (beyond person time)."
  },
  {
    "objectID": "workflow.html#approach",
    "href": "workflow.html#approach",
    "title": "3  Workflow",
    "section": "3.2 Approach",
    "text": "3.2 Approach\ntraits.build can be viewed through two lenses, its output structure and its underlying conceptual framework.\n\nOutput structure\ntraits.build is a relational database. There is a core traits table, supported by ancillary tables that document location properties (include latitude & longitude), context properties, dataset and measurement methods, taxon concepts, contributor details, and sources. All tables are stored in long format, such that there is a single column that includes all trait names (or location properties, or context properties) and column for trait values.\nA series of identifiers, including both textual fields (i.e. taxon_name, trait_name, dataset_id) and numeric identifiers link the ancillary tables to the traits table. The many numeric identifiers are overwhelming until you consider the conceptual framework, or mindmap, that underpins traits.build. They include: observation_id, population_id, individual_id, temporal_id, location_id, entity_context_id, plot_id, and source_id.\nStoring the resource as a relational table greatly reduces file sizes and facilitates searching for a particular metadata field\n\n\nConceptual framework\ntraits.build’s database structure effectively captures the complexities inherent to ecological data. Each dataset is unique, with measurements recorded on different entities (individuals, populations, species), in specific locations, and under countless environmental and experimental conditions, the so-called context of a measurement. The context offers essential meaning to each trait value. An ecological database must effectively capture all relevant contexts or the accompanying trait measurements lose much of their value.\ntraits.build is designed around the concept of an observation, a collection of measurements of different traits made at a specific point in time on a specific entity. OBOE, the Extensible Observation Ontology, first developed this conceptual framework, explicitly for complex ecological datasets. An observation_id links the measurements made on different traits that comprise a single observation. Measurements made at different points in time, on different entities, at different locations, or under different contextual conditions require unique observation_id values. The additional identifiers in the traits table (and the ancillary tables) were developed to ensure that unique observation_ids were generated for distinct observations, per this framework.\nWithin a dataset, observation_id’s are generated for unique combinations of the fields: taxon_name, population_id, individual_id, temporal_id, entity_type, and source_id. For instance, if measurements are made on the same individual during the wet versus dry season, the two observations will share an individual_id but have distinct temporal_id values, and therefore have distinct observation_id’s. See traits.build schema for definitions of identifiers"
  },
  {
    "objectID": "workflow.html#concepts",
    "href": "workflow.html#concepts",
    "title": "3  Workflow",
    "section": "3.3 Concepts",
    "text": "3.3 Concepts\nOur workflow is structured with the following concepts.\n\nData sources\nThe data in a traits.build compilation is derived from distinct sources, each contributed by an individual researcher, government entity (e.g. herbaria), or NGO. Each reflects the research agenda of the individual/organisation who contributed the data - the species selected, traits measured, manipulative treatments performed, and locations sampled encompass the diversity of research interests present in Australia throughout past decades. These datasets use different variable trait names, units and methods and have different data structures.\n\n\nStandardising and harmonising data\nTo create a single database for distribution to the research community, we developed a reproducible and transparent workflow in R for merging each dataset into AusTraits. The pipeline ensures the following information is standardised across all datasets in AusTraits. A metadata file for each study documents how the data tables submitted by an individual contributor are translated into the standardised terms used in the AusTraits database.\n\ntaxonomic nomenclature follows the Australian Plant Census (APC), with a pipeline to update outdated taxonomy, correct minor spelling mistakes, and align with a known genus when a full species names isn’t provided.\n\ntrait names are defined in our traits.yml file and only data for traits included in this file can be merged into AusTraits. The trait names used in the incoming dataset are mapped onto the appropriate AusTraits trait name.\nFor numeric traits the traits.yml file includes units and the allowable range of values. All incoming data are converted to the appropriate units and data outside the range of allowable values are removed from the main AusTraits data table.\nFor categorical traits the traits.yml file includes a list of allowable values, allowed terms for the trait. Each categorical trait value is defined in the traits.yml file. Lists of substitutions translate the exact syntax and terms in a submitted dataset into the values allowed by AusTraits. This ensures that for a certain trait the same value has an identical meaning throughout the AusTraits database.\nSite locations are recorded in decimal degrees.\n\n\n\nReferencing sources and recording methods\nThe metadata file also includes all metadata associated with the study:\n\nThe source information for each dataset is recorded. Most frequently, these are the primary publications derived from the dataset.\nPeople associated with the collection of the data are listed, including their role in the project.\nCollection methods are included.\nFields capture value type (mean, min, max, mode, range, bin) and associated replicate numbers, basis of value (measurement, expert_score, model_derived), entity type (species, population, individual), life stage (adult, juvenile,sapling, seedling), basis of record (field, field_experiment, preserved_specimen, captive_cultivated, lab, literature), and any additional measurement remarks.\nAvailable data on location properties are recorded.\nAvailable data on plot and treatment contextual properties are recorded.\nA context field, temporal_context_id, indicates if repeat measures were made on the same individual over time.\nA context field, method_context_id, indicates if the same trait was measured using multiple methods.\nCollection date is recorded.\n\n\n\nError checking\nWe consider deatiled error checking to be an inmporant and ongoing part of our workflow. The following steps are taken to ensure data quality.\n\nThe data curator can rus a series of tests on each data set, detailed in the adding data vignette\nThese tests identify misaligned units, unrecognised taxon names, and unsupported categorical trait values\nThese tests also identify and eliminate most duplicate data - instances where the same numeric trait data is submitted by multiple people\nEach dataset is then compiled into a report which summarises metadata and plots/charts trait values in comparison to other measurements of that trait in AusTraits. The report is reviewed by the data contributor to ensure metadata is complete and data values are as expected.\nA second member of the AusTraits team double checks each dataset before it is merged into the main repository."
  },
  {
    "objectID": "usage_examples.html#austraits",
    "href": "usage_examples.html#austraits",
    "title": "4  Usage examples",
    "section": "4.1 AusTraits",
    "text": "4.1 AusTraits\nThe workflow described here evolved out of the AusTraits project. AusTraits is an open-source, harmonized database of Australian plant trait data. It synthesises data on nearly 500 traits across more than 30,000 taxa from over 300 sources. Begun in 2016 as an initiative between three lab groups, it has grown to be the largest collation of plant trait data for Australian plants.\nThe traits.build workflow is used to build the AusTraits database. The workflow XXXX"
  },
  {
    "objectID": "usage_examples.html#other-examples",
    "href": "usage_examples.html#other-examples",
    "title": "4  Usage examples",
    "section": "4.2 Other examples",
    "text": "4.2 Other examples\nThere are now multiple new porjects using the traits.build workflow and package. At this stage, most are in private repositories, but we envisage that they will be made public in the near future.\n\nAusTraits: A compilation of traits data for Australian plants\nAusInverTraits: A compilation of traits data for invertebrates, led by Inverterbates Australia\nAusFizz: A compilation of physiological response curves for plants"
  },
  {
    "objectID": "long_wide.html#background",
    "href": "long_wide.html#background",
    "title": "5  Long vs Wide data",
    "section": "5.1 Background",
    "text": "5.1 Background\n\nEase of use (manipulation & analysis)\nSize of data product: storage costs, speed of loading, transfer\n\nConcepts\n\nLong data\nWide data\nTidy data"
  },
  {
    "objectID": "long_wide.html#our-approach",
    "href": "long_wide.html#our-approach",
    "title": "5  Long vs Wide data",
    "section": "5.2 Our approach",
    "text": "5.2 Our approach\n\nMultiple products available\n\n\nPivotting between long and wide"
  },
  {
    "objectID": "database_standard.html",
    "href": "database_standard.html",
    "title": "6  Data standard",
    "section": "",
    "text": "Overview\nA single universally accepted trait database standard does not currently exist among plant ecologists, limiting our ability to merge together distinct databases.\nA broadly accepted standard must achieve two goals:\n\nBe able to fully document the metadata and, in particular, the contextual information, essential to interpreting ecological data.\nBe fully described in an ontology (or through figures) that clearly capture the meaning of each database field (column) and the relationships between database fields.\n\nThe AusTraits Project aims to offer a trait database standard that achieves both these goals.\nOur database standard has not been invented from scratch. We have built upon ideas and standards described in previous publications and ontologies. In particular:\n\nOBOE, the Extensible Observation Ontology (Madin, Joshua, et al. (2007) “An ontology for describing and synthesizing ecological observation data.” Ecological informatics) doi: 10.1016/j.ecoinf.2007.05.004\nETS, the Ecological Trait-data Standard (Schneider, Florian D., et al. (2019) “Towards an ecological trait‐data standard.” Methods in Ecology and Evolution.) doi: 10.1111/2041-210X.13288\nDarwinCore dwc.tdwg.org\n\n\n\nDatabase that fully documents ecological metadata\nTo date, AusTraits, Australia’s Plant Trait Database, has incorporated more than 370 unique datasets, spanning the breadth of plant trait data collected across Australia. It has been more than 6 months (and ~50 datasets) since we have identified a class of metadata or contextual information that could not be mapped into the traits.build structure.\nThe database structure can capture:\n\nlocation information, including coordinates and location properties\nthe entity associated with each trait measurement (e.g. species, population, individual)\ntemporal contexts, reflecting repeat measurements of an entity across time\ntreatment contexts, indicating distinct treatments applied to different entities\nentity contexts, documenting defining features of individual entities (e.g. sex or age of an individual)\nmethod contexts, documenting slight variations in sampling protocols\nresponse curve measurements are grouped into a single observation and assigned sequential identifiers\n\nThe database standard will continue to evolve as traits.build is used to build a greater diversity of trait databases, but, already offers a flexible, detailed structure for capturing and harmonising ecological data.\nSee relationships between database variables for more details.\n\n\nDatabase standard described by an ontology\nA database standard is only useful if:\n\nEach database field has a clear and consistent meaning\nThe relationships between database fields are explicit and meaningful\nThis information can readily be interpreted by both trait database custodians and database users\n\nBuilding both a formal ontology and visualisations of the ontology has been essential toward reaching these goals. To quote a seminal paper, “Ontological analysis clarifies the structure of knowledge” (Chandrasekaran et al., 1999).\nIndeed, the construction of knowledge graphs and ontological analysis has undercovered locations where a lack of semantic clarity in database structure reduced the interpretation of the output data. This iterative process leads to the continual refinement of the database structure and mapped links across database fields.\nOur ontology remains a work-in-progress, but an outdated version is available at traits.build ontology."
  },
  {
    "objectID": "database_structure.html",
    "href": "database_structure.html",
    "title": "7  Data structure",
    "section": "",
    "text": "8 Components\nThe core components are defined as follows."
  },
  {
    "objectID": "database_structure.html#traits",
    "href": "database_structure.html#traits",
    "title": "7  Data structure",
    "section": "8.1 traits",
    "text": "8.1 traits\nDescription: A table containing measurements of traits.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\ntaxon_name\n\n\nScientific name of the taxon on which traits were sampled, without authorship. When possible, this is the currently accepted (botanical) or valid (zoological) scientific name, but might also be a higher taxonomic level.\n\n\n\n\nobservation_id\n\n\nA unique integral identifier for the observation, where an observation is all measurements made on an individual at a single point in time. It is important for joining traits coming from the same observation_id. Within each dataset, observation_id’s are unique combinations of taxon_name, population_id, individual_id, and temporal_context_id.\n\n\n\n\ntrait_name\n\n\nName of the trait sampled. Allowable values specified in the table definitions.\n\n\n\n\nvalue\n\n\nThe measured value of a trait, location property or context property.\n\n\n\n\nunit\n\n\nUnits of the sampled trait value after aligning with AusTraits standards.\n\n\n\n\nentity_type\n\n\nA categorical variable specifying the entity corresponding to the trait values recorded.\n\n\n\n\nvalue_type\n\n\nA categorical variable describing the statistical nature of the trait value recorded.\n\n\n\n\nbasis_of_value\n\n\nA categorical variable describing how the trait value was obtained.\n\n\n\n\nreplicates\n\n\nNumber of replicate measurements that comprise a recorded trait measurement. A numeric value (or range) is ideal and appropriate if the value type is a mean, median, min or max. For these value types, if replication is unknown the entry should be unknown. If the value type is raw_value the replicate value should be 1. If the trait is categorical or the value indicates a measurement for an entire species (or other taxon) replicate value should be .na.\n\n\n\n\nbasis_of_record\n\n\nA categorical variable specifying from which kind of specimen traits were recorded.\n\n\n\n\nlife_stage\n\n\nA field to indicate the life stage or age class of the entity measured. Standard values are adult, sapling, seedling and juvenile.\n\n\n\n\npopulation_id\n\n\nA unique integer identifier for a population, where a population is defined as individuals growing in the same location (location_id /location_name) and plot (plot_context_id, a context category) and being subjected to the same treatment (treatment_context_id, a context category).\n\n\n\n\nindividual_id\n\n\nA unique integer identifier for an individual, with individuals numbered sequentially within each dataset by taxon by population grouping. Most often each row of data represents an individual, but in some datasets trait data collected on a single individual is presented across multiple rows of data, such as if the same trait is measured using different methods or the same individual is measured repeatedly across time.\n\n\n\n\nrepeat_measurements_id\n\n\nA unique integer identifier for repeat measurements of a trait that comprise a single observation, such as a response curve.\n\n\n\n\ntemporal_context_id\n\n\nA unique integer identifier assigned where repeat observations are made on the same individual (or population, or taxon) across time. The identifier links to specific information in the context table.\n\n\n\n\nsource_id\n\n\nFor datasets that are compilations, an identifier for the original data source.\n\n\n\n\nlocation_id\n\n\nA unique integer identifier for a location, with locations numbered sequentially within a dataset. The identifier links to specific information in the location table.\n\n\n\n\nentity_context_id\n\n\nA unique integer identifier indicating specific contextual properties of an individual, possibly including the individual’s sex or caste (for social insects).\n\n\n\n\nplot_context_id\n\n\nA unique integer identifier for a plot, where a plot is a distinct collection of organisms within a single geographic location, such as plants growing on different aspects or blocks in an experiment. The identifier links to specific information in the context table.\n\n\n\n\ntreatment_context_id\n\n\nA unique integer identifier for a treatment, where a treatment is any experimental manipulation to an organism’s growing/living conditions. The identifier links to specific information in the context table.\n\n\n\n\ncollection_date\n\n\nDate sample was taken, in the format yyyy-mm-dd, yyyy-mm or yyyy, depending on the resoluton specified. Alternatively an overall range for the study can be indicating, with the starting and ending sample date sepatated by a /, as in 2010-10/2011-03\n\n\n\n\nmeasurement_remarks\n\n\nBrief comments or notes accompanying the trait measurement.\n\n\n\n\nmethod_id\n\n\nA unique integer identifier to distinguish between multiple sets of methods used to measure a single trait within the same dataset. The identifier links to specific information in the methods table.\n\n\n\n\nmethod_context_id\n\n\nA unique integer identifier indicating a trait is measured multiple times on the same entity, with different methods used for each entry. This field is only used if a single trait is measured using multiple methods within the same dataset. The identifier links to specific information in the context table.\n\n\n\n\noriginal_name\n\n\nName given to taxon in the original data supplied by the authors.\n\n\n\n\n\n\nEntity type\nAn entity is the feature of interest, indicating what a trait value applies to. While an entity can be just a component of an organism, within the scope of AusTraits, an individual is the finest scale entity that can be documented. The same study might measure some traits at a population-level (entity = population) and others at an individual-level (entity = individual).\nIn detail:\n\nentity_type is a categorical variable specifying the entity corresponding to the trait values recorded. Possible values are:\n\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\nindividual\n\n\nValue comes from a single individual.\n\n\n\n\npopulation\n\n\nValue represents a summary statistic from multiple individuals at a single location.\n\n\n\n\nmetapopulation\n\n\nValue represents a summary statistic from individuals of the taxon across multiple locations.\n\n\n\n\nspecies\n\n\nValue represents a summary statistic for a species or infraspecific taxon across its range or as estimated by an expert based on their knowledge of the taxon. Data fitting this category include estimates from reference books that represent a taxon’s entire range and values for categorical variables obtained from a reference book or identified by an expert.\n\n\n\n\ngenus\n\n\nValue represents a summary statistic or expert score for an entire genus.\n\n\n\n\nfamily\n\n\nValue represents a summary statistic or expert score for an entire family.\n\n\n\n\norder\n\n\nValue represents a summary statistic or expert score for an entire order.\n\n\n\n\n\n\n\nIdentifiers\nThe traits table includes 12 identifiers, dataset_id, observation_id, taxon_name, population_id, individual_id, temporal_context_id, source_id, location_id, entity_context_id, plot_context_id, treatment_context_id, and method_context_id.\ndataset_id, source_id and taxon_name have easy-to-interpret values. The others are simply integral identifiers that link groups of measurements and are automatically generated through the AusTraits workflow (individual_id can be assigned in the metadata file or automatically generated.)\nTo expand on the definitions provided above,\n\nobservation_id links measurements made on the same entity (individual, population, or species) at a single point in time.\npopulation_id indicates entites that share a common location_id, plot_context_id, and treatment_context_id. It is used to align measurements and observation_id’s for individuals versus populations (i.e. distinct entity_types) that share a common population_id. It is numbered sequentially within a dataset.\nindividual_id indicates a unique organisms. It is numbered sequentially within a dataset by population. Multiple observations on the same organism across time (with distinct observation_id values), share a common individual_id.\ntemporal_context_id indicates a distinct point in time and is used only if there are repeat measurements on a population or individual across time. The identifier links to context properties (& their associated information) in the contexts table for context properties of type temporal.\nsource_id is applied if not all data within a single dataset (dataset_id) is from the same source, such as when a dataset represents a compilation for a meta-analysis.\nlocation_id links to a distinct location_name and associated location_properties in the location table.\nentity_context_id links to information in the contexts table for context properties (& associated values/descriptions) with category entity_context. Entity_contexts include organism sex, organism caste and any other features of an entity that needs to be documented.\nplot_context_id links to information in the contexts table for context properties (& associated values/descriptions) with category plot. Plot contexts include both blocks/plots within an experimental design as well as any stratified variation within a location that needs to be documented (e.g. slope position).\ntreatment_context_idlinks to information in the contexts table for context properties (& associated values/descriptions) with category treatment. Treatment contexts are experimental manipulations applied to groups of individuals.\nmethod_context_idlinks to information in the contexts table for context properties (& associated values/descriptions) with category method. A method context indicates that the same trait was measured on or across individuals using different methods.\n\nAs well, measurement_remarks is used to document brief comments or notes accompanying the trait measurement.\n\n\nLife stage, basis of record\n\nlife_stage is a field to indicate the life stage or age class of the entity measured. standard values are adult, sapling, seedling and juvenile..\nbasis_of_record is a categorical variable specifying from which kind of specimen traits were recorded. Possible values are:\n\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\nfield\n\n\nTraits were recorded on entities living naturally in the field.\n\n\n\n\nfield_experiment\n\n\nTraits were recorded on entities living under experimentally manipulated conditions in the field.\n\n\n\n\ncaptive_cultivated\n\n\nTraits were recorded on entities living in a common garden, arboretum, or botanical or zoological garden.\n\n\n\n\nlab\n\n\nTraits were recorded on entities growing in a lab, glasshouse or growth chamber.\n\n\n\n\npreserved_specimen\n\n\nTraits were recorded from specimens preserved in a collection, eg. herbarium or museum\n\n\n\n\nliterature\n\n\nTraits were sourced from values reported in the literature, and where the basis of record is not otherwise known.\n\n\n\n\n\n\n\nValues, Value types, basis of value\nEach record in the table of trait data has an associated value, value_type, and basis_of_value.\nValue type: A trait’s value_type is either numeric or categorical. - For traits with numerical values, the recorded value has been converted into standardised units and the AusTraits workflow has confirmed the value can be converted into a number and lies within the allowable range. - For categorical variables, records have been aligned through substitutions to values listed as allowable values (terms) in a trait’s definition. * we use _ for multi-word terms, e.g. semi_deciduous\n* we use a space for situations where two values co-occur for the same entity. For instance, a flora might indicate that a plant species can be either annual or biennial, in which case the trait is scored as annual biennial.\nEach trait measurement has an associated value_type, which is a categorical variable describing the statistical nature of the trait value recorded. Possible values are:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\nraw\n\n\nValue recorded for an entity.\n\n\n\n\nminimum\n\n\nValue is the minimum of values recorded for an entity.\n\n\n\n\nmean\n\n\nValue is the mean of values recorded for an entity.\n\n\n\n\nmedian\n\n\nValue is the median of values recorded for an entity.\n\n\n\n\nmaximum\n\n\nValue is the maximum of values recorded for an entity.\n\n\n\n\nmode\n\n\nValue is the mode of values recorded for an entity. This is the appropriate value type for a categorical trait value.\n\n\n\n\nrange\n\n\nValue is a range of values recorded for an entity.\n\n\n\n\nbin\n\n\nValue for an entity falls within specified limits.\n\n\n\n\nunknown\n\n\nNot currently known.\n\n\n\n\n\nEach trait measurement also has an associated basis_of_value, which is a categorical variable describing how the trait value was obtained. Possible values are:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\nmeasurement\n\n\nValue is the result of a measurement(s) made on a specimen(s).\n\n\n\n\nexpert_score\n\n\nValue has been estimated by an expert based on their knowledge of the entity.\n\n\n\n\nmodel_derived\n\n\nValue is derived from a statistical model, for example via gap-filling.\n\n\n\n\nunknown\n\n\nNot currently known."
  },
  {
    "objectID": "database_structure.html#locations",
    "href": "database_structure.html#locations",
    "title": "7  Data structure",
    "section": "8.2 locations",
    "text": "8.2 locations\nDescription: A table containing observations of location/site characteristics associated with information in traits. Cross referencing between the two dataframes is possible using combinations of the variables dataset_id, location_name.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\nlocation_id\n\n\nA unique integer identifier for a location, with locations numbered sequentially within a dataset. The identifier links to specific information in the location table.\n\n\n\n\nlocation_name\n\n\nlocation name\n\n\n\n\nlocation_property\n\n\nThe location characteristic being recorded. The name should include units of measurement, e.g. MAT (C). Ideally we have at least the following variables for each location, longitude (deg), latitude (deg), description.\n\n\n\n\nvalue\n\n\nThe measured value of a location property."
  },
  {
    "objectID": "database_structure.html#contexts",
    "href": "database_structure.html#contexts",
    "title": "7  Data structure",
    "section": "8.3 contexts",
    "text": "8.3 contexts\nDescription: A table containing observations of contextual characteristics associated with information in traits. Cross referencing between the two dataframes is possible using combinations of the variables dataset_id, link_id, and link_vals.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\ncontext_property\n\n\nThe contextual characteristic being recorded. If applicable, name should include units of measurement, e.g. CO2 concentration (ppm).\n\n\n\n\ncategory\n\n\nThe category of context property, with options being plot, treatment, individual_context, temporal and method.\n\n\n\n\nvalue\n\n\nThe measured value of a context property.\n\n\n\n\ndescription\n\n\nDescription of a specific context property value.\n\n\n\n\nlink_id\n\n\nVariable indicating which identifier column in the traits table contains the specified link_vals.\n\n\n\n\nlink_vals\n\n\nUnique integer identifiers that link between identifier columns in the traits table and the contextual properties/values in the contexts table."
  },
  {
    "objectID": "database_structure.html#methods",
    "href": "database_structure.html#methods",
    "title": "7  Data structure",
    "section": "8.4 methods",
    "text": "8.4 methods\nDescription: A table containing details on methods with which data were collected, including time frame and source. Cross referencing with the traits table is possible using combinations of the variables dataset_id, trait_name.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\ntrait_name\n\n\nName of the trait sampled. Allowable values specified in the table definitions.\n\n\n\n\nmethods\n\n\nA textual description of the methods used to collect the trait data. Whenever available, methods are taken near-verbatim from the referenced source. Methods can include descriptions such as ‘measured on botanical collections’, ‘data from the literature’, or a detailed description of the field or lab methods used to collect the data.\n\n\n\n\nmethod_id\n\n\nA unique integer identifier to distinguish between multiple sets of methods used to measure a single trait within the same dataset. The identifier links to specific information in the methods table.\n\n\n\n\ndescription\n\n\nA 1-2 sentence description of the purpose of the study.\n\n\n\n\nsampling_strategy\n\n\nA written description of how study locations were selected and how study individuals were selected. When available, this information is lifted verbatim from a published manuscript. For preserved specimens, this field ideally indicates which records were ‘sampled’ to measure a specific trait.\n\n\n\n\nsource_primary_key\n\n\nCitation key for the primary source in sources. The key is typically formatted as Surname_year.\n\n\n\n\nsource_primary_citation\n\n\nCitation for the primary source. This detail is generated from the primary source in the metadata.\n\n\n\n\nsource_secondary_key\n\n\nCitation key for the secondary source in sources. The key is typically formatted as Surname_year.\n\n\n\n\nsource_secondary_citation\n\n\nCitations for the secondary source. This detail is generated from the secondary source in the metadata.\n\n\n\n\nsource_original_dataset_key\n\n\nCitation key for the original dataset_id in sources; for compilations. The key is typically formatted as Surname_year.\n\n\n\n\nsource_original_dataset_citation\n\n\nCitations for the original dataset_id in sources; for compilationse. This detail is generated from the original source in the metadata.\n\n\n\n\ndata_collectors\n\n\nThe person (people) leading data collection for this study.\n\n\n\n\nassistants\n\n\nNames of additional people who played a more minor role in data collection for the study.\n\n\n\n\ndataset_curators\n\n\nNames of AusTraits team member(s) who contacted the data collectors and added the study to the AusTraits repository."
  },
  {
    "objectID": "database_structure.html#excluded_data",
    "href": "database_structure.html#excluded_data",
    "title": "7  Data structure",
    "section": "8.5 Excluded_data",
    "text": "8.5 Excluded_data\nDescription: A table of data that did not pass quality tests and so were excluded from the master dataset. The structure is identical to that presented in the traits table, only with an extra column called error indicating why the record was excluded. Common reasons are missing_unit_conversions, missing_value, and unsupported_trait_value.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\nerror\n\n\nIndicating why the record was excluded. Common reasons are missing_unit_conversions, missing_value, and unsupported_trait_value.\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\ntaxon_name\n\n\nScientific name of the taxon on which traits were sampled, without authorship. When possible, this is the currently accepted (botanical) or valid (zoological) scientific name, but might also be a higher taxonomic level.\n\n\n\n\nobservation_id\n\n\nA unique integral identifier for the observation, where an observation is all measurements made on an individual at a single point in time. It is important for joining traits coming from the same observation_id. Within each dataset, observation_id’s are unique combinations of taxon_name, population_id, individual_id, and temporal_context_id.\n\n\n\n\ntrait_name\n\n\nName of the trait sampled. Allowable values specified in the table definitions.\n\n\n\n\nvalue\n\n\nThe measured value of a trait.\n\n\n\n\nunit\n\n\nUnits of the sampled trait value after aligning with AusTraits standards.\n\n\n\n\nentity_type\n\n\nA categorical variable specifying the entity corresponding to the trait values recorded.\n\n\n\n\nvalue_type\n\n\nA categorical variable describing the statistical nature of the trait value recorded.\n\n\n\n\nbasis_of_value\n\n\nA categorical variable describing how the trait value was obtained.\n\n\n\n\nreplicates\n\n\nNumber of replicate measurements that comprise a recorded trait measurement. A numeric value (or range) is ideal and appropriate if the value type is a mean, median, min or max. For these value types, if replication is unknown the entry should be unknown. If the value type is raw_value the replicate value should be 1. If the trait is categorical or the value indicates a measurement for an entire species (or other taxon) replicate value should be .na.\n\n\n\n\nbasis_of_record\n\n\nA categorical variable specifying from which kind of specimen traits were recorded.\n\n\n\n\nlife_stage\n\n\nA field to indicate the life stage or age class of the entity measured. Standard values are adult, sapling, seedling and juvenile.\n\n\n\n\npopulation_id\n\n\nA unique integer identifier for a population, where a population is defined as individuals growing in the same location (location_id /location_name) and plot (plot_context_id, a context category) and being subjected to the same treatment (treatment_context_id, a context category).\n\n\n\n\nindividual_id\n\n\nA unique integer identifier for an individual, with individuals numbered sequentially within each dataset by taxon by population grouping. Most often each row of data represents an individual, but in some datasets trait data collected on a single individual is presented across multiple rows of data, such as if the same trait is measured using different methods or the same individual is measured repeatedly across time.\n\n\n\n\nrepeat_measurements_id\n\n\nA unique integer identifier for repeat measurements of a trait that comprise a single observation, such as a response curve.\n\n\n\n\ntemporal_context_id\n\n\nA unique integer identifier assigned where repeat observations are made on the same individual (or population, or taxon) across time. The identifier links to specific information in the context table.\n\n\n\n\nsource_id\n\n\nFor datasets that are compilations, an identifier for the original data source.\n\n\n\n\nlocation_id\n\n\nA unique integer identifier for a location, with locations numbered sequentially within a dataset. The identifier links to specific information in the location table.\n\n\n\n\nentity_context_id\n\n\nA unique integer identifier indicating specific contextual properties of an individual, possibly including the individual’s sex or caste (for social insects).\n\n\n\n\nplot_context_id\n\n\nA unique integer identifier for a plot, where a plot is a distinct collection of organisms within a single geographic location, such as plants growing on different aspects or blocks in an experiment. The identifier links to specific information in the context table.\n\n\n\n\ntreatment_context_id\n\n\nA unique integer identifier for a treatment, where a treatment is any experimental manipulation to an organism’s growing/living conditions. The identifier links to specific information in the context table.\n\n\n\n\ncollection_date\n\n\nDate sample was taken, in the format yyyy-mm-dd, yyyy-mm or yyyy, depending on the resoluton specified. Alternatively an overall range for the study can be indicating, with the starting and ending sample date sepatated by a /, as in 2010-10/2011-03\n\n\n\n\nmeasurement_remarks\n\n\nBrief comments or notes accompanying the trait measurement.\n\n\n\n\nmethod_id\n\n\nA unique integer identifier to distinguish between multiple sets of methods used to measure a single trait within the same dataset. The identifier links to specific information in the methods table.\n\n\n\n\nmethod_context_id\n\n\nA unique integer identifier indicating a trait is measured multiple times on the same entity, with different methods used for each entry. This field is only used if a single trait is measured using multiple methods within the same dataset. The identifier links to specific information in the context table.\n\n\n\n\noriginal_name\n\n\nName given to taxon in the original data supplied by the authors."
  },
  {
    "objectID": "database_structure.html#taxa",
    "href": "database_structure.html#taxa",
    "title": "7  Data structure",
    "section": "8.6 taxa",
    "text": "8.6 taxa\nDescription: A table containing details on taxa that are included in the table traits. We have attempted to align species names with known taxonomic units in the Australian Plant Census (APC) and/or the Australian Plant Names Index (APNI); the sourced information is released under a CC-BY3 license.\nVersion 0.1.0 of AusTraits contains records for 1255 different taxa.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ntaxon_name\n\n\nScientific name of the taxon on which traits were sampled, without authorship. When possible, this is the currently accepted (botanical) or valid (zoological) scientific name, but might also be a higher taxonomic level.\n\n\n\n\ntaxonomic_reference\n\n\nName of the taxonomy (tree) that contains this concept. ie. APC, AusMoss etc.\n\n\n\n\ntaxon_rank\n\n\nThe taxonomic rank of the most specific name in the scientific name.\n\n\n\n\ntrinomial\n\n\nThe infraspecific taxon name match for an original name. This column is assigned na for taxon name that are at a broader taxonomic_resolution.\n\n\n\n\nbinomial\n\n\nThe species-level taxon name match for an original name. This column is assigned na for taxon name that are at a broader taxonomic_resolution.\n\n\n\n\ngenus\n\n\nGenus of the taxon without authorship.\n\n\n\n\nfamily\n\n\nFamily of the taxon.\n\n\n\n\ntaxon_distribution\n\n\nKnown distribution of the taxon, by Australian state.\n\n\n\n\nestablishment_means\n\n\nStatement about whether an organism or organisms have been introduced to a given place and time through the direct or indirect activity of modern humans.\n\n\n\n\ntaxonomic_status\n\n\nThe status of the use of the scientificName as a label for the taxon in regard to the ‘accepted (or valid) taxonomy’. The assigned taxonomic status must be linked to a specific taxonomic reference that defines the concept.\n\n\n\n\nscientific_name\n\n\nThe full scientific name, with authorship and date information if known.\n\n\n\n\nscientific_name_authorship\n\n\nThe authorship information for the scientific name formatted according to the conventions of the applicable.\n\n\n\n\ntaxon_id\n\n\nAn identifier for the set of taxon information (data associated with the taxon class). May be a global unique identifier or an identifier specific to the data set. Must be resolvable within this dataset.\n\n\n\n\nscientific_name_id\n\n\nAn identifier for the set of taxon information (data associated with the taxon class). May be a global unique identifier or an identifier specific to the data set. Must be resolvable within this dataset."
  },
  {
    "objectID": "database_structure.html#taxonomic_updates",
    "href": "database_structure.html#taxonomic_updates",
    "title": "7  Data structure",
    "section": "8.7 taxonomic_updates",
    "text": "8.7 taxonomic_updates\nDescription: A table of all taxonomic changes implemented in the construction of AusTraits. Changes are determined by comparing the originally submitted taxon name against the taxonomic names listed in the taxonomic reference files, best placed in a subfolder in the config folder . Cross referencing with the traits table is possible using combinations of the variables dataset_id and taxon_name.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\noriginal_name\n\n\nName given to taxon in the original data supplied by the authors.\n\n\n\n\ncleaned_name\n\n\nThe taxon name without authorship after implementing automated syntax standardisation and spelling changes as well as manually encoded syntax alignments for this taxon in the metadata file for the corresponding dataset_id. This name has not yet been matched to the currently accepted (botanical) or valid (zoological) taxon name in cases where there are taxonomic synonyms, isonyms, orthographic variants, etc.\n\n\n\n\ntaxonomic_resolution\n\n\nThe rank of the most specific taxon name (or scientific name) to which a submitted orignal name resolves.\n\n\n\n\ncleaned_scientific_name_id\n\n\nAn identifier for the cleaned name before it is updated to the currently accepted name usage. This may be a global unique identifier or an identifier specific to the data set. Must be resolvable within this dataset.\n\n\n\n\ncleaned_name_taxonomic_status\n\n\nThe status of the use of the cleaned_name as a label for a taxon. Requires taxonomic opinion to define the scope of a taxon. Rules of priority then are used to define the taxonomic status of the nomenclature contained in that scope, combined with the experts opinion. It must be linked to a specific taxonomic reference that defines the concept.\n\n\n\n\ncleaned_name_alternative_taxonomic_status\n\n\nThe taxonomic status of alternative taxonomic records with cleaned_name as the accepted (botanical) or valid (zoological) taxon name.\n\n\n\n\ntaxon_id\n\n\nAn identifier for the set of taxon information (data associated with the taxon class). May be a global unique identifier or an identifier specific to the data set. Must be resolvable within this dataset.\n\n\n\n\ntaxon_name\n\n\nScientific name of the taxon on which traits were sampled, without authorship. When possible, this is the currently accepted (botanical) or valid (zoological) scientific name, but might also be a higher taxonomic level.\n\n\n\n\n\nBoth the original and the updated taxon names are included in the traits table."
  },
  {
    "objectID": "database_structure.html#definitions",
    "href": "database_structure.html#definitions",
    "title": "7  Data structure",
    "section": "8.8 definitions",
    "text": "8.8 definitions\nDescription: A copy of the definitions for all tables and terms. Information included here was used to process data and generate any documentation for the study.\nDetails on trait definitions: The allowable trait names and trait values are defined in the definitions file. Each trait is labelled as either numeric or categorical. An example of each type is as follows. For an example, see the the Trait definitions for AusTraits.\nleaf_mass_per_area\n\nnumber of records: 0\nnumber of studies: 0\n\nwoodiness\n\nnumber of records: 0\nnumber of studies: 0"
  },
  {
    "objectID": "database_structure.html#contributors",
    "href": "database_structure.html#contributors",
    "title": "7  Data structure",
    "section": "8.9 contributors",
    "text": "8.9 contributors\nDescription: A table of people contributing to each study.\nContent:\n\n\n\n\n\nkey\n\n\nvalue\n\n\n\n\n\n\ndataset_id\n\n\nPrimary identifier for each study contributed to AusTraits; most often these are scientific papers, books, or online resources. By default this should be the name of the first author and year of publication, e.g. Falster_2005.\n\n\n\n\nlast_name\n\n\nLast name of the data collector.\n\n\n\n\ngiven_name\n\n\nGiven names of the data collector.\n\n\n\n\nORCID\n\n\nORCID of the data collector.\n\n\n\n\naffiliation\n\n\nLast known institution or affiliation.\n\n\n\n\nadditional_role\n\n\nAdditional roles of data collector, mostly contact person."
  },
  {
    "objectID": "database_structure.html#sources",
    "href": "database_structure.html#sources",
    "title": "7  Data structure",
    "section": "8.10 sources",
    "text": "8.10 sources\nFor each dataset in the compilation there is the option to list primary and secondary citations. The primary citation is defined as, The secondary citation is defined as,\nThe element sources includes bibtex versions of all sources which can be imported into your reference library:\nRefManageR::WriteBib(austraits$sources, \"refs.bib\") # write all sources to file\nRefManageR::WriteBib(austraits$sources[\"Falster_2005_1\"], \"refs.bib\") # write a single reference to a file\nOr individually viewed:\naustraits$sources[\"Falster_2005_1\"]\nA formatted version of the sources also exists within the table methods."
  },
  {
    "objectID": "database_structure.html#metadata",
    "href": "database_structure.html#metadata",
    "title": "7  Data structure",
    "section": "8.11 metadata",
    "text": "8.11 metadata\nDescription: Metadata associated with the dataset, including title, creators, license, subject, funding sources."
  },
  {
    "objectID": "database_structure.html#build_info",
    "href": "database_structure.html#build_info",
    "title": "7  Data structure",
    "section": "8.12 build_info",
    "text": "8.12 build_info\nDescription: A description of the computing environment used to create this version of the dataset, including version number, git commit and R session_info."
  },
  {
    "objectID": "database_metadata.html#using-the-oboe-structure",
    "href": "database_metadata.html#using-the-oboe-structure",
    "title": "8  Overview of traits.build ontology",
    "section": "8.1 Using the OBOE structure",
    "text": "8.1 Using the OBOE structure\nThe traits.build ontology is built upon the Extensible Observation Ontology (OBOE). OBOE was developed explicitly to document the multitude of information associated with ecological observations.\nOBOE has 4 core classes:\n\nthe entity, the “thing” being measured. For instance, an entity might be an individual, population or species.\nan observation of the entity at a single point in time. An observation can be comprised of multiple measurements and therefore can be represented by multiple rows of data within a database.\nthe measurements that comprise an observation, where each measurement is of a single trait. A single measured value is associated with each measurement.\nthe trait being measured, which must be documented within the accompanying trait dictionary.\n\n\nWithin the OBOE structure, observations can be grouped into a series of nested observation collections, representing broader groupings of measurements. This flexibility within the OBOE structure, means that the database structure can uniquely indicate which traits are measured at the"
  },
  {
    "objectID": "database_metadata.html#metadata-for-each-trait-value",
    "href": "database_metadata.html#metadata-for-each-trait-value",
    "title": "8  Overview of traits.build ontology",
    "section": "8.2 Metadata for each trait value",
    "text": "8.2 Metadata for each trait value\nEcological data is only meaningful when each data value is linked to all necessary metadata. \nThe OBOE structure also accommodates this requirement, allowing various metadata and context types to be linked to each of the core classes.\nThe following metadata/context fields are linked to each measurement in traits.build:\n\nentity metadata \n-   entity type (species, population, individual) \n-   life stage (adult, seedling, sapling) \n-   basis of record (field, glasshouse, field experiment)     \n-   sex, size, etc.\n\n\nvalue metadata \n-   value type (mean, minimum, maximum, mode, raw) \n-   basis of value (measurement, expert observation, literature) \n-   replicate count \n-   units \n\n\n\nmeasurement metadata \n-   trait collection methods \n\n\nlocation properties \n-   latitude/longitude \n-   vegetation description \n-   location properties, such as \n    -   soil nutrients (soil N)  \n    -   climate variables (MAT, MAP, aridity index)  \n    -   fire history (fire intensity, fire severity, years since fire)          \n\n\ncontext properties \nThat is, anything else recorded about a specific row of data that explains differences between trait values across rows of data \n-   entity contexts (*e.g. plant sex, plant age, leaf type measured, tree diameter*) \n-   treatment contexts (*e.g. nutrient addition treatment, drought treatment*) \n-   plot contexts (*differences within a single \"location\"; e.g. slope position, fire history*)  \n-   temporal contexts (*e.g. sampling season, sampling time of day*) \n-   method contexts (*e.g. canopy position, branch length sampled*)      \n\n\n\ntaxonomic information \n-   genus, family \n-   full scientific name \n-   taxon rank \n-   taxon identifiers (from APC, APNI) \n\n\ndataset metadata \n-   collection date \n-   dataset description \n-   dataset sampling strategy \n-   dataset citation \n-   data collectors"
  },
  {
    "objectID": "database_metadata.html#documenting-the-relationships-between-variables",
    "href": "database_metadata.html#documenting-the-relationships-between-variables",
    "title": "8  Overview of traits.build ontology",
    "section": "8.3 Documenting the relationships between variables",
    "text": "8.3 Documenting the relationships between variables\nTo satisfy the OBOE structure, it is important that these many metadata fields are linked to the correct class, a measurement, specific observation or observation collection, entity, value, etc.\nThe many identifiers present within the traits.build data tables are required to accomplish this.\nFor instance:\n\na population is defined as a group of individuals of the same species, subjected to the same treatment and plot contexts and occurring in the same location. Populations are defined by having a common population_id.\na temporal_context indicates measurements on an entity have been stratified across time, such as occurs when the same individual is measured during two different growing seasons. The column temporal_context_id in the traits table offers an identifier that links to details about the temporal context in the context table."
  },
  {
    "objectID": "dictionary.html",
    "href": "dictionary.html",
    "title": "9  Trait dictionary",
    "section": "",
    "text": "A core component of the traits.build database structure is a trait dictionary that clearly defines and describes each trait concept included within the trait database.\nBuilding upon the standard outlined by the ETS Ecological Trait-data Standard, all numeric traits must have standard units and an allowable range. Categorical traits must have a list of allowable trait values.\nEach trait concept must have a complete, explicit definition to eliminate confusion between trait concepts.\nThe trait dictionary is compiled in a simple yml format, allowing trait definitions to be easily added and edited.\nA sample numeric trait definition:\n\nseed_dry_mass:\n    label: Seed dry mass\n    description: A seed morphology trait [TO:0000184] which is the dry [PATO:0001824]\n    mass [PATO:0000125] of a mature [PATO:0001701] seed [PO:0009010].;Dry mass\n    of a mature seed, including both oven dried and air-dried samples.\n    comments: Standard methods people will have used to dry seeds include, 'fresh'\n    (at dispersal, mature); 'air dried' (at local ambient conditions); 'seed bank\n    air dried' (to 15% relative humidity); and 'oven dried' (&gt;100 deg C for a\n    set number of hours; e.g. seed bank standard is 103 deg C for 17 hours). It\n    is expected that some observations in AusTraits mapped onto â€˜seed_dry_mass'\n    will actually include both the seed and some dispersal tissue, if the two\n    cannot easily be separated; these should be mapped to 'diaspore_dry_mass'.\n    type: numeric\n    units: mg\n    allowed_values_min: 1.0e-05\n    allowed_values_max: 1000000.0\n\nA sample categorical trait definition:\n\nleaf_compoundness:\n    label: Leaf compoundness\n    description: A leaf shape trait [TO:0000492] which is whether a leaf [PO:0025034]\n    is a simple leaf [PO:0020042] or is divided into leaflets [PO:0020049] making\n    it a compound leaf [PO:0020043].;A binary trait that indicates whether a leaf\n    lamina is simple or divided into discontinuous leaflets (compound).\n    comments: See also the trait 'leaf_division' for more detailed leaf compoundness\n    trait values. Note that there might be some species that grade from deeply\n    lobed simple leaves (see trait `leaf_lobation`)  to compound leaves.\n    type: categorical\n    allowed_values_levels:\n        compound: A leaf that is divided into multiple leaflets. [PO:0020043]\n        simple: A leaf with a single undivided blade. [PO:0020042]\n\nFor additional examples, see the complete dictionary used for the AusTraits database."
  },
  {
    "objectID": "traits_build.html",
    "href": "traits_build.html",
    "title": "10  The package",
    "section": "",
    "text": "The traits.build package provides functions needed to build a compilation from sources to the standards specified\nXXXX"
  },
  {
    "objectID": "tutorial_compilation.html#clone-traits.build-template",
    "href": "tutorial_compilation.html#clone-traits.build-template",
    "title": "11  Tutorial: Example compilation",
    "section": "11.1 Clone traits.build-template",
    "text": "11.1 Clone traits.build-template\ntraits.build-template is a GitHub repository that contains:\n\nCore files required to build your own database:\n\na sample trait dictionary\na unit conversions file\na generic database metadata file\na placeholder taxon list\na script of useful R functions\n\nTwo pre-added datasets.\n\nSee how metadata is documented in fully propagated metadata files.\nBuild a skeletal database before adding your own datasets.\n\nDatasets with explicit tutorials for filling in the dataset metadata files.\n\nThe tutorials for these datasets introduce you to the traits.build functions.\nBy working through the dataset tutorials, you are progressively introduced to how the complexities of ecological datasets can be documented within the traits.build structure\nThe datasets offer explicit examples of the types of complexities summarised in the Adding Data vignette.\nAs of September 2023 there are 3 tutorials, but this will expand to 6 tutorials by October 2023."
  },
  {
    "objectID": "tutorial_compilation.html#build-the-example",
    "href": "tutorial_compilation.html#build-the-example",
    "title": "11  Tutorial: Example compilation",
    "section": "11.2 Build the example",
    "text": "11.2 Build the example\nOnce you have cloned the repository and installed the package, the next step is to build the simplistic sample database:\n\nbuild_setup_pipeline()\ntraits.build_database &lt;- remake::make(\"austraits\")\n\nAs described in the chapter on data structure the database is comprised of a series of relational tables and additional lists with metadata.\nOnce you’ve explored and feel familiar with the basic structure of a traits.build database, continue onto the tutorials to learn how to add datasets using the traits.build workflow."
  },
  {
    "objectID": "file_organisation.html#repository-structure",
    "href": "file_organisation.html#repository-structure",
    "title": "12  File organisation",
    "section": "12.1 Repository structure",
    "text": "12.1 Repository structure\nThe main directory for the austraits.build repository contains the following files and folders, with purpose as indicated. Not all of these files are required for a compilation, some are used for extra features such as website. They are included here for completeness.\nFiles used for data compilation\n├── remake.yml/build.R    # instructions for build\n├── config                # configuration files\n├── data                  # raw data files\n├── R                     # folder with custom functions\n├── export                # folder for output\n└── scripts               # scripts for processing files before/after build\nR project file\n├── traits.build.Rproj     # Rstudio project\nFiles for maintaining a repo on github\n├── README.md         # landing page\n├── .github           # folder containing github actions, issue templates, code of conduct\n├── LICENCE\n├── NEWS.md\n├── _pkgdown.yml      # used to create packagedown website\n├── docs              # contains website\n├── Dockerfile        # creates an image of R environment used in build\nFiles used for creation of R package for this compilation\nXXX Explain this more\n├── NAMESPACE             # functions being exported\n├── DESCRIPTION           # R package description\n├── tests                 # defines tests applied to datasets\n├── vignettes             # documentation of repo file structure, AusTraits database structure, definitions, data input processes"
  },
  {
    "objectID": "file_organisation.html#config-folder",
    "href": "file_organisation.html#config-folder",
    "title": "12  File organisation",
    "section": "12.2 /config folder",
    "text": "12.2 /config folder\nThe folder config contains four files which govern the building of the dataset.\nconfig\n├── metadata.yml\n├── traits.yml\n├── taxon_list.csv\n└── unit_conversions.csv\n\nmetadata.yml\nThe file metadata.yml documents dataset-level metadata, including a database description, authors, and funders.\n\n\ntraits.yml\nThe file traits.yml provides the trait definitions used to compile AusTraits, including allowable trait values. The trait definitions are fully described in an additional vignette. A .yml file is a structured data file where information is presented in a hierarchical format (see appendix for details).\n\n\ntaxon_list.csv\nThe file taxon_list.csv is our master list of taxa in the trait database.\nIt includes all unique taxon names after typos have been corrected (through taxonomic_updates). It includes both accepted/valid taxon concepts and outdated taxonomic names. It includes taxon names indicating a taxon that can be identified to species and names that indicate name-alignment to a lower taxon rank.\nThe file taxon_list.csv is added to if a study includes taxa not previously represented in the trait database.\nFor any species (& infraspecific taxon concepts) or genera that align with known taxon concepts, identifiers for these taxon concepts are included if available in a master taxon list. For instance for AusTraits, identifiers from the two vascular plants National Species Lists (NSL), the APC (Australian Plant Census) and the Australian Plant Name Index (APNI) are included.\nIn the file, cleaned_name refers to the taxon name after any typos have been corrected, while taxon_name is the taxon name following updates to the currently accepted/valid taxon name (when available).\n\n\n\n\n\n\ntaxon_name\ncleaned_name\nfamily\ntaxonomic_reference\ntaxon_rank\ncleaned_name_taxonomic_status\ntaxon_id\nscientific_name\nscientific_name_id\n\n\n\n\nAbelia x grandiflora\nAbelia x grandiflora\nCaprifoliaceae\nAPC\nSpecies\naccepted\nhttps://id.biodiversity.org.au/taxon/apni/51432945\nAbelia x grandiflora (Rovelli ex AndrÃ©) Rehder\nhttps://id.biodiversity.org.au/name/apni/190758\n\n\nAbelmoschus ficulneus\nAbelmoschus ficulneus\nMalvaceae\nAPC\nSpecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2897916\nAbelmoschus ficulneus (L.) Wight\nhttps://id.biodiversity.org.au/name/apni/55929\n\n\nAbelmoschus manihot\nAbelmoschus manihot\nMalvaceae\nAPC\nSpecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2901085\nAbelmoschus manihot (L.) Medik.\nhttps://id.biodiversity.org.au/name/apni/55937\n\n\nAbelmoschus manihot subsp. manihot\nAbelmoschus manihot subsp. manihot\nMalvaceae\nAPC\nSubspecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2917035\nAbelmoschus manihot (L.) Medik. subsp. manihot\nhttps://id.biodiversity.org.au/name/apni/116920\n\n\nAbelmoschus manihot subsp. tetraphyllus\nAbelmoschus manihot subsp. tetraphyllus\nMalvaceae\nAPC\nSubspecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2892917\nAbelmoschus manihot subsp. tetraphyllus (Roxb. ex Hornem.) Borss.Waalk.\nhttps://id.biodiversity.org.au/name/apni/55945\n\n\nAbelmoschus moschatus\nAbelmoschus moschatus\nMalvaceae\nAPC\nSpecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2900572\nAbelmoschus moschatus Medik.\nhttps://id.biodiversity.org.au/name/apni/55953\n\n\nAbelmoschus moschatus subsp. biakensis\nAbelmoschus moschatus subsp. biakensis\nMalvaceae\nAPC\nSubspecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2907435\nAbelmoschus moschatus subsp. biakensis (Hochr.) Borss.Waalk.\nhttps://id.biodiversity.org.au/name/apni/116595\n\n\nAbelmoschus moschatus subsp. moschatus\nAbelmoschus moschatus subsp. moschatus\nMalvaceae\nAPC\nSubspecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2911283\nAbelmoschus moschatus Medik. subsp. moschatus\nhttps://id.biodiversity.org.au/name/apni/243806\n\n\nAbelmoschus moschatus subsp. tuberosus\nAbelmoschus moschatus subsp. tuberosus\nMalvaceae\nAPC\nSubspecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2919287\nAbelmoschus moschatus subsp. tuberosus (Span.) Borss.Waalk.\nhttps://id.biodiversity.org.au/name/apni/55961\n\n\nAbildgaardia ovata\nAbildgaardia ovata\nCyperaceae\nAPC\nSpecies\naccepted\nhttps://id.biodiversity.org.au/node/apni/2919627\nAbildgaardia ovata (Burm.f.) Kral\nhttps://id.biodiversity.org.au/name/apni/150737\n\n\n\n\n\n\n\n\n\n\nunit_conversions.csv\nThe file unit_conversions.csv defines the unit conversions that are used when converting contributed trait data to common units, e.g.\n\n\n\n\n\nunit_from\nunit_to\nfunction\n\n\n\n\n%\nmg/g\nx*10\n\n\n%\ng/g\nx*0.01\n\n\n%\nmg/mg\nx*0.01\n\n\n%\nmg/kg\nx*10000\n\n\n%\n{dimensionless}\nx*.01\n\n\n%\n{count}/{count}\nx*.01\n\n\n{dimensionless}\n{count}/{count}\nx*1\n\n\na\nmo\nx*12\n\n\n{count}/m2\n{count}/mm2\nx*1/1000000\n\n\ncm\nm\nx*0.01"
  },
  {
    "objectID": "file_organisation.html#data-folder",
    "href": "file_organisation.html#data-folder",
    "title": "12  File organisation",
    "section": "12.3 /data folder",
    "text": "12.3 /data folder\nThe folder data contains the raw data from individual studies included in AusTraits.\nRecords within the data folder are organised as coming from a particular study, defined by the dataset_id. Data from each study are organised into a separate folder, with two files:\n\ndata.csv: a table containing the actual trait data.\nmetadata.yml: a file that contains study metadata (source, methods, locations, and context), maps trait names and units onto standard types, and lists any substitutions applied to the data in processing.\n\nThe folder data thus contains a long list of folders, one for each study and each containing two files:\ndata\n├── Angevin_2010\n│   ├── data.csv\n│   └── metadata.yml\n├── Barlow_1981\n│   ├── data.csv\n│   └── metadata.yml\n├── Bean_1997\n│   ├── data.csv\n│   └── metadata.yml\n├── ....\n\nwhere Angevin_2010, Barlow_1981, & Bean_1997 are each a unique dataset_id in the final dataset."
  },
  {
    "objectID": "file_organisation.html#dataset_iddata.csv",
    "href": "file_organisation.html#dataset_iddata.csv",
    "title": "12  File organisation",
    "section": "12.4 dataset_id/data.csv",
    "text": "12.4 dataset_id/data.csv\nThe file data.csv contains raw measurements and can be in either long or wide format.\nRequired columns include the taxon name, the trait name (column in long format, header in wide format), units (column in long format, part of header in wide format), location (if applicable), context (if applicable), date (if available), and trait values.\nIt is important that all trait measurements made on the same individual or that are the mean of a species’ measurements from the same location are kept linked.\n\nIf the data is in wide format, each row should include measurements made on a single individual at a single point in time or a single species-by-location mean, with different trait values as consecutive columns.\nIf the data is in long format, an additional column, individual_id, is required to ensure multiple trait measurements made on the same individual, or the mean of a species’ measurements from the same location, are linked. If the data is in wide format and there are multiple rows of data for the same individual, an individual_id column should be included. These individual_id columns ensure that related data values remain linked.\n\nWe aim to keep the data file in the rawest form possible (i.e. with as few changes as possible) but it must be a single csv file. Additional custom R code may be required to make the file exactly compatible with the AusTraits format, but these changes should be executed as AusTraits is compiled and should be in the metadata.yml file under dataset/custom_R_code (see below). Any files used to create the submitted data.csv file (e.g. Excel …) should be archived in a sub-folder within the study folder named raw."
  },
  {
    "objectID": "file_organisation.html#dataset_idmetadata.yml",
    "href": "file_organisation.html#dataset_idmetadata.yml",
    "title": "12  File organisation",
    "section": "12.5 dataset_id/metadata.yml",
    "text": "12.5 dataset_id/metadata.yml\nThe metadata is compiled in a .yml file, a structured data file where information is presented in a hierarchical format (see Appendix for details). There are 10 values at the top hierarchical level: source, contributors, dataset, locations, contexts, traits, substitutions, taxonomic_updates, exclude_observations, questions. These are each described below.\nAs a start, you may want to check out some examples from existing studies in Austraits, e.g. Angevin_2010 or Wright_2009.\n\nsource\nThis section provides citation details for the original source(s) for the data, whether it is a published journal article, book, website, or thesis. In general we aim to reference the primary source. References are written in structured yml format, under the category source and then under sub-groupings primary, secondary, and original. A reference is designated as secondary if it is a second publication by the data collector that analyses the data. When the primary reference is a compilation of multiple sources for a meta-analysis, the original references are designated as original.\nGeneral guidelines for describing a source include:\n\nA maximum of one primary source allowed.\nElements are names as in bibtex format.\nKeys should be named in the format Surname_year and the primary source is almost always identical to the name given to the dataset folder. A second instance of the identical Surname_year should have the key Surname_year_2.\nOne or more secondary source may be included if traits from a single dataset were presented in two different manuscripts. Multiple sources are also appropriate if an author has compiled data from a number of sources, which are not individually in AusTraits, for a published or unpublished compilation.\nIf your data is from an unpublished study, only include the elements that are applicable.\nIf someone has transcribed a published source, the primary source will be the published work and the person who has completed the transcription will be acknowledged as the contributor of the dataset.\n\nAn example of a primary source that is a journal article is:\nsource:\n  primary:\n    key: Falster_2005_1\n    bibtype: Article\n    author: Daniel S. Falster, Mark Westoby\n    year: 2005\n    title: Alternative height strategies among 45 dicot rain forest species from tropical Queensland, Australia\n    journal: Journal of Ecology\n    volume: 93\n    pages: 521--535\n    publisher: Wiley-Blackwell\n    doi: 10.1111/j.0022-0477.2005.00992.x\nIf a secondary source is included it may look like:\n  primary:\n    key: Choat_2006\n    bibtype: Article\n    year: '2006'\n    author: B. Choat and M. C. Ball and J. G. Luly and C. F. Donnelly and J. A. M.\n      Holtum\n    journal: Tree Physiology\n    title: Seasonal patterns of leaf gas exchange and water relations in dry rain\n      forest trees of contrasting leaf phenology\n    volume: '26'\n    number: '5'\n    pages: 657--664\n    doi: 10.1093/treephys/26.5.657\n  secondary:\n    key: Choat_2005\n    bibtype: Article\n    year: '2005'\n    author: Brendan Choat and Marilyn C. Ball and Jon G. Luly and Joseph A. M. Holtum\n    journal: Trees\n    title: Hydraulic architecture of deciduous and evergreen dry rainforest tree species\n      from north-eastern Australia\n    volume: '19'\n    number: '3'\n    pages: 305--311\n    doi: 10.1007/s00468-004-0392-1\n\n\ncontributors\nThis section provides a list of contributors to the study, their respective affiliations, roles in the study, and orcids. The following information is recorded for each data contributor:\n\n\n\n\n\n\nkey\nvalue\n\n\n\n\nlast_name\nLast name of data collector.\n\n\ngiven_name\nGiven name of data collector.\n\n\naffiliation\nAffiliation of data collector.\n\n\nORCID\nORCID ID (Open Researcher and Contributor ID) for the data collector, if available.\n\n\nnotes\noptional notes for the data collector.\n\n\nadditional_role\nAny additional roles the data collector had in the study, a field most frequently used to identify which data contributor is the contact person for the dataset.\n\n\n\n\n\n\n\n\nAn example is as follows:\n data_collectors:\n  - last_name: Falster\n    given_name: Daniel\n    ORCID: 0000-0002-9814-092X\n    affiliation: Evolution & Ecology Research Centre, School of Biological, Earth,\n      and Environmental Sciences, UNSW Sydney, Australia\n    additional_role: contact\n  - last_name: Westoby\n    given_name: Mark\n    ORCID: 0000-0001-7690-4530\n    affiliation: Department of Biological Sciences, Macquarie University, Australia\nNote that only the AusTraits custodians have the contributors’ e-mail addresses on file. This information will not be directly available to AusTraits users or new contributors via Github.\nAdditional fields within contributors are:\n\nAssistants, names of additional people who played a more minor role in data collection for the study.\ndataset_curators, names of austraits team member(s) who contacted the data collectors and added the study to the austraits repository.\n\n\n\ndataset\nThis section includes study details, including format of the data, custom r code applied to data, and various descriptors. the value entered for each element can be either a header for a column within the data.csv file or the actual value to be used.\nThe following elements are included under the element dataset:\n\ndata_is_long_format: Indicates if the data spreadsheet has a vertical (long) or horizontal (wide) configuration with yes or no terminology.\ncustom_R_code: A field where additional R code can be included. This allows for custom manipulation of the data in the submitted spreadsheet into a different format for easy integration with AusTraits. .na indicates no custom R code was used.\ncollection_date: Date sample was taken, in the format yyyy-mm-dd, yyyy-mm or yyyy, depending on the resoluton specified. Alternatively an overall range for the study can be indicating, with the starting and ending sample date sepatated by a /, as in 2010-10/2011-03\ntaxon_name: Scientific name of the taxon on which traits were sampled, without authorship. When possible, this is the currently accepted (botanical) or valid (zoological) scientific name, but might also be a higher taxonomic level.\nlocation_name: location name\nsource_id: For datasets that are compilations, an identifier for the original data source.\nindividual_id: A unique integer identifier for an individual, with individuals numbered sequentially within each dataset by taxon by population grouping. Most often each row of data represents an individual, but in some datasets trait data collected on a single individual is presented across multiple rows of data, such as if the same trait is measured using different methods or the same individual is measured repeatedly across time.\nrepeat_measurements_id: A unique integer identifier for repeat measurements of a trait that comprise a single observation, such as a response curve.\ntrait_name: Element required for long datasets to specify the column indicating the trait name associated with each row of data.\nvalue: The measured value of a trait.\ndescription: A 1-2 sentence description of the purpose of the study.\nbasis_of_record: A categorical variable specifying from which kind of specimen traits were recorded.\nlife_stage: A field to indicate the life stage or age class of the entity measured. Standard values are adult, sapling, seedling and juvenile.\nsampling_strategy: A written description of how study locations were selected and how study individuals were selected. When available, this information is lifted verbatim from a published manuscript. For preserved specimens, this field ideally indicates which records were ‘sampled’ to measure a specific trait.\nmeasurement_remarks: Brief comments or notes accompanying the trait measurement.\noriginal_file: The name of the file initially submitted to AusTraits.\nnotes: Generic notes about the study and processing of data.\n\nOf these, the fields collection_date, life_stage, basis_of_record, and measurement_remarks can all be specified at the dataset level or the traits level (which overrides a dataset-level entry) or location level (which also overrides a dataset-level entry). In each case, they can be a fixed text value or indicate a column within the data.csv file (or generated through custom_R_code) that includes the relevant information.\n\nlife_stage, basis_of_record, and collection_date are usually included under metadata$dataset unless they vary by trait.\nentity_type, replicates, basis_of_value, and value_type are usually different across traits and are usually mapped under the metadata$traits section (see below), but are allowed to be specified for the entire dataset in this section.\ntraits and value are only specified in metadata$dataset for long-format datasets.\nmeasurement_remarks and individual_id are only included if required. They are absent from the majority of datasets.\n\nAn example is as follows:\n  data_is_long_format: no\n  custom_R_code: '\n    data %&gt;%\n      mutate(\n        location_name = \"Howard River catchment\",\n        date = date %&gt;% mdy()\n      ) %&gt;%\n      arrange(date) %&gt;%\n      group_by(Tree) %&gt;%\n        mutate(observation_number = dplyr::row_number()) %&gt;%\n      ungroup()\n  '\n  collection_date: date\n  taxon_name: species\n  context_name: context\n  location_name: location_name\n  individual_id: Tree\n  description: Measurements of stem CO2 efflux and leaf gas exchange in a tropical\n    savanna ecosystem in northern Australia, and assessed the impact of fire on these\n    processes.\n  basis_of_record: field\n  life_stage: adult\n  sampling_strategy: The stem CO2 efflux was initially measured at two locations,\n    each of which was nested within a 3 km 2 plot...\n  original_file: leaf_summary.xls, Rbranch summary2.xls, and Rstem summary6.xls submitted\n    by Lucas Cernusak and archived in the raw data folder and GoogleDrive folder.\n  notes: none\nA common use of the custom_R_code is to automate the conversion of a verbal description of flowering or fruiting periods into the supported trait values. It might also be used if values for a single trait are expressed across multiple columns and need to be merged. See Catford_2014 as an example of this. The adding data vignette provides additional examples of code regularly implemented in custom_R_code, including functions specifically that were developed for AusTraits data manipulations and are in the file scripts\\custom.R.\n\n\nlocations\nThis section provides a list of study locations (sites) and information about each of the study locations where data were collected. Each should include at least three variables - latitude (deg), longitude (deg) and description. Additional variables can be included where available. Set to .na for botanical collections and field studies where data values are a mean across many locations.\nAlthough the properties listed under each location are not part of a controlled vocabulary, it is best practice to align with in-use properties whenever possible. These can be identified by running austraits$locations %&gt;% distinct(location_property).\nAn example of how a location and its properties, and the value of each property are listed (modified from Vesk_2019), is:\n  Round Hill-Nombinnie Nature Reserve:\n    latitude (deg): -32.965\n    longitude (deg): 146.161\n    precipitation, MAP (mm): 370\n    temperature, summer mean (C): 32.5\n    temperature, winter mean (C): 14.2\n    soil type: loamy red sands light red clays and light red browns earths\n    description: predominantly open Callitris glaucophylla - Eucalyptus populnea woodland\n      and Eucalyptus dumosa - E. socialis shrub mallee woodland\n    fire frequency (years): 5-20 years\n\n\ncontexts\nThis section provides contextual characteristics associated with information in traits.\nWithin the context section is a list of contextual properties, each encapsulating information read in through a different column or created through custom_R_code or as elements within specific traits (see below).\n\ncontext_property: The context property represented by the data in the column specified by var_in.\ncategory: The category of contextual data. Options are plot (a distinct collection of organisms within a single geographic location, such as plants growing on different aspects or blocks in an experiment), treatment (an experimental treatment), entity_context (contextual information to record about the entity the isn’t documented elsewhere, including the entity’s sex, caste), temporal (indicating when repeat observations are made on the same individual (or population, or taxon) across time) and method (indicating the same trait was measured on the same individual (or population, or taxon) using multiple methods).\nvar_in: Name of column with contextual data in the original data submitted.\nfind: The contextual values in the original data submitted (optional)\nvalue: The standardised contextual values, aligning syntax and wording with other studies.\ndescription: A description of the contextual values.\n\nIf the contextual values read in are appropriate and no substitutions are required, the field find can be omitted, with the values from the data.csv column entered under the field value. The field description can likewise be omitted if it is redundant; for instance, if the values are simply sequential observation numbers, times of day, or taxon names (e.g. insect host plants).\nAs with location, the context properties are not part of a controlled vocabulary, but it is best practice to align syntax with in-use properties whenever possible. These can be identified by running austraits$contexts %&gt;% distinct(context_property).\nAn example of how the contexts for a study are formatted (modified from Crous_2013), is:\ncontexts:\n- context_property: sampling season\n  category: temporal_context\n  var_in: month\n  values:\n  - find: AUG\n    value: August\n    description: August (late winter)\n  - find: DEC\n    value: December\n    description: December (early summer)\n  - find: FEB\n    value: February\n    description: February (late summer)\n- context_property: temperature treatment\n  category: treatment_context\n  var_in: Temp-trt\n  values:\n  - value: ambient\n    description: Plants grown at ambient temperatures; Jan average max = 29.4 dec\n      C / July average min = 3.2 dec C.\n  - value: elevated\n    description: Plants grown 3 deg C above ambient temperatures.\n- context_property: CO2 treatment\n  category: treatment_context\n  var_in: CO2_Treat\n  values:\n  - find: ambient CO2\n    value: 400 ppm\n    description: Plants grown at ambient CO2 (400 ppm).\n  - find: added CO2\n    value: 640 ppm\n    description: Plants grown at elevated CO2 (640 ppm); 240 ppm above ambient.\n- context_property: measurement temperature\n  category: method_context\n  var_in: method_context\n  values:\n  - find: Measurement made at 20°C\n    value: 20°C\n    description: Measurement made at 20°C\n  - find: Measurement made at 25°C\n    value: 25°C\n    description: Measurement made at 25°C\n\n\ntraits\nThis section provides a translation table, mapping traits and units from a contributed study onto corresponding variables in AusTraits. The methods used to collect the data are also specified here.\nFor each trait submitted to AusTraits, there is the following information:\n\nvar_in: Name of trait in the original data submitted.\nunit_in: Units of trait in the original data submitted.\ntrait_name: Name of the trait sampled. Allowable values specified in the table definitions.\nentity_type: A categorical variable specifying the entity corresponding to the trait values recorded.\nvalue_type: A categorical variable describing the statistical nature of the trait value recorded.\nbasis_of_record: A categorical variable specifying from which kind of specimen traits were recorded.\nbasis_of_value: A categorical variable describing how the trait value was obtained.\nsource_id: For datasets that are compilations, an identifier for the original data source.\nreplicates: Number of replicate measurements that comprise a recorded trait measurement. A numeric value (or range) is ideal and appropriate if the value type is a mean, median, min or max. For these value types, if replication is unknown the entry should be unknown. If the value type is raw_value the replicate value should be 1. If the trait is categorical or the value indicates a measurement for an entire species (or other taxon) replicate value should be .na.\ncollection_date: Date sample was taken, in the format yyyy-mm-dd, yyyy-mm or yyyy, depending on the resoluton specified. Alternatively an overall range for the study can be indicating, with the starting and ending sample date sepatated by a /, as in 2010-10/2011-03\nmeasurement_remarks: Brief comments or notes accompanying the trait measurement.\nmethods: A textual description of the methods used to collect the trait data. Whenever available, methods are taken near-verbatim from the referenced source. Methods can include descriptions such as ‘measured on botanical collections’, ‘data from the literature’, or a detailed description of the field or lab methods used to collect the data.\nlife_stage: A field to indicate the life stage or age class of the entity measured. Standard values are adult, sapling, seedling and juvenile.\nrepeat_measurements_id: A unique integer identifier for repeat measurements of a trait that comprise a single observation, such as a response curve.\n\nThe elements trait_name, entity_type, value_type, basis_of_record, and basis of value are controlled vocabularies; the values for these elements must be from the list of allowable values. Those for traits are listed in the traits.yml file or vignette. For the other elements, see the database structure vignette.\nThe fields replicates, basis_of_value, value_type, life_stage, basis_of_record, and measurement_remarks can all be specified at the dataset level or the traits level (which overrides a dataset-level entry). In each case, they can be a fixed text value or indicate a column (within the data.csv file or generated through custom_R_code) that includes the relevant information. In addition, fields can be added to specify a specific context (most commonly a method context, but occasionally a temporal context). If such a field is added, the same name must appear in both the contexts section and for some (or all) of the traits.\nTwo examples are as follows:\n- var_in: LeafP.m\n  unit_in: mg/g\n  trait_name: leaf_P_per_dry_mass\n  entity_type: individual                   # fixed value\n  value_type: value_type_column             # referencing a column\n  basis_of_value: measurement               # fixed value\n  replicates: count                         # referencing a column\n  methods: Oven-dried leaf material was used for determination of total leaf nitrogen\n    and phosphorus. Dried ground leaf material was hot-digested in acid-peroxide before\n    colorimetric analysis using a flow injection system (QuikChem 8500, Lachat Instruments,\n    Loveland, Colorado, USA).\n\nand\n- var_in: Jmax25\n  unit_in: umol/m2/s\n  trait_name: Jmax_per_area\n  entity_type: individual                    # fixed value\n  value_type: raw                            # fixed value\n  basis_of_value: measurement                # fixed value\n  replicates: 1                              # fixed value\n  method_context: 25C                        # optional field\n  methods: Controlled photosynthetic CO2 response curve measurements were made using\n    Li-Cor 6400 portable infrared gas analysers (LiCor Inc., Lincoln, NE, USA). CO2\n    response curves of net CO2 assimilation (Anet) were developed at a constant temperature\n    (termed 'Anet-Ci curves') for intact leaves within each tree chamber. These Anet-Ci\n    curve measurements progressed at four to five specified leaf temperatures for\n    the same leaf (i.e. one leaf per chamber) in each of three seasons (early summer,\n    December 2010; late summer, February 2011...\n\n\n\nsubstitutions\nThis section provides a list of any “find and replace” substitutions needed to get the data into the right format.\nSubstitutions are required whenever the exact word(s) used to describe a categorical trait value in AusTraits is different from the vocabulary used by the author in the data.csv file. It is preferable to align vocabulary using substitutions rather than changing the data.csv file. The trait definitions file provides a list of supported values for each trait.\nEach substitution is documented using the following elements:\n\ntrait_name: Trait where substitutions are required.\nfind: Contributor’s trait value that needs to be changed.\nreplace: AusTraits supported replacement value.\n\nAn example is as follows:\nsubstitutions:\n- trait_name: life_history\n  find: p\n  replace: perennial\n- trait_name: plant_growth_form\n  find: s\n  replace: shrub\n- ...\n\n\ntaxonomic_updates\nThis section provides a table of taxonomic name changes needed to align original names in the dataset with taxon names in the chosen taxonomic reference(s).\nEach substitution is documented using the following elements:\n\nfind: Name given to taxon in the original data supplied by the authors.\nreplace: Scientific name of the taxon on which traits were sampled, without authorship. When possible, this is the currently accepted (botanical) or valid (zoological) scientific name, but might also be a higher taxonomic level.\nreason: Records why the change was implemented, e.g. typos, taxonomic synonyms, and standardising spellings\n\nAlgorithms within AusTraits automatically align outdated taxonomy and taxonomic synonyms to their currently accepted scientific name, so such adjustments are not documented as substitutions.\nSome examples of taxonomic updates are as follows:\ntaxonomic_updates:\n- find: Drummondita rubroviridis\n  replace: Drummondita rubriviridis\n  reason: match_07_fuzzy. Fuzzy alignment with accepted canonical name in APC (2022-11-21)\n  taxonomic_resolution: Species\n- find: Acacia ancistrophylla/sclerophylla\n  replace: Acacia sp. [Acacia ancistrophylla/sclerophylla; White_2020]\n  reason: match_04. Rewording taxon where `/` indicates uncertain species identification\n    to align with `APC accepted` genus (2022-11-10)\n  taxonomic_resolution: genus\n- find: Polyalthia (Wyvur)\n  replace: Polyalthia sp. (Wyvuri B.P.Hyland RFK2632)\n  reason: match_15_fuzzy. Fuzzy match alignment with species-level canonical name\n    in `APC known` when everything except first 2 words ignored (2022-11-10)\n  taxonomic_resolution: Species\n\n\nquestions\nThis section provides a place to record any queries we have about the dataset (recorded as a named array), including notes on any additional traits that may have been collected in the study but have not been incorporated into austraits.\nAn example is as follows:\nquestions:\n  questions for author: Triglochin procera has very different seed masses in the main traits spreadsheet and the field seeds worksheet. Which is correct? There are a number of species with values in the field leaves worksheet that are absent in the main traits worksheet - we have included this data into Austraits; please advise if this was inappropriate.\n  austraits: need to map aquatic_terrestrial onto an actual trait once one is created."
  },
  {
    "objectID": "file_organisation.html#rcustom_r_code.r",
    "href": "file_organisation.html#rcustom_r_code.r",
    "title": "12  File organisation",
    "section": "12.6 R/custom_R_code.R",
    "text": "12.6 R/custom_R_code.R\nThe austraits.build compilation contains an extra folder, R containing a file custom_R_code.R. This file documents any custom functions used in the compilation, called as part of the custom_R_code section of metadata files. These functions are also avaiable in the traits.build-template repo\nIt includes functions to:\n\nReplace duplicate trait values with NA’s\nConvert various month formats into a string of 12 NY’s (to document flowering, fruiting, recruitment times)\nMove specific categorical trait values to a second trait\n\nFor instance, there are many datasets where a species-level trait measurement is repeated across many rows of data but should only be incorporated into the dataset a single time:\ncustom_R_code: `\n  data %&gt;%\n    mutate(\n      across(c(`plant_growth_form`, `leaf_shape`), replace_duplicates_with_NA)\n    )\n`\nThis file can be added to within specific traits.build projects, as required for different dataset styles."
  },
  {
    "objectID": "create_dictionary.html",
    "href": "create_dictionary.html",
    "title": "13  Creating a dictionary",
    "section": "",
    "text": "All with data within a trait database must be including in an accompanying trait dictionary.\nFollowing the ETS Ecological Trait-data Standard:\n\nall traits must have a label and definition\nall traits must be defined as being either type: numeric or type: categorical\nnumeric traits must have standard units and an allowable range\ncategorical traits must have a list of allowable trait values.\n\nExamples are available here, while the complete AusTraits dictionary is available here.\nThe trait dictionary is compiled in a simple yml format, allowing trait definitions to be easily added and edited.\nCreating a trait dictionary that allows a database to build is quick; it simply requires filling in a brief definition and the required units/range/trait value fields. However, drafting meaningful, reusable trait definitions and categorical trait values requires complete, explicit definitions to eliminate confusion between trait concepts. The definitions and trait values should be agreed upon by the broader research community to allow the database to be reused by other researchers and supported by the research community.\nFor instance, to achieve these broader goals, the trait dictionary used for the AusTraits database has been formalised into a standalone output, the AusTraits Plant Dictionary (APD), available in both machine-readable and humman-friendly outputs through the w3id.org/APD namespace. The traits included in the APD have undergone a rigorous internal and external review process to ensure the trait concepts and their definitions are complete and robust."
  },
  {
    "objectID": "adding_data_brief.html",
    "href": "adding_data_brief.html",
    "title": "14  Adding datasets (brief)",
    "section": "",
    "text": "This section gives a brief overview of how to add datasets to the database. For a more detailed guide, see adding_datasets.\nIt is important that all steps are followed so that our automated workflow proceeds without problems.\n\nClone the traits.build repository from github\nCreate a new branch in the repo, named for the new dataset_id in author_year format, e.g. Gallagher_2014.\nCreate a new folder within the folder data with the name dataset_id, e.g. Gallagher_2014.\nPrepare the file data.csv and place it within the new folder.\nPrepare the file metadata.yml and place it within the new folder.\nRun tests on the newly added dataset and correct the data.csv and metadata.yml files as necessary.\nAdd the new study into the build framework and rebuild AusTraits, by running build_setup_pipeline() and source(build.R).\n\nYou can then rebuild AusTraits, including your dataset.\n\nRun quality checks on the newly added dataset and correct the data.csv and metadata.yml files as necessary.\nGenerate and proofread a report on the data building reports. In particular, check that numeric trait values fall within a logical range relative to other studies, and that individual trait observations are not unnecessarily excluded because their trait values are unsupported.\nReturn to step 6 if changes are made to the data.csv or metadata.yml files.\nPush the GitHub branch.\n\nThe best place to get started learning how to add datasets is to work through a series of 7 tutorials. Each introduces you to specific traits.build functions designed to facilitate the addition of dataset metadata or the metadata formats for specific types of datasets.\nIt may also help to download one of the two sample datasets to use as a template for your own files and a guide on required content. Or alteratively, to see a greater diversity of dataset styles, look at the austraits.build repository\nYou should look at the files in the config folder, particularly the definitions file for the list of traits in AusTraits and how trait definitions are formatted.\nThe exhaustive adding data section offers a fairly comprehensive run through of each step and all the known variants on how to fill the various metadata fields, but is likely overwhelming until you are familiar with the traits.build workflow and metadata format."
  },
  {
    "objectID": "custom_R_code.html",
    "href": "custom_R_code.html",
    "title": "15  Adding custom_R_code",
    "section": "",
    "text": "Occasionally all the changes we want to make to dataset may not fit into the prescribed workflow used in AusTraits. For example, we assume that each trait has a single unit. But there are a few datasets where data on different rows have different units. So we want to make to make some custom modifications to this particular dataset before the common pipeline of operations gets applied. To make this possible, the workflow allows for some custom R code to be run as a first step in the processing pipeline. That pipeline (in the function read_data_study) looks like:\n\ndata &lt;-\n  read_csv(filename_data_raw, col_types = cols()) %&gt;%\n  process_custom_code(metadata[[\"dataset\"]][[\"custom_R_code\"]])() %&gt;%\n  process_parse_data(dataset_id, metadata, contexts, schema) %&gt;%\n  ...()\n\nNote the second line.\n\nExample problem\nAs an example, for Blackman_2010 we want to combine two columns to create an appropriate location variable. Here is the code that was included in data/Blackman_2010/metadata.yml under custom_R_code.\n\ndata %&gt;% mutate(\n  location = ifelse(location == \"Mt Field\" & habitat == \"Montane rainforest\", \"Mt Field_wet\", location),\n  location = ifelse(location == \"Mt Field\" & habitat == \"Dry sclerophyll\", \"Mt Field_dry\", location)\n)\n\nThis is the finished solution, but to get there we did as follows:\nGenerally, this code should\n\nassume a single object called data, and apply whatever fixes are needed\nuse dplyr functions like mutate, rename, etc\nuse pipes to weave together a single statement, if possible. (Otherwise you’ll need a semi colon ; at the end of each statement).\nbe fully self-contained (we’re not going to use any of the other remake machinery here)\n\nFirst, load an object called data:\n\nlibrary(readr)\nlibrary(yaml)\n\ndata &lt;- read_csv(file.path(\"data\", \"Blackman_2010\", \"data.csv\"), col_types = cols())\ndata\n\nSecond, write your code to manipulate data, like the example above\nThird, once you have some working code, you then want to add it into your yml file under dataset -&gt; custom_R_code.\nFinally, check it works. Let’s assume you added it in. The function metadata_check_custom_R_code loads the data and applies the custom R code:\n\nmetadata_check_custom_R_code(\"Blackman_2010\")"
  },
  {
    "objectID": "publishing.html",
    "href": "publishing.html",
    "title": "16  Publishing",
    "section": "",
    "text": "traits.build is an R package that was first developed to build AusTraits, a harmonised, open-source database of Australian plant traits. The code has been transformed into a standalone package allowing anyone to build a relational, tabular database for any taxonomic group and any collection of traits.\nThe project’s guiding principles are to:\n\nCreate open-source, harmonised, reproducible databases from disparate datasets.\nProvide a fully transparent workflow.\nOffer a relational database structure that fully documents the contextual data essential to interpreting ecological data.\nOffer a straightforward, robust template for building a trait dictionary.\nOffer a database structure that is flexible enough to accommodate the complexities inherent to ecological data.\nOffer a database structure that is underlain by a documented ontology, ensuring each database field is interpretable and interoperable with other databases and data structures."
  },
  {
    "objectID": "adding_data_long.html",
    "href": "adding_data_long.html",
    "title": "17  Adding datasets",
    "section": "",
    "text": "18 Getting started\nThe traits.build repository includes a selection of functions that help build the repository. To use these, you’ll need to make them available.\nThe easiest way to load the functions into your workspace is to run the following (from within the repository)\nBefore starting the quality checks, it is helpful to assign a variable, current_study:\nThis lets you have a list of tests you run for each study and you just have to reassign a new dataset_id to current_study.\nIt is best to run tests and fix formatting first.\nBy far our preferred way of contributing is for you to contribute files directly into the repository and then send a pull request with your input. You can do this by\nIn short,\nBefore you make a substantial pull request, you should always file an issue and make sure someone from the team agrees that it’s worth pursuing the problem. If you’ve found a bug, create an associated issue and illustrate the bug with a minimal reprex illustrating the issue.\nIf this is not possible, you could email the relevant files (see above) to the AusTraits email: austraits.database@gmail.com"
  },
  {
    "objectID": "adding_data_long.html#add-a-new-folder",
    "href": "adding_data_long.html#add-a-new-folder",
    "title": "17  Adding datasets",
    "section": "19.1 Add a new folder",
    "text": "19.1 Add a new folder\nAdd a new folder within the data folder. Its name should be the study’s dataset_id, the core organising unit behind AusTraits.\nThe preferred format for dataset_id is the surname of the first author of any corresponding publication, followed by the year, as surname_year. E.g. Falster_2005. Wherever there are multiple studies with the same id, we add a suffix _2, _3 etc. E.g.Falster_2005, Falster_2005_2."
  },
  {
    "objectID": "adding_data_long.html#csv_file",
    "href": "adding_data_long.html#csv_file",
    "title": "17  Adding datasets",
    "section": "19.2 Constructing the data.csv file",
    "text": "19.2 Constructing the data.csv file\nAll data for a study (dataset_id) must be merged into a single spreadsheet: data.csv. All accompanying metadata is read in through the metadata.yml file. Some information must be input explicitly through the data.csv or metdata.yml file, while other information can be entered via either file; this is explicitly indicated for each element.\n\nRequired columns: Columns within the data.csv file must include taxon name, location_name (if there are multiple locations), contexts (if appropriate), and collection_date (if appropriate). The data.csv file can either be in a wide format (1 column for each trait, with trait name as the column header) or long format (a single column for all trait values and additional columns for trait name and units)\n\n\nFor all field studies, ensure there is a column for location_name. If all measurements were made at a single location, a location_name column can easily be mutated using custom_R_code within the metadata.yml file. See sections adding locations and adding contexts below for more information on compiling location and context data.\nIf available, be sure to include a column with collection date. If possible, provide in yyyy-mm-dd (e.g. 2020-03-05) format or, if the day of the month isn’t known, as yyyy-mm (e.g. 2020-03). However, any format is allowed and the column can be parsed to the proper yyyy-mm-dd format using custom_R_code. If the same collection date applies to the entire study it can be added directly into the metadata.yml file.\nIf applicable, ensure there are columns for an context properties, including experimental treatments, specific differences in method, a stratified sampling scheme within a plot, or sampling season. Additional context columns could be added through custom_R_code or keyed in where traits are added, but it is best to include a column in the data.csv file whenever possible. The protocol for adding context properties to the metadata file is under adding contexts\n\n\nSummarising data: Data submitted by a contributor should be in the rawest form possible; always request data with individual measurements over location/species means. Some studies make replicate measurements on an individual at a single point in time. For these studies, individual means need to be calculated, as AusTraits does not include multiple measurements per individual. The raw values are preserved in the contributor’s raw data files. Be sure to calculate the number of replicates that contributed to each mean value.\n\nWhen there is just a single row of values to summarise, use:\nread_csv(\"data/dataset_id/raw/raw_data.csv\") %&gt;%\n  mutate(leaf_area_replicates = 1) %&gt;%\n  group_by(individual, `species name`, location, context, etc) %&gt;%\n  summarise(\n    leaf_area_mean = mean(leaf_area),\n    leaf_area_replicates = sum(leaf_area_replicates)\n    ) %&gt;%\n  ungroup()\n(Make sure you group_by all categorical variables you want to retain, for only columns that are grouping variables will be kept)\nWhen you want to take the mean of a series of continuous variables, use:\nread_csv(\"data/dataset_id/raw/raw_data.csv\") %&gt;%\n  mutate(replicates = 1) %&gt;%\n  group_by(individual, `species name`, location, context, etc) %&gt;%\n  summarise(\n    across(\n      c(leaf_area, `leaf N`), .fns = mean,\n      c(replicates), .fns = sum,\n      c(growth_form, `photosynthetic pathway`), .fns = first\n    )\n  ) %&gt;%\n  ungroup()\n\nCategorical variables not included as grouping variables will return NA\nThis allows you to retain character variables, but can be tedious with many columns. Generally use the function first for categorical variables - it simply retains the trait value in the first column. In the rare case when rows in a particular grouping have different categorical values, more complex manipulations are required.\nYou can identify runs of columns by column number/position. For instance c(5:25), .fns = mean or c(leaf_area:leaf_N), .fns = mean\n\n\nMerging multiple spreadsheets: If multiple spreadsheets of data are submitted these must be merged together.\n\n\nIf the spreadsheets include different trait measurements made on the same individual (or location means for the same species), they are best merged using full_join, specifying all conditions that need to be matched across spreadsheets (e.g. individual, species, location, context). Ensure the column names are identical between spreadsheets or specify columns that need to be matched.\n\nread_csv(\"data/dataset_id/raw/data_file_1.csv\") -&gt; data_1\nread_csv(\"data/dataset_id/raw/data_file_2.csv\") -&gt; data_2\ndata_1 %&gt;% full_join(data_2, by = c(\"Individual\", \"Taxon\", \"Location\", \"Context\"))\n\nIf the spreadsheets include trait measurements for different individuals (or possibly data at different scales - such as individual level data for some traits and species means for other traits), they are best merged using bind_rows. Ensure the column names for taxon name, location name, context, individual, and collection date are identical between spreadsheets. If there are data for the same traits in both spreadsheets, make sure those column headers are identical as well.\n\nread_csv(\"data/dataset_id/raw/data_file_1.csv\") -&gt; data_1\nread_csv(\"data/dataset_id/raw/data_file_2.csv\") -&gt; data_2\ndata_1 %&gt;% bind_rows(data_2)\n\nTaxon names: Taxon names need to be complete names. If the main data file includes code names, with a key as a separate file, they need to be merged:\n\nread_csv(\"data/dataset_id/raw/species_key.csv\") -&gt; species_key\nread_csv(\"data/dataset_id/raw/data_file.csv\") %&gt;%\n  left_join(species_key, by = \"code\")\n\nUnexpected hangups\n\nWhen Excel saves an .xls file as a .csv file it only preserves the number of significant figures that are displayed on the screen. This means that if, for some reason, a column has been set to display a very low number of significant figures or a column is very narrow, data quality is lost.\nIf you’re reading a file into R where there are lots of blanks at the beginning of a column of numeric data, the defaults for read_csv fail to register the column as numeric. It is fixed by adding the argument guess_max:\n\nread_csv(\"data/dataset_id/raw/raw_data.csv\", guess_max = 10000)\nThis checks 10,000 rows of data before declaring the column is non-numeric. The value can be set even higher…"
  },
  {
    "objectID": "adding_data_long.html#metadata_file",
    "href": "adding_data_long.html#metadata_file",
    "title": "17  Adding datasets",
    "section": "19.3 Constructing the metadata.yml file",
    "text": "19.3 Constructing the metadata.yml file\nOne way to construct the metadata.yml file is to use one of the existing files and modify yours to follow the same format. As a start, check out some examples from existing studies in AusTraits, e.g. Angevin_2011 or Wright_2009.\nNote, when editing the metadata.yml, edits should be made in a proper text editor (Microsoft word tends to mess up the formatting). For example, Rstudio, textmate, sublime text, and Visual Studio Code are all good editors.\nTo assist you in constructing the metadata.yml file, we have developed functions to help fill in the different sections of the file. You can then manually edit the file further to fill in missing details.\nFirst run the following to make the functions available\nlibrary(traits.build)\nThe functions for populating the metadata file all begin with metadata_. A list of the available functions is automatically generated within the man/ folder within the traits.build directory.\n\nCreating a template\nCreate a basic template for the metadata.yml file for your study. Note, it requires you to have already created a file data.csv in the folder data/your_dataset_id.\nLet’s imagine you’re entering a study called Yang_2028\ncurrent_study &lt;- \"Yang_2028\"\n\nmetadata_create_template(current_study)\n\n# or simply\n\nmetadata_create_template(\"Yang_2028\")\nThe function will ask a series of questions and then create a relatively empty file data/your_dataset_id/metadata.yml. The key questions are:\n\nIs the data long vs wide? A wide dataset has each variable (i.e. trait ) as a column. A long dataset has a single row containing all trait values and additional columns specifying units and trait_name.\nSelect column for taxon_name\nSelect column for trait_name (long datasets only)\nSelect column for trait values (long datasets only)\nSelect column for location_name\nSelect column for individual_id (a column that links measurements on the same individual)\nSelect column for collection_date\n\nIf your data.csv file does not yet have a location_name column, this information can later be added manually.\n\n\nAdding a source\nThree functions are available to help with entering citation details for the source data.\nThe function metadata_create_template creates a template for the primary source with default fields for a journal article, which you can then edit manually.\nIf you have a doi for your study, use the function:\nmetadata_add_source_doi(dataset_id = current_study, doi = \"doi\")\nand the different elements within the source will automatically be generated. Double check the information added to ensure: 1. The title is in sentence case 2. Overall, the information isn’t in all caps (information from a few journals is read in like this) 3. Pages numbers are present and added as, for example, 123 -- 134 ; note the -- between page numbers\nBy default, details are added as the primary source. If multiple sources are linked to a single dataset_id, you can specify a source as secondary. Attempting to add a second primary source will overwrite the information already input.\nmetadata_add_source_doi(dataset_id, doi, type = \"secondary\")\n\nSecondary sources will be assigned the same dataset_id as the primary source. Manually edit the key in the metadata.yml file to be the appropriate author_yyyy code for the secondary reference. Sequential qualifiers can be used if necessary (e.g. author_yyyy_2)\nA “secondary” source might be either a second research output from the main dataset (truly a secondary source) or the original source of some data compiled for a metaanalysis (an original source). After adding a second source, you must manually change the source’s header to beginning with either secondary or original, as is appropriate.\nIf there are many sources to add (i.e. for datasets compiled for metaanalyses), after you add each reference, go to the metadata.yml file and manually change the source’s header from original to original_01 (and then original_02, etc.; or secondary_01 ; secondary_02). See Richards_2008 for an example of a complex source list.\n\nAlternatively, if you have reference details saved in a bibtex file called myref.bib you can use the function\nmetadata_add_source_doi(dataset_id, file = \"myref.bib\")\n(These options require the packages rcrossref and RefManageR to be installed.)\nFor a book, the proper format is:\nsource:\n  primary:\n      key: Cooper_2013\n      bibtype: Book\n      year: 2013\n      author: Wendy Cooper and William T. Cooper\n      title: Australian rainforest fruits\n      publisher: CSIRO Publishing\n      pages: 272\nFor an online resource, the proper format is:\nsource:\n  primary:\n    key: TMAG_2009\n    bibtype: Online\n    author: '{Tasmanian Herbarium}'\n    year: 2009\n    title: Flora of Tasmania Online\n    publisher: Tasmanian Museum & Art Gallery (Hobart)\n    url: http://www.tmag.tas.gov.au/floratasmania\nFor a thesis, the proper format is:\nsource:\n  primary:\n      key: Kanowski_2000\n      bibtype: Thesis\n      year: 1999\n      author: John Kanowski\n      title: Ecological determinants of the distribution and abundance of the folivorous\n        marsupials endemic to the rainforests of the Atherton uplands, north Queensland.\n      type: PhD\n      institution: James Cook University, Townsville\nFor an unpublished dataset, the proper format is:\nsource:\n  primary:\n    key: Ooi_2018\n    bibtype: Unpublished\n    year: 2018\n    author: Mark K. J. Ooi\n    title: \"Unpublished data: Herbivory survey within Royal National Park, University\n      of New South Wales\"\nIf you manually add information, note that if there is a colon (:) or apostrophe (’) in a reference, the text for that line must be in quotes (“).\n\n\nAdding contributors\nThe skeletal metadata.yml file created by the function metadata_create_template includes a template for entering details about data contributors. Edit this manually, duplicating if details for multiple people are required.\n\nAuthorship is extended to anyone who played a key intellectual role in the experimental design and data collection. Most studies have 1-3 authors. For each author, please provide a last_name, given_name, institutional affiliation, email address, and their ORCID (if available). Nominate a single contributor to be the dataset’s point of contact; this person’s email will not be listed in the metadata file, but is the person future AusTraits users are likely to seek out if they have questions.\nAdditional field assistants can be listed under assistants:\nThe AusTraits data entry person is listed under dataset_curators:\n\nFor example, in Roderick_2002\ncontributors:\n  data_collectors:\n  - last_name: Roderick\n    given_name: Michael\n    ORCID: 0000-0002-3630-7739\n    affiliation: The Australian National University, Australia\n    additional_role: contact\n  assistants: Michelle Cochrane\n  dataset_curators: Elizabeth Wenk\n\n\nCustom R code\nFor many studies there are changes we want to make to a dataset before the data.csv file is read into AusTraits. These most often include applying a function to transform data, a function to filter data, or a function to replace a contributor’s “measurement missing” placeholder symbol with NA. In each case it is appropriate to leave the rawer data in data.csv.\n\nBackground\nIn each case we want to make some custom modifications to a particular dataset before the common pipeline of operations gets applied. To make this possible, the workflow allows for some custom R code to be run as a first step in the processing pipeline. That pipeline (the function process_custom_code called within dataset_process) looks like this:\ndata &lt;-\n  read_csv(filename_data_raw, col_types = cols(), guess_max = 1e5) %&gt;%\n  process_custom_code(metadata[[\"dataset\"]][[\"custom_R_code\"]])() %&gt;%\n  process_parse_data(dataset_id, metadata, contexts, schema)\nNote the second line. This is where the custom code gets applied, right after the file is loaded.\n\n\nSummary\n\nsource the file containing functions the AusTraits team have explicitly developed to use within the custom_R_code field: source(\"R/custom_R_code.R\")\nassume a single object called data, and apply whatever fixes are needed\n\nuse functions from the packages dplyr or tiydr, like mutate, rename, etc, and otherwise avoid external packages\n\nalternatively, use the functions we’ve created explicitly for pre-processing data that were sourced through the file custom.R. In consultation with AusTraits team leaders you can add functions to this file.\nbe fully self-contained (we’re not going to use any of the other remake machinery here)\nuse pipes to weave together a single statement, where possible. (Otherwise you’ll need a semi colons ; at the end of each statement).\nplace a single apostrophe (’) at the start and end of your custom R code; this allows you to add line breaks between pipes.\n\n\n\nExamples of appropriate use of custom R code\n\nMost sources from herbaria record flowering_time and fruiting_time as a span of months, while AusTraits codes these variables as a sequence of 12 N’s and Y’s for the 12 months. A series of functions make this conversion in custom_R_code. These include:\n\n‘format_flowering_months’ (Create flowering times from start to end pair)\n‘convert_month_range_string_to_binary’ (Converts flowering and fruiting month ranges to 12 element character strings of binary data)\n‘convert_month_range_vec_to_binary’ (Convert vectors of month range to 12 element character strings of binary data)\n‘collapse_multirow_phenology_data_to_binary_vec’ (Converts multirow phenology data to a 12 digit binary string)\n\nMany datasets from herbaria record traits like leaf_length, leaf_width, seed_length, etc. as a range (e.g. 2-8). The function separate_range separates this data into a pair of columns with minimum and maximum values, required to properly align units\nDuplicate values within a study need to be filtered out.\n\nIf a species-level measurement has been entered for all within-location replicates, you need to filter out the duplicates. This is true for both numeric and categorical values.\ndata %&gt;%\n  group_by(Species) %&gt;%\n    mutate(\n      across(c(`leaf_percentN`, `plant growth form`), replace_duplicates_with_NA)\n      ) %&gt;%\n  ungroup()\nNote: You would use group_by(Species, Location) if there are unique values at the species x location level.\n\nValues that were sourced from a different study need to be filtered out. See Duplicates between studies below - functions to automate this process are in progress.\nAuthor has represented missing data values with a symbol, such as 0 :\n\ndata %&gt;% mutate(across(c(`height (cm)`, `leaf area (mm2)`), ~ na_if(., 0)))\n\nIf a subset of data in a column are also values for a second trait in AusTraits, some data values can be duplicated in a second temporary column. In the example below, some data in the contributor’s fruit_type column also apply to the trait fruit_fleshiness in AusTraits:\n\ndata %&gt;% mutate(fruit_fleshiness = ifelse(`fruit type` == \"pome\", \"fleshy\", NA))\n\nIf a subset of data in a column are instead values for a second trait in AusTraits, some data values can be moved to a second column (second trait), using the function ‘move_values_to_new_trait’. In the example below, some data in the contributor’s growth_form column only apply to the trait parasitic in AusTraits. Note you need to create a blank variable to move the trait values to.\n\ndata %&gt;%\n  mutate(new_trait = NA_character) %&gt;%\n  move_values_to_new_trait(\n    original_trait= \"growth form\",\n    new_trait = \"parasitic\",\n    original_values = \"parasitic\",\n    values_for_new_trait = \"parasitic\",\n    values_to_keep = \"NA\")\nor\ndata %&gt;%\n  mutate(dispersal_appendage = NA.char) %&gt;%\n  move_values_to_new_trait(\n    \"fruits\", \"dispersal_appendage\",\n    c(\"dry & winged\", \"enclosed in aril\"),\n    c(\"wings\", \"aril\"),\n    c(\"NA\", \"enclosed\")\n  )\n\nNote, the parameter values_to_keep currently doesn’t accept NA ; this bug is known and will be fixed.\n\n\nIf the data.csv file includes raw data that you want to manipulate into a trait, or the contributor presents the data in a different formulation than AusTraits:\n\ndata %&gt;% mutate(root_mass_fraction = `root mass` / (`root mass` + `shoot mass`))\n\nYou can do manipulations, such as adding a column with locations or manipulating location names. This is only recommended for studies with a single (or few) location, where manually adding the location data to the metadata.yml file is fast, since in precludes automatically propagating location data into metadata (see Adding location details). As an example, see Blackman_2010:\n\ndata %&gt;%\n  mutate(\n    location_name = ifelse(location_name == \"Mt Field\" & habitat == \"Montane rainforest\", \"Mt Field_wet\", location_name),\n    location_name = ifelse(location_name == \"Mt Field\" & habitat == \"Dry sclerophyll\", \"Mt Field_dry\", location_name)\n  )\n\nYou can generate observation_numbers for sequential measurements on the same individual\n\ndata %&gt;%\n  group_by(Tree) %&gt;%\n    mutate(observation_number = row_number()) %&gt;%\n  ungroup()\n\nYou can generatemeasurement_remarks from more cryptic notes\n\ndata %&gt;%\n  mutate(\n        measurement_remarks = ifelse(material == \"FRESH\",\"fresh leaves (indicating amount of leaf moisture)\", NA),\n        measurement_remarks = ifelse(material == \"DRIED\",\"dry leaves (indicating amount of leaf moisture)\", measurement_remarks),\n        measurement_remarks = ifelse(material == \"SENESCED\",\"senesced leaves (indicating amount of leaf moisture)\", measurement_remarks),\n  )\n\nYou can reformat collection_dates supplied into the yyyy-mm-dd format, or add a date column\n\nConverting from any mdy format to yyyy-mm-dd (e.g. Dec 3 2015 to 2015-12-03)\ndata %&gt;% mutate(Date = Date %&gt;% mdy())\nConverting from any dmy format to yyyy-mm-dd (e.g. 3-12-2015 to 2015-12-03)\ndata %&gt;% mutate(Date = Date %&gt;% dmy())\nConverting from a mmm-yyyy (string) format to yyyy-mm (e.g. Dec 2015 to 2015-12)\ndata %&gt;% mutate(Date = parse_date_time(Date, orders = \"my\") %&gt;% format.Date(\"%Y-%m\"))\nConverting from a mdy format to yyyy-mm (e.g. Excel has reinterpreted the data as full dates 12-01-2015 but the resolution should be “month” 2015-12)\ndata %&gt;% mutate(Date = parse_date_time(Date, orders = \"mdy\") %&gt;% format.Date(\"%Y-%m\"))\nA particularly complicated example where some dates are presented as yyyy-mm and others as yyyy-mm-dd\ndata %&gt;%\n    mutate(\n      weird_date = ifelse(str_detect(gathering_date, \"^[0-9]{4}\"), gathering_date, NA),\n      gathering_date = gathering_date %&gt;% mdy(quiet = T) %&gt;% as.character(),\n      gathering_date = coalesce(gathering_date, weird_date)\n    ) %&gt;%\n    select(-weird_date)\n\n\nTesting your custom R code\nAfter you’ve added the custom R code to a file, check that it has completed the intended data frame manipulation:\nmetadata_check_custom_R_code(\"Blackman_2010\")\nYou could alternatively read the data.csv file into R and run the code line by line.\n\n\n\nFill in metadata$dataset\nThe dataset section is a mix of fields that are filled in automatically during metadata_create_template() and fields that need to be manually filled in.\n\nindividual_id Individual_id is one of the fields that can be read in during metadata_create_template. However, you may instead mutate your own individual_id using custom_R_code and add it in manually. For a wide dataset individual_id is required anytime there are multiple rows of data for the same individual and you want to keep these linked. This field should only be included if it is required.\n\nWARNING If you have an entry individual_id: unknown this assigns all rows of data to an individual named “unknown” and the entire dataset will be assumed to be from a single individual. This is why it is essential to omit this field if there isn’t an actual row of data being read in.\n\ncollection_date If this is not read in as a specified column, it needs to be filled in manually as start date/end date in yyyy-mm-dd, yyyy-mm, or yyyy format, depending on the relevant resolution. If the collection dates are unknown, write unknown/publication year\ndescription: 1-2 sentence description of the study’s goals. The abstract of a manuscript usually includes some good sentences/phrases to borrow.\nbasis_of_record: Allowable values include: field, field_experiment, captive_cultivated, lab, preserved_specimen, and literature. See the top of system.file(\"support\", \"traits.build_schema.yml\", package = \"traits.build\" or database structure vignette for definitions of these accepted basis_of_record values. This field can also be read in from a column or can be specified at the location or trait level, as described below. Entries under metadata$locations or metadata$traits (which apply to only that specific location or trait) override the global value entered under metadata$dataset.\nlife_stage: Allowable values include: adult, sapling, seedling, juvenile. This field can also be read in from a column or can be specified at the location or trait level, as described below. Entries under metadata$locations or metadata$traits (which apply to only that specific location or trait) override the global value entered under metadata$dataset.\nsampling_strategy: Often a quite long description of the sampling strategy, extracted verbatim from a manuscript.\noriginal_file: The name of the file initially submitted to AusTraits and archived in a Google Drive folder and usually in the dataset folder, in a subfolder named raw.\nnotes: Notes about the study and processing of data, especially if there were complications or if some data is suspected duplicates with another study and were filtered out.\n\nThere are also fields that will only be used for a subset of datasets:\n\nmeasurement_remarks: Measurement remarks is a field to capture a miscellaneous notes column. This should be information that is not captured by “methods” (which is fixed to a single value for a trait). It can be read in for the whole dataset, or entered under dataset$traits if the remarks only apply to specific traits.\nentity_type, value_type, replicates, and basis_of_value are standardly added to each trait, but a fixed value or column could be read in under metadata$dataset\n\n\n\nAdd traits\nBegin by automatically adding all traits to your skeletal metadata.yml file:\nmetadata_add_traits(current_study)\nYou will be asked to indicate the columns you wish to keep as distinct traits. Include all columns with trait data.\nThis automatically propagates each trait selected into metadata.yml as follows where var_in is the name of a column in the data.csv file (for wide datasets) or a unique trait name values in the trait_name column (for a long dataset):\n- var_in: leaf area (mm2)\n  unit_in: .na\n  trait_name: .na\n  entity_type: .na\n  value_type: .na\n  basis_of_value: .na\n  replicates: .na\n  methods: .na\nThe trait details then need to be filled in manually.\n\nunits: fill in the units specified by the author - such as mm2. If you’re uncertain about the syntax/format used for some more complex units, look through the traits definition file (config/traits.yml) or the file showing unit conversions (config/unit_conversions.csv). For categorical variables, leave this as .na.\ntrait_name: This is the appropriate trait name from config/traits.yml. If no appropriate trait exists in AusTraits, a new trait can often be added - just ensure it is a trait where data will be comparable across studies and has been measured for a fair number (~&gt;50) species. For currently unsupported traits, we leave this as .na but then fill in the rest of the data and flag this study as having a potential new trait. Then in the future, when this trait is added to the traits.yml file, the data can be read into AusTraits by simply replacing the .na with a trait name.\nentity_type: Entity types indicate the taxonomic/ecological hierarchical level corresponding to the trait value. Entity types can be individual, population, species, genus, family or order. Metapopulation-level measurements are coded as population and infraspecific taxon-level measurements are coded as species. See the top of system.file(\"support\", \"traits.build_schema.yml\", package = \"traits.build\") for definitions of these accepted entity types. Note: entity_type is about the hierarchical level to which the trait measurement refers; this is separate from the taxonomic resolution of the entity’s name.\nvalue_type: Allowable value types are mean, minimum, maximum, mode, range, raw, and bin. See the top of system.file(\"support\", \"traits.build_schema.yml\", package = \"traits.build\") for definitions of these accepted value types. All categorical traits are generally scored as being a mode, the most commonly observed value. Note that for values that are bins, the two numbers are separated by a double-hyphen, 1 -- 10.\nbasis_of_value: Basis of value indicates how a value was determined. Allowable terms are measurement, expert_score, model_derived, and literature. See the top of system.file(\"support\", \"traits.build_schema.yml\", package = \"traits.build\") for definitions of these accepted value types, but in general most categorical traits are values that have been scored by an expert (expert_score) and more numeric trait values are measurements.\nreplicates: Fill in with the appropriate value. For categorical variables, leave this as .na. If there is a column that specifies replicate number, you can list the column name in the field.\nmethods: This information can usually be copied verbatim from a manuscript. In general, methods sections extracted from pdfs include “special characters” (non-UTF-8 characters). Non-English alphabet characters are recognised (e.g. é, ö) and should remain unchanged. Other characters will be re-formatted during the study input process, so double check that degree symbols (º), en-dashes (–), em-dashes (—), and curly quotes (‘,’,“,”) have been maintained or reformatted with a suitable alternative. Greek letters and some other characters are replaced with their Unicode equivalent (e.g. &lt;U+03A8&gt; replaces Psi (Ψ)); for these it is best to replace the symbol with an interpretable English-character equivalent.\n\nNote with methods, if the identical methods apply to a string of traits, for the first trait use the following syntax, where the &leaf_length_method notation assigns the remaining text in the field as the leaf_length_method.\n\n  methods: &leaf_length_method All measurements were from dry herbarium collections, with leaf and bracteole measurements taken from the largest of these structures on each specimen.\nThen for the next trait that uses this method you can just include. At the end of processing you can read/write the yml file and this will fill in the assigned text throughout.\n  methods: *leaf_length_method\n\nIn addition to the automatically propagated fields, there are a number of optional fields you can add if appropriate.\n\nlife_stage If all measurements in a dataset were made on plants of the same life stage a global value should be entered under metadata$dataset. However if different traits were measured at different life stages or different rows of data represent measurements at different life stages you can specify a unique life stage for each trait or indicate a column where this information is stored.\nbasis_of_record If all measurements in a dataset represent the same basis_of_record a global value should be entered under metadata$dataset. However if different traits have different basis_of_record values or different rows of data represent different basis_of_record values you can specify a unique basis_of_record value for each trait or indicate a column where this information is stored.\nmeasurement_remarks: Measurement remarks is a field to indicate miscellaneous comments. If these comments only apply to specific trait(s), this field should be specified with those trait’s metadata sections. This meant to be information that is not captured by “methods” (which is fixed to a single value for a trait).\nmethod_context If different columns in a wide data.csv file indicate measurements on the same trait using different methods, this needs to be designated. At the bottom of the trait’s metadata, add a method_context_name field (e.g. method_context words well). Write a word or short phrase that indicates which method context applies to that trait (data column). For instance, one trait might have method_context: fully expanded leaves and a second entry with the same trait name and method might have method_context: leaves still expanding. The method context details must also be added to the contexts section.\ntemporal_context If different columns in a wide data.csv file indicate measurements on the same trait, on the same individuals at different points in time, this needs to be designated. At the bottom of the trait’s metadata, add a temporal_context_name field (e.g. temporal_context words well). Write a word or short phrase that indicates which temporal context applies to that trait (data column). For instance, one trait might have temporal_context: dry season and a second entry with the same trait name and method might have temporal_context: after rain. The temporal context details must also be added to the contexts section.\n\n\n\nAdding location details\nLocation data includes location names, latitude/longitude coordinates, verbal location descriptions, and any additional abiotic/biotic location variables provided by the contributor (or in the accompanying manuscript). For studies with more than a few locations, it is most efficient to create a table of this data that is automatically read into the metadata.yml file.\n\nLocation names must be identical (including syntax, case) to those in data.csv\nColumn headers for latitude and longitude data must read latitude (deg) and longitude (deg)\nLatitude and longitude must be in decimal degrees (i.e. -46.5832). There are many online converters to convert from degrees,minutes,seconds format or UTM. Or use the following formula: decimal_degrees = degrees + (minutes/60) + (seconds/3600)\nIf there is a column with a general vegetation description (i.e. rainforest, coastal heath it should be titled description)\nAlthough location properties are not restricted to a controlled vocabulary, newly added studies should use the same location property syntax as others whenever possible, to allow future discoverability. To generate a list of already used under location_property, use:\n\naustraits$locations %&gt;% distinct(location_property)\nA few contributors provide a standalone file of all location data. Otherwise, the following sequence works well:\n\nIdentify all location names in the data.csv file. The following code extracts a list of location names and any other columns in the data file that include location-specific information:\n\nread_csv(\"data/dataset_id/data.csv\") %&gt;%\n  distinct(location, .keep_all = TRUE) %&gt;% # the argument `.keep_all` ensures columns aren't dropped\n  select(location, rainfall, lat, lon) %&gt;% # list of relevant columns to keep\n  rename(all_of(c(\"latitude (deg)\" = \"lat\", \"longitude (deg)\" = \"long\")))  # rename columns to how you want them to appear in the metadata file. Faster to do it once here than repeatedly in the metadata file\n  write_csv(\"data/dataset_id/raw/location_data.csv\")\n\nOpen the spreadsheet in Excel (or any editor of your choice) and manually add any additional data from the manuscript. Save as a .csv file.\nOpen in R\n\nread_csv(\"data/dataset_id/raw/location_data.csv\") -&gt; location_data\nAs an example of what the location table should look like:\n\nThis location data can then be read into metadata.yml:\n\nmetadata_add_locations(current_study, site_data)\nYou are first prompted to identify the column with the location name and then to list all columns that contain location data. This automatically fills in the location component on the metadata file.\nIt is possible that you will want to specify life_stage or basis_of_record at the location_level. You can later manually add these fields to some or all locations.\n(During processing location_id’s are automatically generated and paired with each location_name.)\n\n\nContext details\nThe dictionary definition of a context is the situation within which something exists or happens, and that can help explain it. This is exactly what context_properties are in AusTraits, ancillary information that is important to explaining and understanding a trait value.\nAusTraits recognises 5 categories of contexts: - treatment contexts are experimental treatments applied to individuals, such as soil nutrient manipulations, growing temperatures, or CO2 enchancement. - plot contexts are either blocks/plots within an experimental design or a variable that has been measured within a location and measurements have been stratified across this variable. Topographic position within a location is an example of this. - temporal contexts relate to repeat measurements on the same entity (individual, population, or species) across time. They may simply be number observations or might be explicitly linked to growing season or time of day. - method contexts indicate that the same trait has been measured on the same entity (individual, population or species) using multiple methods. These might be samples from different canopy light environments, different leaf ages, or sapwood samples from different branch diameters. - entity_contexts capture ancillary information about the entity (individual, population or species) that helps explain the measured trait values. This might be the entity’s sex, caste (for social insects), or host plant (for insects).\nContext properties are not restricted to a controlled vocabulary. However, newly added studies should use the same context property syntax as others whenever possible, to allow future discoverability. To generate a list of terms already used under context_property, use:\naustraits$contexts %&gt;% distinct(context_property)\nThe AusTraits workflow can handle as many context properties as is required. These are most easily read with the dedicated function\nmetadata_add_contexts(dataset_id)\nThe function first displays a list of all data columns (from the data.csv file) and prompts you to select those that are context properties. For each column you are asked to indicate its category (those described above). You are shown a list of unique values present in the data column and asked if these require any substitutions. This function adds the following information to the section metadata$contexts (example from Crous_2013)\n- context_property: unknown\n  category: temporal_context\n  var_in: month\n  values:\n  - find: AUG\n    value: unknown\n    description: unknown\n  - find: DEC\n    value: unknown\n    description: unknown\n  - find: FEB\n    value: unknown\n    description: unknown\n- context_property: unknown\n  category: treatment_context\n  var_in: Temp-trt\n  values:\n  - value: ambient\n    description: unknown\n  - value: elevated\n    description: unknown\n- context_property: unknown\n  category: treatment_context\n  var_in: CO2_Treat\n  values:\n  - find: ambient CO2\n    value: unknown\n    description: unknown\n  - find: added CO2\n    value: unknown\n    description: unknown\nYou must then manually fill in the fields designated as unknown. You are permitted to omit the description field if the context_property value itself provides sufficient description.\nIf there are additional context properties that were designated in the traits section, these will have to be added manually, as this information is not captured in a column. A final output might be:\n- context_property: sampling season\n  category: temporal_context\n  var_in: month\n  values:\n  - find: AUG\n    value: August\n    description: August (late winter)\n  - find: DEC\n    value: December\n    description: December (early summer)\n  - find: FEB\n    value: February\n    description: February (late summer)\n- context_property: temperature treatment\n  category: treatment_context\n  var_in: Temp-trt\n  values:\n  - value: ambient\n    description: Plants grown at ambient temperatures; Jan average max = 29.4 dec\n      C / July average min = 3.2 dec C.\n  - value: elevated\n    description: Plants grown 3 deg C above ambient temperatures.\n- context_property: CO2 treatment\n  category: treatment_context\n  var_in: CO2_Treat\n  values:\n  - find: ambient CO2\n    value: 400 ppm\n    description: Plants grown at ambient CO2 (400 ppm).\n  - find: added CO2\n    value: 640 ppm\n    description: Plants grown at elevated CO2 (640 ppm); 240 ppm above ambient.\n- context_property: measurement temperature\n  category: method_context\n  var_in: method_context          #this field would be included in the relevant traits\n  values:\n  - value: 20°C                   # this value would be keyed in through the relevant traits\n    description: Measurement made at 20°C\n  - value: 25°C\n    description: Measurement made at 25°C\n\n\nUsing substitutions\nIt is very unlikely that a contributor will use categorical trait values that are entirely identical to those in the traits.yml file. You need to add substitutions for those that do not exactly align to match the wording and syntax supported by AusTraits. Combinations of multiple trait values are allowed - simply list them, space delimited (e.g. shrub tree for a species whose growth form includes both). Note that combinations of multiple trait values are reorganised into alphabetic order in order to collapse into fewer combinations (e.g. “fire_killed resprouts” and “resprouts fire_killed” are alphabetised and hence collapsed into one combination, “fire_killed resprouts”).\nSingle substitutions can be added by running:\nmetadata_add_substitution(current_study, \"trait_name\", \"find\", \"replace\")\nwhere trait_name is the AusTraits defined trait name, find is the trait value used in the data.csv file and replace is the trait value supported by AusTraits.\nIf you have many substitutions to add, the following may be more efficient:\n\nAdd a single substitution via the function and then copy and paste the lines many times in the metadata.yml file, changing the relevant fields\nCreate a spreadsheet with a list of all trait_name by trait_value combinations requiring substitutions. The spreadsheet would have four columns with headers dataset_id, trait_name, find and replace. This table can be read directly into the metadata.yml file using the function metadata_add_substitutions_table. This is described below under Adding many substitutions.\n\n\nExcluded data\nThis section of the metadata.yml file provides the capacity to explicitly exclude specific trait values or taxon names. These are values that are in the data.csv file but should be excluded from AusTraits.\nIt includes three elements: - variable: A variable from the traits table, typically taxon_name, location_name or context_name - find: Value of variable to remove - reason: Records why the data was removed, e.g. exotic\nMultiple, comma-delimited values can be added under find.\nFor example, in Munroe_2019:\nexclude_observations:\n- variable: taxon_name\n  find: Campylopus introflexus, Dicranoloma menziesii, Philonotis tenuis, Polytrichastrum\n    alpinum, Polytrichum juniperinum, Sphagnum cristatum\n  reason: moss (E Wenk, 2020.06.18)\n- variable: taxon_name\n  find: Xanthoparmelia semiviridis\n  reason: lichen (E Wenk, 2020.06.18)\n\n\nQuestions\nThe final section of the metadata.yml file is titled questions. This is a location to:\n\nAsk the data contributor targeted questions about their study. When you generate a report (described below) these questions will appear at the top of the report.\n\nPreface the first question you have with contributor: (indented once), and additional questions with question2:, etc.\nAsk contributors about missing metadata\nPoint contributors attention to odd data distributions, to make sure they look at those traits extra carefully.\nLet contributors know if you’re uncertain about their units or if you transformed the data in a fairly major way.\nAsk the contributors if you’re uncertain you aligned their trait names correctly.\n\nThis is a place to list any trait data that are not yet traits supported by AusTraits. Use the following syntax, indented once: additional_traits:, followed by a list of traits.\n\n\n\nHooray! You now have a fully propagated metadata.yml file!\nNext is making sure it has captured all the data exactly as you’ve intended."
  },
  {
    "objectID": "adding_data_long.html#clear-formatting",
    "href": "adding_data_long.html#clear-formatting",
    "title": "17  Adding datasets",
    "section": "20.1 Clear formatting",
    "text": "20.1 Clear formatting\nThe clear formatting code below reads and re-writes the yaml file. This is the same process that is repeated when running functions that automatically add substitutions or check taxonomy. Running it first ensures that any formatting issues introduced (or fixed) during the read/write process are identified and solved first.\nFor instance, the write_metadata function inserts line breaks every 80 characters and reworks other line breaks (except in custom_R_code). It also reformats special characters in the text, substituting in its accepted format for degree symbols, en-dashes, em-dashes and quotes, and substituting in Unicode codes for more obscure symbols.\nf &lt;- file.path(\"data\", current_study, \"metadata.yml\")\nread_metadata(f) %&gt;% write_metadata(f)"
  },
  {
    "objectID": "adding_data_long.html#running_tests",
    "href": "adding_data_long.html#running_tests",
    "title": "17  Adding datasets",
    "section": "20.2 Running tests",
    "text": "20.2 Running tests\nBegin by running some automated tests to ensure the dataset meets the required set up. The tests run through a collection of pre-specified checks on the files for each study. The output alerts you to possible issues needing to be fixed, by comparing the data in the files with the expected structure and allowed values, as specified in the schema and definitions.\nCertain special characters may show up as errors and need to be manually adjusted in the metadata.yml file\nThe tests also identify mismatches between the location names in the data.csv file vs. metadata.yml file (same for context), unsupported trait names, etc.\nTo run the tests, the variable dataset_ids must be defined in the global namespace, containing a vector of ids to check. For example:\n# load relevant functions\nlibrary(traits.build)\n\n# Tests run test on one study\ndataset_ids &lt;- \"Bragg_2002\"\ndataset_test(dataset_ids)\n\n# Tests run test on all studies\ndataset_ids &lt;- dir(\"data\")\ndataset_test(dataset_ids)\nFix as many errors as you can and then rerun dataset_test() repeatedly until no errors remain.\nSee below for suggestions on how to implement large numbers of trait value substitutions."
  },
  {
    "objectID": "adding_data_long.html#rebuild-austraits",
    "href": "adding_data_long.html#rebuild-austraits",
    "title": "17  Adding datasets",
    "section": "20.3 Rebuild AusTraits",
    "text": "20.3 Rebuild AusTraits\nNow incorporate the new study into AusTraits:\nbuild_setup_pipeline()\naustraits &lt;- remake::make(\"austraits\")"
  },
  {
    "objectID": "adding_data_long.html#check-excluded-data",
    "href": "adding_data_long.html#check-excluded-data",
    "title": "17  Adding datasets",
    "section": "20.4 Check excluded data",
    "text": "20.4 Check excluded data\nAusTraits automatically excludes data for a number of reasons. These are available in the frame excluded_data.\nWhen you are finished running quality checks, no data should be excluded due to Missing unit conversion and Unsupported trait.\nA few values may be legitimately excluded due to other errors, but check each entry.\nThe best way to view excluded data for a study is:\naustraits$excluded_data %&gt;%\n  filter(\n    dataset_id == current_study,\n    error != \"Observation excluded in metadata\"\n  ) %&gt;%\n  View()\nMissing values (blank cells, cells with NA) are not included in the excluded_data table, because they are assumed to be legitimate blanks. If you want to confirm this, you need to temporarily change the default arguments for the internal function dataset_process where it is called within the remake.yml file. For instance, the default,\n      dataset_process(\"data/Ahrens_2019/data.csv\",\n                  Ahrens_2019_config,\n                  schema\n                 )\nneeds to be changed to:\n      dataset_process(\"data/Ahrens_2019/data.csv\",\n                  Ahrens_2019_config,\n                  schema,\n                  filter_missing_values = FALSE\n                 )\n\nReasons for data to be excluded\nPossible reasons for excluding a trait value includes:\n\nMissing species name: Species name is missing from data.csv file for a given row of data. This usually occurs when there are stray characters in the data.csv file below the data – delete these rows.\nMissing unit conversion: Value was present but appropriate unit conversion was missing. This requires that you add a new unit conversion to the file config/unit_conversions.csv. Add additional conversions near similar unit conversions already in the file for easier searching in the future.\nObservation excluded in metadata: Specific values, usually certain taxon names can be excluded in the metadata. This is generally used when a study includes a number of non-native and non-naturalised species that need to be excluded. These should be intentional exclusions, as they have been added by you.\nTime contains non-number: Indicates a problem with the value entered into the traits flowering_time and fruiting_time. (Note to AusTraits custodians: This error should no longer appear - will retain for now as a placeholder.)\nUnsupported trait: trait_name not listed in config/traits.yml, under traits. Double check you have used the correct spelling/exact syntax for the trait_name, adding a new trait to the traits.yml file if appropriate. If there is a trait that is currently unsupported by AusTraits, leave trait_name: .na. Do not fill in an arbitrary name.\nUnsupported trait value: This error, referencing categorical traits, means that the value for a trait is not included in the list of supported trait values for that trait in config/traits.yml. See adding many substitutions if there are many trait values requiring substitutions. If appropriate, add another trait value to the traits.yml file, but confer with other curators, as the lists of trait values have been carefully agreed upon through workshop sessions.\nValue does not convert to numeric: Is there a strange character in the file preventing easy conversion? This error is rare and generally justified.\nValue out of allowable range: This error, referencing numeric traits, means that the trait value, after unit conversions, falls outside of the allowable range specified for that trait in config/traits.yml. Sometimes the AusTraits range is too narrow and other times the author’s value is truly an outlier that should be excluded. Look closely at these and adjust the range in config/traits.yml if justified. Generally, don’t change the range until you’ve create a report for the study and confirmed that the general cloud of data aligns with other studies as excepted. Most frequently the units or unit conversion is what is incorrect.\n\nYou can also ask how many of each error type are present for a study:\n\naustraits$excluded_data %&gt;%\n  filter(dataset_id == \"Cheal_2017\") %&gt;%\n  pull(error) %&gt;%\n  table()\n#&gt; &lt; table of extent 0 &gt;\n\nOr produce a table of error type by trait:\n\naustraits$excluded_data %&gt;%\n  filter(\n    dataset_id == \"Cheal_2017\",\n  ) %&gt;%\n  select(trait_name, error) %&gt;%\n  table()\n#&gt; &lt; table of extent 0 x 0 &gt;\n\nNote, most studies have no excluded data. This study is an extreme example!"
  },
  {
    "objectID": "adding_data_long.html#adding_many_substitutions",
    "href": "adding_data_long.html#adding_many_substitutions",
    "title": "17  Adding datasets",
    "section": "20.5 Adding many substitutions",
    "text": "20.5 Adding many substitutions\nFor categorical traits, if you want to create a list of all values that require substitutions:\naustraits$excluded_data %&gt;%\n  filter(\n    dataset_id == current_study,\n    error == \"Unsupported trait value\"\n  ) %&gt;%\n  distinct(dataset_id, trait_name, value) %&gt;%\n  rename(\"find\" = \"value\") %&gt;%\n  select(-dataset_id) %&gt;%\n  write_csv(\"data/dataset_id/raw/substitutions_required.csv\")\nFor studies with a small number of substitutions, add them individually using:\nmetadata_add_substitution(dataset_id, trait_name, find, replace)\nFor studies with large number of substitutions required, you can add an additional column to this table, replace, and fill in all the correct trait values. Then read the list of substitutions directly into the metadata file:\nsubstitutions_to_add &lt;-\n  read_csv(\"data/dataset_id/raw/substitutions_required_after_editing.csv\")\n\nmetadata_add_substitutions_list(dataset_id, substitutions_to_add)"
  },
  {
    "objectID": "adding_data_long.html#add-taxonomic-updates",
    "href": "adding_data_long.html#add-taxonomic-updates",
    "title": "17  Adding datasets",
    "section": "20.6 Add taxonomic updates",
    "text": "20.6 Add taxonomic updates\nThe function add_taxonomic_updates allows you to manually align submitted taxon names (the original_name) with the taxon names in the taxonomic resource.\n  metadata_add_taxonomic_change &lt;- function(dataset_id, find, replace, reason, taxonomic_resolution)\n\nfind is the name in the taxon name column in the dataset\nreplace is the equivalent taxon name in the taxonomic resource\nreason provides information about why the taxonomic update is required\ntaxonomic_resolution indicates the most specific taxon rank that the name in replace aligns to\n\nAs examples:\nA simple fix correcting a minor typo to align with an accepted taxon name:\ntaxonomic_updates:\n- find: Drummondita rubroviridis\n  replace: Drummondita rubriviridis\n  reason: match_07_fuzzy. Fuzzy alignment with accepted canonical name in APC (2022-11-21)\n  taxonomic_resolution: Species\nAn example of a taxon name that can only be aligned to genus. The taxonomic_resolution is therefore specified as genus. The portion of the name that can be aligned to the taxonomic resource must be before the square brackets. Any information within the square brackets is important for uniquely identifying this entry within AusTraits, but does not provide additional taxonomic information.\n- find: Acacia ancistrophylla/sclerophylla\n  replace: Acacia sp. [Acacia ancistrophylla/sclerophylla; White_2020]\n  reason: match_04. Rewording taxon where `/` indicates uncertain species identification\n    to align with `APC accepted` genus (2022-11-10)\n  taxonomic_resolution: genus\nA taxonomic update that aligns a name to the most similar taxon_name within a taxonomic resource (the APC), but this is a taxonomic synonym and the austraits workflow will update it to its currently accepted name (since this is documented within the taxon_list.csv file):\n- find: Polyalthia (Wyvur)\n  replace: Polyalthia sp. (Wyvuri B.P.Hyland RFK2632)\n  reason: match_15_fuzzy. Fuzzy match alignment with species-level canonical name\n    in `APC known` when everything except first 2 words ignored (2022-11-10)\n  taxonomic_resolution: Species"
  },
  {
    "objectID": "adding_data_long.html#check-if-austraits-pivots-wider",
    "href": "adding_data_long.html#check-if-austraits-pivots-wider",
    "title": "17  Adding datasets",
    "section": "20.7 Check if AusTraits pivots wider",
    "text": "20.7 Check if AusTraits pivots wider\nAusTraits users want to be able to “pivot” between long and wide formats. Each row of data should have a unique combination of the following fields: trait_name, dataset_id, observation_id, source_id, taxon_name, population_id, individual_id, temporal_context_id, method_id, value_type, and original_name\nTherefore, the dataset should be able to pivot wider and the following code should have a 1 in every cell.\naustraits$traits %&gt;%\n  select(dataset_id, trait_name, value, observation_id, source_id, taxon_name, population_id, individual_id, temporal_context_id, method_id, value_type, original_name) %&gt;%\n  pivot_wider(names_from = trait_name, values_from = value, values_fn = length) %&gt;% View()\nIf AusTraits fails to pivot_wider, likely problems are: - Not all context information has been captured. For instance, is it possible that you have two columns with data for the same trait, measured using different methods? In this case you need to add a method_context to both the relevant traits and to the contexts section. - There are multiple observations per entity. In a number of large studies which, in theory, include a single observation per species, have a few scattered instances of a second row of trait values with the same taxon name. They might be true duplicates and can be removed or perhaps they are indeed some alternate values. In this case the following custom_R_code works:\n' data %&gt;%\n    group_by(taxon_name) %&gt;%\n      mutate(observation_number = dplyr::row_number()) %&gt;%\n    ungroup()'\nThen add observation_number as a context with category: temporal_context"
  },
  {
    "objectID": "adding_data_long.html#check-for-duplicates",
    "href": "adding_data_long.html#check-for-duplicates",
    "title": "17  Adding datasets",
    "section": "20.8 Check for duplicates",
    "text": "20.8 Check for duplicates\nAusTraits strives to have no duplicate entries for numeric (continuous) trait measurements. That is, each value in AusTraits should represent a unique measurement, rather than a measurement sourced from another study.\nWhen you receive/solicit a dataset, ask the data contributor if all data submitted was collected for the specific study and if they suspect other studies from their lab/colleagues may also have contributed any of this data.\nIn addition, there are tests to check for duplicates within and across dataset_ids.\nTo check for duplicates:\naustraits_deduped &lt;- remove_suspected_duplicates(austraits)\nduplicates_for_dataset_id &lt;-\n  austraits_deduped$excluded_data %&gt;%\n  filter(\n    dataset_id == current_study\n  )\n\nDuplicates within the study\n\nFirst sort duplicates_for_dataset_id by the column error and scan for duplicates within the study (these will be entries under error that begin with the same dataset_id as the dataset being processed)\nFor legitimately identical measurements, do nothing. For instance, if %N has been measured on 50 replicates of a species and is reported to the nearest 0.01% it is quite likely there will be a few identical values within the study.\nIf a species-level measurement has been entered for all within-location replicates, you need to filter out the duplicates. This is true for both numeric and categorical values. Enter the following code as custom_R_code in the dataset’s metadata file:\n\ndata %&gt;%\n  group_by(Species) %&gt;%\n  mutate(\n    across(\n      c(leaf_percentN, `plant growth form`), replace_duplicates_with_NA)\n    )\n  ) %&gt;%\n  ungroup()\nNote: Using custom R code instead of filtering the values in the data.csv file itself ensures the relevant trait values are still associated with each line of data in the data.csv file, but only read into AusTraits a single time. Note: You would use group_by(Species, Location) if there are unique values at the species x location level.\n\n\nDuplicates between studies\nAusTraits does not attempt to filter out duplicates in categorical traits between studies. The commonly duplicated traits like life_form, plant_growth_form, photosynthetic_pathway, fire_response, etc. are legitimately duplicated and if the occasional study reported a different plant_growth_form or fire_response it would be important to have documented that one trait value was much more common than another. Such categorical trait values may have been sourced from reference material or measured/identified by this research team.\nIdentifying duplicates in numeric traits between studies can be difficult, but it is essential that we attempt to filter out all duplicate occurrences of the same measurement. Some common patterns of duplication include:\n\nFor a single trait, if there are a large number of values duplicated in a specific other dataset_id (i.e. the error repeatedly starts with the same dataset_id), be suspicious. Before contacting the author, check the metadata for the two datasets, especially authors and study locations, to see if it is likely these are data values that have been jointly collected and shared across studies. Similar location names/locations, identical university affiliations, or similar lists of traits being measured are good clues.\nplant_height, leaf_length, leaf_width, seed_length, seed_width and seed_mass are the numeric variables that are most frequently sourced from reference material (e.g. floras, herbarium collections, reference books, Kew seed database, etc.)\nThe following datasets are flagged in AusTraits as reference studies and are the source of most duplicates for the variables listed above: Kew_2019_1, Kew_2019_2, Kew_2019_3, Kew_2019_4, Kew_2019_5, Kew_2019_6, ANBG_2019, GrassBase_2014, CPBR_2002, NTH_2014,RBGK_2014, NHNSW_2016, RBGSYD__2014_2, RBGSYD_2014, TMAG_2009, WAH_1998, WAH_2016,Brock_1993, Barlow_1981, Hyland_2003, Cooper_2013\n\nData from these studies are assumed to be the source, and the other study with the value is assumed to have sourced it from the above study. We recognise this is not always accurate, especially for compilations within Kew_2019_1, Kew’s seed mass database. Whenever we input a raw dataset that is also part of the Kew compilation, we filter that contributors data from Kew_2019_1.\n\nData for wood_density is also often sourced from other studies, most commonly Ilic_2000 or Zanne_2009.\nData from a number of studies from Leishman and Wright have been extensively shared within the trait ecology community, especially through TRY\n\nIf the dataset you are processing has a number of numeric trait duplicates that follow one of the patterns of duplication listed, the duplicates should be filtered out. Any other data explicitly indicated in the manuscript as sourced should also be filtered out. Most difficult are studies that have partially sourced data, often from many small studies, and partially collected new data, but not identified the source of each value.\nFiltering duplicate data is a three-step process. In brief:\n\nIdentify traits and studies with duplicates you believe should be removed.\nAdd additional columns to data.csv, identifying certain trait_values as duplicates.\nAdd custom R code that filters out identified duplicates when the study is merged into AusTraits.\n\n\nIdentify traits and studies\n\nEither in R or Excel, manipulate duplicates_for_dataset_id to remove rows that you believe are legitimate duplicates, including duplicates values due to replicate measurements within a single study and stray duplicates across studies that likely true, incidental duplicate values. Carefully consider which datasets and traits to include/exclude from the filter.\n\nAs an example:\n# Note, this code will be replaced by a function in the future.\nduplicates_to_filter &lt;-\n  duplicates_for_dataset_id %&gt;%\n  mutate(\n    dataset_with_duplicate =\n      error %&gt;%\n        gsub(\"Duplicate of \", \"\", .) %&gt;%\n        gsub(\"[[:alnum:]]$\", \"\", .) %&gt;%\n        gsub(\"[[:punct:]]$\", \"\", .)\n  ) %&gt;%\n  filter(dataset_with_duplicate %in% c(\"Ilic_2000\", \"Zanne_2009\", \"Kew_2019_1\", \"Barlow_1981\", \"NTH_2014\")) %&gt;%\n  filter(trait_name %in% c(\"wood_density\", \"seed_mass\", \"leaf_length\", \"leaf_width\"))\n\nUse the following code to add columns to data.csv that identify specific values as duplicates:\n\n# Note, this code will be replaced by a function in the future.\nwood_density_duplicates &lt;-\n  duplicates_to_filter %&gt;%\n  filter(trait_name == \"wood_density\") %&gt;%\n  select(error, original_name) %&gt;%\n  rename(\"wood_density_duplicate\" = \"error\")\n\nseed_mass_duplicates &lt;-\n  duplicates_to_filter %&gt;%\n  filter(trait_name == \"seed_width\") %&gt;%\n  select(error, original_name) %&gt;%\n  rename(\"seed_mass_duplicate\" = \"error\")\n\nleaf_width_min_duplicates &lt;-\n  duplicates_to_filter %&gt;%\n  filter(trait_name == \"leaf_width\", value_type == \"expert_min\") %&gt;%\n  select(error, original_name) %&gt;%\n  rename(\"leaf_width_min_duplicate\" = \"error\")\n\nleaf_width_max_duplicates &lt;-\n  duplicates_to_filter %&gt;%\n  filter(trait_name == \"leaf_width\", value_type == \"expert_max\") %&gt;%\n  select(error, original_name) %&gt;%\n  rename(\"leaf_width_max_duplicate\" = \"error\")\n\nleaf_length_min_duplicates &lt;-\n  duplicates_to_filter %&gt;%\n  filter(trait_name == \"leaf_length\", value_type == \"expert_min\") %&gt;%\n  select(error, original_name) %&gt;%\n  rename(\"leaf_length_min_duplicate\" = \"error\")\n\nleaf_length_max_duplicates &lt;-\n  duplicates_to_filter %&gt;%\n  filter(trait_name == \"leaf_length\", value_type == \"expert_max\") %&gt;%\n  select(error, original_name) %&gt;%\n  rename(\"leaf_length_max_duplicate\" = \"error\")\n\nread_csv(\"data/dataset_id/data.csv\") %&gt;%\n  left_join(wood_density_duplicates, by = c(\"column_with_taxon_name\" = \"original_name\")) %&gt;%\n  left_join(seed_mass_duplicates, by = c(\"column_with_taxon_name\" = \"original_name\")) %&gt;%\n  left_join(leaf_width_min_duplicates, by = c(\"column_with_taxon_name\" = \"original_name\")) %&gt;%\n  left_join(leaf_width_max_duplicates, by = c(\"column_with_taxon_name\" = \"original_name\")) %&gt;%\n  left_join(leaf_length_min_duplicates, by = c(\"column_with_taxon_name\" = \"original_name\")) %&gt;%\n  left_join(leaf_length_max_duplicates, by = c(\"column_with_taxon_name\" = \"original_name\")) %&gt;%\n  write_csv(\"data/dataset_id/data.csv\")\n\nFor the above example, then add the following code to custom R code, removing the duplicate values from the data columns (by setting them as NA) as the dataset is read into AusTraits.\n\ndata %&gt;%\n  mutate(\n    `wood density` = ifelse(is.na(wood_density_duplicate), `wood density`, NA),\n    `seed mass (mg)` = ifelse(is.na(seed_mass_duplicate), `seed mass (mg)`, NA),\n    `leaf width minimum (mm)` = ifelse(is.na(leaf_width_min_duplicate), `leaf width minimum (mm)`, NA),\n    `leaf width maximum (mm)` = ifelse(is.na(leaf_width_max_duplicate), `leaf width maximum (mm)`, NA),\n    `leaf length minimum (mm)` = ifelse(is.na(leaf_length_min_duplicate), `leaf length minimum (mm)`, NA),\n    `leaf length maximum (mm)` = ifelse(is.na(leaf_length_max_duplicate), `leaf length maximum (mm)`, NA)\n  )\nDifficulties:\n\nThis method only identifies values as duplicates if they have the same number of significant figures.\nMore complex matching may reveal further duplicates. For seed mass in particular, some studies likely source values from the Kew database and then round these values. They may similarly source several values from Kew and then include the mean in their dataset. If their methods or correspondence with the contributor suggests the values were sourced from Kew (or another lab, papers, etc.) it is best to filter out all values, EXCEPT species that are not yet represented in AusTraits for the trait in question."
  },
  {
    "objectID": "adding_data_long.html#build-study-report",
    "href": "adding_data_long.html#build-study-report",
    "title": "17  Adding datasets",
    "section": "20.9 Build study report",
    "text": "20.9 Build study report\nThe report is located in the export folder.\nCheck the study report to ensure:\n\nAll possible metadata fields were filled in\nThe locations plot sensibly on the map of Australia\nFor numeric traits, the trait values plot sensibly relative to other studies\nThe list of unknown/unmatched species doesn’t include names you think should be recognised/aligned\n\nIf necessary, cycle back through earlier steps to fix any errors, rebuilding the study report as necessary\nAt the very end, re-clear formatting, re-run tests, rebuild AusTraits, rebuild report.\nIf you’re uncertain, also recheck excluded data and duplicates before these final steps.\nf &lt;- file.path(\"data\", current_study, \"metadata.yml\")\nread_metadata(f) %&gt;% write_metadata(f)\n\ndataset_ids &lt;- current_study\naustraits_run_tests()\n\naustraits &lt;- remake::make(\"austraits\")\ndataset_report(current_study, overwrite = TRUE)\nTo generate a report for a collection of studies:\ndataset_reports(c(\"Falster_2005_1\", \"Wright_2002\"), overwrite = TRUE)\nOr for all studies:\ndataset_reports(overwrite = TRUE)\nAdd the argument overwrite=TRUE if you already have a copy of a specific report stored in your computer and want to replace it with a newer version.\n(Reports are written in Rmarkdown and generated via the knitr package. The template is stored in scripts/report_study.html)."
  },
  {
    "objectID": "adding_data_long.html#merging-a-pull-request",
    "href": "adding_data_long.html#merging-a-pull-request",
    "title": "17  Adding datasets",
    "section": "21.1 Merging a pull request",
    "text": "21.1 Merging a pull request\nThere are multiple ways to merge a pull request, including using GitHub’s built-in options for merging and squashing. When merging a PR, we ideally want\n\na single commit\nto attribute the work to the original author\nto run various checks along the way\n\nThere are two ways to do this. For both, you need to be an approved maintainer.\n\nMerging in your own PR\nYou can merge in your own PR after you’ve had someone else review it.\n\nSend the PR\nTag someone to review\nOnce ready, merge into main choosing “Squash & Merge”, using an informative commit message.\n\n\n\nMerging someone else’s PR\nWhen merging in someone else’s PR, the built-in options aren’t ideal, as they either take all of the commits on a branch (ugh, messy), OR make the commit under the name of the person merging the request.\nThe workflow below describes how to merge a pull request from the command line, with a single commit & attributing the work to the original author. Lets assume a branch of name Smith_1995.\nFirst, from the master branch in the repo, run the following:\ngit merge --squash origin/Smith_1995\nThen in R\nNow back in the terminal\ngit add .\ngit commit\nAdd a commit message, referencing relevant pull requests and issues, e.g.\nSmith_1995: Import new data\n\nFor #224, closes #286\nAnd finally, amend the commit author, to reference the person who did all the work!\ngit commit --amend --author \"XXX &lt;XXX@gmail.com&gt;\""
  },
  {
    "objectID": "adding_data_long.html#commit-messages",
    "href": "adding_data_long.html#commit-messages",
    "title": "17  Adding datasets",
    "section": "21.2 Commit messages",
    "text": "21.2 Commit messages\nInformative commit messages are ideal. Where possible, these should reference the issue being addressed. They should clearly describe the work done and value added to AusTraits in a few, clear, bulleted points."
  },
  {
    "objectID": "adding_data_long.html#version-updating-making-a-new-release",
    "href": "adding_data_long.html#version-updating-making-a-new-release",
    "title": "17  Adding datasets",
    "section": "21.3 Version updating & Making a new release",
    "text": "21.3 Version updating & Making a new release\nReleases of the dataset are snapshots that are archived and available for use.\nWe use semantic versioning to label our versions. As discussed in Falster et al 2019, semantic versioning can apply to datasets as well as code.\nThe version number will have 3 components for actual releases, and 4 for development versions. The structure is major.minor.patch.dev, where dev is at least 9000. The dev component provides a visual signal that this is a development version. So, if the current version is 0.9.1.9000, the release be 0.9.2, 0.10.0 or 1.0.0.\nOur approach to incrementing version numbers is\n\nmajor: increment when you make changes to the structure that are likely incompatible with any code written to work with previous versions.\nminor: increment to communicate any changes to the structure that are likely to be compatible with any code written to work with the previous versions (i.e., allows code to run without error). Such changes might involve adding new data within the existing structure, so that the previous dataset version exists as a subset of the new version. For tabular data, this includes adding columns or rows. On the other hand, removing data should constitute a major version because records previously relied on may no longer exist.\npatch: Increment to communicate correction of errors in the actual data, without any changes to the structure. Such changes are unlikely to break or change analyses written with the previous version in a substantial way.\n\n\nFigure: Semantic versioning communicates to users the types of changes that have occurred between successive versions of an evolving dataset, using a tri-digit label where increments in a number indicate major, minor, and patch-level changes, respectively. From Falster et al 2019, (CC-BY).\nThe process of making a release is as follows. Note that corresponding releases and versions are needed in both austraits and traits.build:\n\nUpdate the version number in the DESCRIPTION file, using `\nCompile traits.build.\nUpdate the documentation.\nCommit and push to github.\nMake a release on github, adding version number\nPrepare for the next version by updating version numbers."
  },
  {
    "objectID": "adding_data_long.html#extracting-data-from-pdf-tables",
    "href": "adding_data_long.html#extracting-data-from-pdf-tables",
    "title": "17  Adding datasets",
    "section": "21.4 Extracting data from PDF tables",
    "text": "21.4 Extracting data from PDF tables\nIf you encounter a PDF table of data and need to extract values, this can be achieved with the tabula-java tool. There’s actually an R wrapper (called tabulizer), but we haven’t succeeded in getting this running. However, it’s easy enough to run the java tool at the command line on OSX.\n\nDownload latest release of tabula-java and save the file in your path\nRun\n\njava -jar tabula-1.0.3-jar-with-dependencies.jar my_table.pdf -o my_data.csv\nThis should output the data from the table in my_table.pdf into the csv my_data.csv\n\nClean up in Excel. check especially that the locations of white spaces are correct."
  },
  {
    "objectID": "data_common_issues.html#unsupported-trait-values",
    "href": "data_common_issues.html#unsupported-trait-values",
    "title": "18  Common issues",
    "section": "18.1 Unsupported trait values",
    "text": "18.1 Unsupported trait values\nThis error occurs when, for a categorical trait, the value in data.csv is different to the value in the traits dictionary (config/traits.yml).\n\ntable &lt;- my_database$excluded_data %&gt;%\n  filter(dataset_id == current_study) %&gt;%\n  filter(error == \"Unsupported trait value\") %&gt;%\n  select(dataset_id, trait_name, value) %&gt;%\n  distinct()\n\nYou can individually add substitutions to metadata.yml using the function metadata_add_substitution\n\nmetadata_add_substitution(dataset_id = current_study, trait_name = \"plant_growth_form\", find = \"T\", replace = \"tree\")\n\nOr, you can add an additional column to the table output (code above) and read it into metadata.yml using the function metadata_add_substitutions_table\nThe table read in must have the columns dataset_id, trait_name, find, and replace.\nThis is a hypothetical example for a table that contains 5 rows with plant_growth_form value that need updating.\n\ntable &lt;- table %&gt;%\n  rename(find = value) %&gt;%\n  mutate(replace = c(\"tree\", \"mallee\", \"shrub\", \"graminoid\", \"herb\"))\n\nmetadata_add_substitutions_table(table, dataset_id = dataset_id, trait_name = trait_name, find = find, replace = replace)\n\nYou can of course also write the table to a csv file, edit it in Excel or a text editor, then read it back into R.\n\nwrite_csv(table, \"data/dataset_id/raw/substitutions_required.csv\")\n\n...edit outside of R\n\ntable &lt;- read_csv(\"data/dataset_id/raw/substitutions_required.csv\")"
  },
  {
    "objectID": "data_common_issues.html#dataset-cant-pivot-wider",
    "href": "data_common_issues.html#dataset-cant-pivot-wider",
    "title": "18  Common issues",
    "section": "18.2 Dataset can’t pivot wider",
    "text": "18.2 Dataset can’t pivot wider\nIn order to convert a traits.build database into a wide format, the traits.build$traits table must be able to pivot wider. This dataset was unable to pivot, due to duplication in the following rows:\n\nmy_database$traits %&gt;%\n  filter(dataset_id == dataset_ids) %&gt;%\n  select(\n      dplyr::all_of(c(\"dataset_id\", \"trait_name\", \"value\", \"observation_id\", \"source_id\", \"taxon_name\",\n      \"entity_type\", \"life_stage\", \"basis_of_record\", \"value_type\", \"population_id\", \"individual_id\",\n      \"temporal_id\", \"method_id\", \"method_context_id\", \"entity_context_id\", \"original_name\"))\n          )\n  pivot_wider(names_from = trait_name, values_from = value, values_fn = length) %&gt;%\n  pivot_longer(cols = 16:ncol(.)) %&gt;%\n  rename(trait_name = name, number_of_duplicates = value) %&gt;%\n  select(dataset_id, taxon_name, trait_name, number_of_duplicates, observation_id, entity_type, value_type, population_id, everything()) %&gt;%\n  filter(number_of_duplicates &gt; 1)\n\nThere are two likely explanations – and solutions – to this error:\n\nIf your dataset combines individual (or population) level measurements with species-level measurements, the same species-level measurement may be read in many times. To solve this problem, you need to retain only the first instance of each species-level measurement, by including the following custom_R_code, where taxon_name is the column that contains taxon names and column 1, column 2, etc is a vector of the columns with categorical traits that require de-duplicating.\n\n\ndata %&gt;%\n  group_by(taxon_name) %&gt;%\n  mutate(across(c(\"column 1\", \"column 2\", \"column 3\"), replace_duplicates_with_NA))\n  ungroup()\n\n\nRows of data that represent measurements made at different times,\n\n… TBC"
  },
  {
    "objectID": "tutorial_datasets.html",
    "href": "tutorial_datasets.html",
    "title": "19  Tutorial: Adding datasets",
    "section": "",
    "text": "This tutorial gives detailed exmaples of adding datasets to a traits.build database.\nPrior to completing this tutorial, you should have completed the example of running the template example.\nThe tutorial is divided into the following sections:\n\ntutorial 1: Adding a simple dataset.\ntutorial 2: Adding a more complex dataset.\ntutorial 3: Adding contexts and complex units.\ntutorial 4: Additional complexities for adding datasets.\ntutorial 5: Adding datasets with multiple columns for a trait.\ntutorial 6: Adding datasets with repeat measurements (response curves).\ntutorial 7: Adding a long format dataset and mapping in units from a column.\n\nSee also the guide to common issues when adding data."
  },
  {
    "objectID": "tutorial_dataset_1.html#overview",
    "href": "tutorial_dataset_1.html#overview",
    "title": "20  Tutorial 1: Adding a simple dataset",
    "section": "20.1 Overview",
    "text": "20.1 Overview\nThis is the first of five tutorials on adding datasets to your traits.build database. This introduces you to the basic functions, the user input required, and the manual manipulations required to complete the dataset’s metadata file. The next four tutorials introduce you to progressively more complex datasets, functions, and decisions.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the datasets in traits.build-template . Instructions are available at Tutorial: Example compilation.\n\nGoals\n\nLearn how to build a metadata.yml file for a dataset.\nLearn how to merge a new dataset into a traits.build database.\n\n\n\nNew Functions Introduced\n\nmetadata_create_template\nmetadata_add_source_doi\nmetadata_add_locations\nmetadata_add_traits\ndataset_test\nbuild_setup_pipeline"
  },
  {
    "objectID": "tutorial_dataset_1.html#adding-tutorial_dataset_1",
    "href": "tutorial_dataset_1.html#adding-tutorial_dataset_1",
    "title": "20  Tutorial 1: Adding a simple dataset",
    "section": "20.2 Adding tutorial_dataset_1",
    "text": "20.2 Adding tutorial_dataset_1\n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_1 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_1 folder. \nThere is a folder raw nested within the tutorial_dataset_1 folder, that contains one file, notes.txt. \n\n\n\nsource necessary functions\n\nSource the functions in the traits.build package:\n\n\nlibrary(traits.build)\n\n\n\nUse functions to create a metadata.yml file\n\nCreate a metadata template\nAll dataset metadata is documented within a .yml file that also resides within the dataset’s folder.\nA function quickly creates the skeletal metadata.yml file.\n\nmetadata_create_template(\"tutorial_dataset_1\")\n\nThis function cycles through a series of user-input menus, querying about both the data format (long versus wide) and which columns contain which variables (taxon name, location name, individual identifiers, collection date).\n\nThe menus are shown below, with the menu in blue and the appropriate user input in red.\nIs the data long or wide format?\n\n1: Long\n2: Wide\nSelection: 2\nThis dataset is considered wide, because the data for each trait is documented in its own column.\nSelect column for taxon_name\n\n1: Species\n2: site\n3: LMA (mg mm-2)\n4: Leaf nitrogen (mg mg-1)\n5: leaf size (mm2)\n6: latitude (deg)\n7: longitude (deg)\n8: description`\nSelection: 1\nSelect 1 since taxon names are documented in the column Species.\nSelect column for location_name\n1: NA\n2: Species\n3: site\n4: LMA (mg mm-2)\n5: Leaf nitrogen (mg mg-1)\n6: leaf size (mm2)\n7: latitude (deg)\n8: longitude (deg)\n9: description\nSelection: 3\nSelect 3 since location names are documented in the column site.\nSelect column for individual_id\n1: NA\n2: Species\n3: site\n4: LMA (mg mm-2)\n5: Leaf nitrogen (mg mg-1)\n6: leaf size (mm2)\n7: latitude (deg)\n8: longitude (deg)\n9: description\n\nSelection: 1\nThis dataset does not include a column for individual_id, so 1: NA is the appropriate input.\nSelect column for collection_date\n1: NA\n2: Species\n3: site\n4: LMA (mg mm-2)\n5: Leaf nitrogen (mg mg-1)\n6: leaf size (mm2)\n7: latitude (deg)\n8: longitude (deg)\n9: description\nSelection: 1\nThis dataset does not include a column for collection_date, so 1: NA is the appropriate input.\nA follow-up question then allows you to add a fixed collection_date as a range. The information can be manually updated later.\nEnter collection_date range in format ‘2007/2009’: 2002-11/2002-11\n\nA final user prompt asks if, for any traits, a sequence of rows represents repeat observations. \nDo all traits need repeat_measurements_id’s?\n1: Yes 2: No\nThis only occurs if the dataset documents response curve data (e.g. an A-ci or light response curve for plants; or a temperature response curve for animal or plant behaviour) and the answer is almost always no.\n\n2\n\nNavigate to the dataset’s folder to find the metadata.yml file.\n\nOpen this file in Visual Studio Code (or another text-based editor of choice; NOT Word!), so you can see how it is progressively filled in as you work through the next steps.\n\n\n\nPropagate source information into the metadata.yml file\nThis dataset is from a published source with a doi and therefore the source information can be added with a single line of code:\n\nmetadata_add_source_doi(dataset_id = \"tutorial_dataset_1\", doi = \"10.1111/j.0022-0477.2005.00992.x\")\n\nThe following information is automatically propagated into the source field:\n\nprimary: \n  key: Test_1\n  bibtype: Article\n  year: '2005'\n  author: Daniel S. Falster and Mark Westoby\n  journal: Journal of Ecology\n  title: Alternative height strategies among 45 dicot rain forest species from tropical Queensland, Australia\n  volume: '93'\n  number: '3'\n  pages: 521--535\n  doi: 10.1111/j.0022-0477.2005.00992.x\n\nOnce you’ve run this line of code, look at the metadata file to confirm:\n\nthe authors’ names are formatted as first name last name or first initial last name (Daniel S. Falster or D. S. Falster if first names weren’t available)\n\nsequential author’s names are separated by and \nthe article title is in sentence case\n\nthe page numbers are filled in as a range, separated by a double dash (521--535 is correct) \n\nNote, there is also a function metadata_add_source_bibtex if your source information is in this format.\n\n\n\nAdd location details\nLocation data can be automatically propagated into the metadata file if it is available in tabular format. For instance, for this study:\n\nlocations &lt;-\n  read_csv(\"data/tutorial_dataset_1/data.csv\") %&gt;%\n    select(site, description, `latitude (deg)`, `longitude (deg)`) %&gt;%\n    distinct()\n\nYou can then add this location information directly into the metadata file by running:\n\nmetadata_add_locations(dataset_id = \"tutorial_dataset_1\", location_data = locations)\n\nThis leads to the following user prompts:\nSelect column for location_name \n1: site\n2: description\n3: latitude (deg)\n4: longitude (deg)\n\nSelection: 1 \nSelect the same column that you indicated contained location names when you created the metadata template.\nIndicate all columns you wish to keep as distinct location_properties in tutorial_dataset_1 (by number separated by space; e.g. ‘1 2 4’):\n\n1: description\n2: latitude (deg)\n3: longitude (deg)\n\nSelection: 1 2 3\nSelect all columns that include location properties that should be documented within the metadata.yml file. In this case, it is all three columns.\nFollowing locations added to metadata for tutorial_dataset_1: ‘Atherton’, ‘Cape Tribulation’\nwith variables ‘description’, ‘latitude (deg)’, ‘longitude (deg)’\nPlease complete information in data/tutorial_dataset_1/metadata.yml\n\nAll available location data has now been automatically added to the metadata.yml file.\n\nlocations:\n  Atherton:\n    description: Tropical rain forest vegetation\n    latitude (deg): -17.117\n    longitude (deg): 145.65\n  Cape Tribulation:\n    description: Complex mesophyll vine forest in tropical rain forest\n    latitude (deg): -16.1\n    longitude (deg): 145.45\n\n\n\n\nAdd traits\nThe next step is to select which columns in the data.csv file have trait information you want to include in the database.\nThe function metadata_add_traits automatically adds the trait-scaffold to metadata.yml:\n\nmetadata_add_traits(dataset_id = \"tutorial_dataset_1\")\n\nThe user is prompted to select the columns with trait data.\nIndicate all columns you wish to keep as distinct traits in tutorial_dataset_1 (by number separated by space; e.g. ‘1 2 4’):\n\n1: Species\n2: site\n3: LMA (mg mm-2)\n4: Leaf nitrogen (mg mg-1)\n5: leaf size (mm2)\n6: latitude (deg)\n7: longitude (deg)\n8: description\n\nSelection: 3 4 5\n\nYou select columns 3, 4, 5, as these contain trait data.\nFollowing traits added to metadata for tutorial_dataset_1: ‘LMA (mg mm-2)’, ‘Leaf nitrogen (mg mg-1)’, ‘leaf size (mm2)’\nPlease complete information in data/tutorial_dataset_1/metadata.yml\n\nmetadata.yml now includes a framework in which to manually fill in details about each trait:\n\ntraits:\n- var_in: LMA (mg mm-2)  \n  unit_in: unknown  \n  trait_name: unknown  \n  entity_type: unknown  \n  value_type: unknown  \n  basis_of_value: unknown  \n  replicates: unknown  \n  methods: unknown  \n- var_in: Leaf nitrogen (mg mg-1)  \n  unit_in: unknown  \n  trait_name: unknown  \n  entity_type: unknown  \n  value_type: unknown  \n  basis_of_value: unknown  \n  replicates: unknown  \n  methods: unknown  \n- var_in: leaf size (mm2)  \n  unit_in: unknown  \n  trait_name: unknown  \n  entity_type: unknown  \n  value_type: unknown  \n  basis_of_value: unknown  \n  replicates: unknown  \n  methods: unknown  \n\n\n\n\n\nManual filling in of metadata\nThe remaining fields within the metadata.yml file must now be filled in manually.\nThese include:\nthe contributors section\nthe description, basis_of_record, life_stage, sampling_strategy, original_file, and notes under the dataset section\ndetails for each trait, including unit_in, trait_name, entity_type, value_type, basis_of_record, replicates and methods\nThese are all fields that contain the word unknown.\n\nAdding contributors\n\n\n\nContributor field\nInformation to add\n\n\n\n\nlast_name, first_name\nThe contributors first and last names should be available from the source\n\n\nORCID\nContributors are identified by their ORCID, available for most active researches at orcid.org\n\n\naffiliation\nAvailable from the source or the orcid.org website. Use the same syntax for the same affiliation throughout your database.\n\n\nadditional_role\nFor the lead dataset contributor, add the field: additional_role: contact\n\n\n\n\nYou can add multiple data collectors by duplicating the relevant 4 lines of code; see the Adding dataset vignette for protocols on who to add as a data collector.\nThe line assistants: can be deleted if there aren’t any assistants’ names to add.\nAdd yourself as the dataset_curator.\n\n\n\n\nDataset fields\n\nThe file data/tutorial_dataset_1/raw/tutorial_dataset_1_notes.txt indicates how to fill in the unknown dataset fields for this study.\nIn general, the information to fill in these fields should be available from the source (article) or obtained directly from the dataset contributor.\n\n\n\n\n\nDataset field\nInformation to add\n\n\n\n\nbasis_of_record\nSee traits.build_schema for allowable terms.\n\n\nlife_stage\nSee traits.build_schema for allowable terms.\n\n\ndescription\nA 1-2 sentence summary of the dataset. This can generally be formulated by information in the abstract.\n\n\nsampling_stategy\nA description of how sites and sampling protocols were chosen. Can generally be taken verbatim from the methods section of a manuscript.\n\n\noriginal_file\nName of the file submitted by the data contributor and archived in the raw folder.\n\n\nnotes\nnone (or .na) for this study, but any notes added by the data curator about data quality, edits to the data during dataset curation.\n\n\n\n\n\nTrait details\n\ntrait_name\nThe trait_name must match a trait_name within the traits dictionary. For this example:\n\n\n\ncolumn in dataset\ntrait concept\n\n\n\n\nLMA (mg mm-2)\nleaf_mass_per_area\n\n\nLeaf nitrogen (mg mg-1)\nleaf_N_per_dry_mass\n\n\nleaf size (mm2)\nleaf_area\n\n\n\nA dataset curator must be familiar with the likely traits in their discipline to accurately match those in a contributed dataset to traits in the dictionary, and be able to determine if a new trait definition is warranted.\n\n\nunit_in\nUnits are formatted according to the UCUM convention:\n\nunits in the numerator are separated by a ‘.’/\nunits in the denominator are each preceded by a ‘/’./\n“extra” information that is commonly informally included as part of the units for clarity can be included in curly brackets, {}\n\nAs examples:\n\n\n\nunits\nUCUM format\n\n\n\n\nmilligram per square millimetre\nmg/mm2\n\n\nmicromole per square metre second\numol/m2/s\n\n\nmicromole carbon dioxide per square metre second\numol{CO2}/m2/s\n\n\n\nIf the units being read in for a specific trait differ from those defined for the trait in the traits dictionary the trait values are converted using the conversion rules specified in unit_conversions.csv.\n\n\nentity_type, value_type, basis_of_value, replicates, methods\n\n\n\nfield\nvalue for this dataset\ndescription\n\n\n\n\nentity_type\npopulation\nThe entity corresponding to the trait value. Uses a controlled vocabulary. See traits.build_schema for allowable terms.\n\n\nvalue_type\nmean\nThe statistical nature of the trait value. Uses a controlled vocabulary. See traits.build_schema for allowable terms.\n\n\nbasis_of_value\nmeasurement\nHow the trait value was obtained. See traits.build_schema for allowable terms.\n\n\nreplicates\n3\nThe number of replicate measurements that comprise the trait measurement recorded in the spreadsheet.\n\n\nmethods\nSee the study’s metadata_notes.txt file\nA verbatim (free-form) text field documenting the methods used to collect the trait measurements. This is generally available from the reference or directly from the author.\n\n\n\nThe values for entity_type, value_type, basis_of_value, and replicates can vary by trait – and indeed by measurement – but for this study are identical for all traits.\n\n\n\nFinal steps\n\ndouble check the metadata.yml file\nYou should now have a completed metadata.yml file, with no unknown fields.\nYou’ll notice five sections we haven’t used, contexts, substitutions, taxonomic_updates, exclude_observations, and questions.\nThese should each contain an .na (as in substitutions: .na). They will be explored in future lessons.\n\n\nrun tests on the metadata file\nConfirm there are no errors in the metadata.yml file:\n\ndataset_test(\"tutorial_dataset_1\")\n\nThis should result in the following output:\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 79 ]\n\n\n\nadd dataset to the database\nNext add the dataset_id to the remake file that builds the database and remake the database\n\nbuild_setup_pipeline()\ntraits.build_database &lt;- remake::make(\"austraits\")\n\n\n\nbuild dataset report\nAs a final step, build a report for the study\n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_1\", traits.build_database, overwrite = TRUE)\n\nHave a look at the report, but there reports become much more interesting once there are more datasets in the database."
  },
  {
    "objectID": "tutorial_dataset_2.html#overview",
    "href": "tutorial_dataset_2.html#overview",
    "title": "21  Tutorial 2: Adding a more complex dataset",
    "section": "21.1 Overview",
    "text": "21.1 Overview\nThis is the second of five tutorials on adding datasets to your traits.build database.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the datasets in traits.build-template. Instructions are available at Tutorial: Example compilation.\n\n\nGoals\n\nLearn how to merge in location data from a standalone spreadsheet.\nLearn how to add substitutions for categorical trait values.\nLearn how to add custom R code to the metadata file.\nUnderstand the importance of attributing traits to the correct entity_type.\nUnderstand the importance of having the dataset pivot.\n\n\n\nNew Functions Introduced\n\nmetadata_add_substitution\nmetadata_add_substitutions_list\nmetadata_check_custom_R_code"
  },
  {
    "objectID": "tutorial_dataset_2.html#adding-tutorial_dataset_2",
    "href": "tutorial_dataset_2.html#adding-tutorial_dataset_2",
    "title": "21  Tutorial 2: Adding a more complex dataset",
    "section": "21.2 Adding tutorial_dataset_2",
    "text": "21.2 Adding tutorial_dataset_2\n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_2 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_2 folder. \nThere is a folder raw nested within the tutorial_dataset_2 folder, that contains two files, locations.csv and notes.txt. \n\n\n\nsource necessary functions\n\nYou’ll need to both source the traits.build functions and some ancillary functions that are in a file in the scripts folder:\n\n\nlibrary(traits.build)\nsource(\"R/custom_R_code.R\")\n\n\n\nUse functions to create a metadata.yml file\n\nCreate a metadata template\nTo create the metadata template, run:\n\nmetadata_create_template(\"tutorial_dataset_2\")\n\nAs with tutorial_dataset_1 this function leads you through a series of menus requiring user input. Ensure you select:\ndata format: wide\ntaxon_name column: name_original\nlocation_name column: site_TEXT\nindividual_id column: 1: NA\ncollection_date column: 1: NA\nEnter collection_date range in format ‘2007/2009’: 1996/1997  Do all traits need repeat_measurements_id’s? 2: No\n\nNavigate to the dataset’s folder and open the metadata.yml file in Visual Studio Code, to ensure information is added to the expected sections as you work through the tutorial.\n\n\n\nPropagate source information into the metadata.yml file\nThis dataset is from a published source and therefore the source information can be added with the function metadata_add_source_doi:\n\nmetadata_add_source_doi(dataset_id = \"tutorial_dataset_2\", doi = \"10.1046/j.1365-2745.2000.00506.x\")\n\nconfirm:\n\nthe authors’ names are formatted as first name last name or first initial last name\n\nthe article title is in sentence case\n\nthe page numbers are filled in as a range, separated by a double dash\n\n\n\n\n\nAdd location details\nFor this dataset, location data is provided as a standalone spreadsheet, located in the raw data folder: tutorial_dataset_2\\raw\\locations.csv\nFirst read the location data provided into R:\n\nlocations &lt;-\n  read_csv(\"data/tutorial_dataset_2/raw/location_data.csv\")\n\ntraits.build requires three fields to use a specific syntax:\n\nlatitude must be in decimal degrees and the field name (column header) must be latitude (deg)\nlongitude must be in decimal degrees and the field name (column header) must be longitude (deg)\nA general site description is document in the field description\n\ntraits.build does not require that the labels for other location properties align across datasets, but it is best practice to use a controlled vocabulary, so database users can easily search across all datasets for information on a specific climate variable or soil nutrient content. For new databases or new location properties, any naming/labeling convention can be established.\nTo confirm you are using the correct syntax, check the terms already in use:\n\nlocations_properties &lt;-\n  traits.build_database$sites %&gt;%\n  distinct(location_property) %&gt;%\n  View()\n\nThen rename your columns to match those in use:\n\nlocations &lt;-\n  locations %&gt;%\n    rename(\n      `longitude (deg)` = long,  \n      `latitude (deg)` = lat,  \n      `description` = vegetation,  \n      `elevation (m)` = elevation,  \n      `precipitation, MAP (mm)` = MAP,  \n      `soil P, total (mg/kg)` = `soil P`,  \n      `soil N, total (ppm)` = `soil N`,  \n      `geology (parent material)` = `parent material`  \n    )\n\nNow add the location information into the metadata file:\n\nmetadata_add_locations(dataset_id = \"tutorial_dataset_2\", location_data = locations)\n\nEnsure you select:\nlocation_name: location\nlocation_property columns: 1 2 3 4 5 6 7 8\n\nCheck the metadata.yml file to ensure the location information has been added as expected. If there is a problem, rerun the necessary code; this will overwrite what is present. You can also manually add additional properties if something is forgotten.\n\n\n\nAdd traits\nTo select columns in the data.csv file that include trait data, run:\n\nmetadata_add_traits(dataset_id = \"tutorial_dataset_2\")\n\nSelect columns 3 4 5 6, as these contain trait data.\n\n\n\n\nManual filling in of metadata\nAfter confirming that the skeletal traits section has been added to metadata.yml file, you must fill in all the unknown fields.\nFor this dataset, you will later use functions to add substitutions and exclude unwanted observations, but it is best to first fill in the information for contributors, the dataset, and the traits.\nThese are all fields that contain the word unknown and must be filled in manually: \n\nthe contributors section\n\ndescription, basis_of_record, life_stage, sampling_strategy, original_file, and notes under the dataset section\n\ndetails for each trait, including unit_in, trait_name, entity_type, value_type, basis_of_record, replicates and methods\n\n\n\nAdding contributors\nThe file data/tutorial_dataset_2/raw/tutorial_dataset_2_notes.txt indicates the main data_contributor for this study.\n\nFill in the remaining contributor information as described in the tutorial_dataset_1tutorial.\n\n\n\nDataset fields\nThe file data/tutorial_dataset_2/raw/tutorial_dataset_2_notes.txt indicates how to fill in the unknown dataset fields for this study.\n\n\nTrait details\nThe file data/tutorial_dataset_2/raw/tutorial_dataset_2_notes.txt indicates how to fill in the unknown trait fields for this study, but see below as well.\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn in dataset\ntrait concept\nunits_in\nentity_type\nvalue_type\nbasis_of_value\nreplicates\n\n\n\n\nTRAIT Growth Form CATEGORICAL EP epiphyte (mistletoe) F fern G grass H herb S shrub T tree V vine\nplant_growth_form\n.na\nspecies\nmode\nexpert_score\n.na\n\n\nTRAIT SLA UNITS mm2/g\nleaf_mass_per_area\nmm2/g\npopulation\nmean\nmeasurement\n5\n\n\nTRAIT Leaf Size UNITS mm2\nleaf_area\nmm2\npopulation\nmean\nmeasurement\n5\n\n\nTRAIT Leaf Dry Mass UNITS g\nleaf_dry_mass\ng\npopulation\nmean\nmeasurement\n5\n\n\n\nSome notes:\n\nThe trait_name must match a trait concept within the traits dictionary.\nThe second trait in this dataset is documented as specific leaf area, the inverse of the trait concept leaf mass per area. The unit conversions algorithm inverts data read in as specific leaf area, converting it to leaf mass per area.\nCategorical traits do not have units or replicates, so these fields become .na.\nThe traits.build convention for a categorical trait is value_type: mode, indicating the recorded value is the most commonly observed trait value. In some datasets there may be multiple space-delimited values within a single cell in the data.csv file, indicating there are multiple commonly observed categorical trait values.\nFor most observations of categorical traits, the traits.build convention is that the basis_of_value is determined by an expert examining an individual, population or species, and is therefore an expert_score.\n\n\n\nAdditional steps\nOnce you are well-versed in adding datasets to a traits.build database you will know that there is additional information required in metadata.yml.\nHowever, for this tutorial, let’s begin by assuming we’re finished adding dataset metadata and check for errors:\n\ndataset_test(\"tutorial_dataset_2\")\n\n*Users, please note, not all of these test failures or messages are currently in place. Adding test to document all failures is still a work in progress. \nThree items will fail: \n\nThere are unknown trait values for plant_growth_form (error doesn’t yet exist) \nThere are values out of range for leaf_dry_mass (error doesn’t yet exist) \nThe dataset cannot pivot between long and wide formats. (error doesn’t yet exist) \n\nAs indicated in the output messages, there is a [troubleshooting vignette](https://github…/vignettes/) to help solve these errors. \nFor this tutorial however, keep reading…\nThere are several ways to proceed, but for these errors, it is useful to next build the dataset:\n\nbuild_setup_pipeline()\ntraits.build_database &lt;- remake::make(\"austraits\")\n\nIf you look at the excluded_data table, you’ll find any data for this dataset that could not be mapped to known traits, known trait values, or fell within allowable ranges.\n\ntraits.build_database$excluded_data %&gt;%\n  filter(dataset_id == \"tutorial_dataset_2\") %&gt;%\n  View()\n\nThis code displays a table with 190 rows of excluded data. \n\n187 instances of Unsupported trait value for the trait plant_growth_form \n3 instance of Value out of allowable range for the trait leaf_dry_mass \n\nLooking through the output you’ll notice that the Unsupported trait value error exists because the data.csv file used plant growth form values that are different to those in the trait dictionary.\nThe values that triggered the Value out of allowable range error are all 0’s, a disallowed leaf_dry_mass value; the trait dictionary specifies that leaf_dry_mass can range from 0.01 - 15000.0 mg.\n\nAdding trait value substitutions\nFor categorical traits, only trait values that are indicated in the trait dictionary are recognised. This is an important harmonisation step, as it ensures the same trait concept value is mapped to the same trait value throughout the database.\nHowever, researchers use countless synonyms, abbreviations and syntax to express an identical trait value. traits.build converts all input to lowercase, but all other substitutions must be specified in the dataset’s metadata.yml file.\nFor this example individual letters were used to express 7 plant growth forms: EP, F, G, H, S, T, V\nLooking at the definition for plant_growth_form in the trait dictionary and the helpful column header provided by the contributor, you can deduce that t is for tree; s is for shrub, etc.\nThere are two ways to add substitutions into the metadata.yml file.\n\nMap in individual trait value substitutions using metadata_add_substitution:\n\n\nmetadata_add_substitution(dataset_id = \"tutorial_dataset_2\", trait_name = \"plant_growth_form\", find = \"t\", replace = \"tree\")\n\nLook at the metadata.yml file and you’ll note that a substitution has been added, to indicate the t’s are tree’s\nYou would repeat this step for the remaining unknown trait values.\n\nMap in a table of substitutions using metadata_add_substitutions_list:\n\n\nIf there are quite a few trait values that require replacements, it is easier to first create a table of the required substitutions, then add a column of substitutions in either R or Excel.\n\n\ntable &lt;-\n  traits.build_database$excluded_data %&gt;%\n  filter(\n    dataset_id == \"tutorial_dataset_2\" &\n      error == \"Unsupported trait value\"\n  ) %&gt;%\n  distinct(trait_name, value) %&gt;%\n  rename(find = value)\n\nNext view your table to check the order of trait values and check the allowed values and definitions in the trait dictionary to ensure you replace each abbreviation with an accepted value. Note that epiphyte is not an allowed value for plant_growth_form in the trait dictionary, as, AusTraits uses a narrow definition of plant_growth_form, and separately has a trait plant_growth_substrate which includes the trait value epiphyte. For now, we’ll simply ignore this data.\nTo add a column with substitutions, then add the substitutions to the metadata file:\n\ntable &lt;- table %&gt;%\n  mutate(replace = c(\"shrub\", \"tree\", \"herb\", NA, \"graminoid\", \"fern\", \"vine\"))\n\n## an alternative is `mutate(replace = c(\"shrub\", \"tree\", \"herb\", \"epiphyte\", \"graminoid\", \"fern\", \"climber_herbaceous\"))` which will result in the epiphyte observations remaining in the excluded data table\n\nmetadata_add_substitutions_list(\"tutorial_dataset_2\", table)\n\nAll required substitutions have been added to the metadata.yml file.\nIf you were to rerun dataset_test(\"tutorial_dataset_2\") the error referring to Unsupported trait values would now have vanished.\n\n\nReplacing “placeholder characters” with NA’s\nThe Values out of range error is triggered for numeric traits when the traits.build pipeline detects values that fall outside the range specified for the trait in the traits dictionary.\nThere are three common situations that lead to this error warning: \n\nValues are truly out of range, possibly due to human error or because a plant really was not performing as expected (i.e. not photosynthesising). \nValues appear to be out of range because of a “unit conversion issue” - that is, the dataset curator or the dataset contributor got the units wrong.  This is fixed by working out the correct units and adjusting this in the traits section of the metadata.yml file. \nA dataset contributor has used a “dummy symbol” to indicate missing data, such as 0, x, missing, etc. Truly missing data should be a blank cell - i.e. NA \n\nThis study is likely an example of (3), where 0 is a placeholder symbol. While the 0’s can be left in the excluded_data table, cluttering the excluded_data table with extraneous measurements makes it difficult to scan for true examples of Value out of allowable range errors in the future. (Values that are NA are, by default, omitted from the excluded_data table.)\n\nadding custom R code\nInstead, you can add R code within the metadata file to replace the placeholder symbol with NA.\n\nLook at metadata.yml in Visual Studio Code. \nThe second field under the dataset section is custom_R_code: .na. \nYou can write any code you’d like within this section. \nThe file R/custom_R_code.R that you sourced at the beginning of this tutorial contains customised functions commonly used in custom_R_code. \n\nFor this example, replace: \n\ncustom_R_code: na\n\nwith: \n\ncustom_R_code: '\n  data %&gt;%\n    mutate(\n      across(c(\"TRAIT Leaf Dry Mass UNITS g\"), ~na_if(.x,0))\n    )\n'\n\nThis code replaces all 0’s in the column with NA’s.\nYou can confirm that the custom R code has made the anticipated change with the function metadata_check_custom_R_code. This function reads in the data.csv file, then applies any manipulations from the custom_R_code: \n\nmetadata_check_custom_R_code(\"tutorial_dataset_2\") %&gt;% View()\n\nNote: \n\nuse the format with the single quotes; this allows you to manually add line breaks, not otherwise permitted in the metadata.yml format. \nyou pipe in data to begin with, but do not need to assign your code back to data; that occurs automatically. \n\nBuild the database again, then check the excluded_data table to confirm there are no longer any excluded measurements: \n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\ntraits.build_database$excluded_data %&gt;%\n  filter(dataset_id == \"tutorial_dataset_2\") %&gt;%\n  View()\n\n\n\n\nReplacing duplicate values with NA’s\nRun the tests again to confirm the errors related to disallowed trait values and values out of range have vanished: \n\ndataset_test(\"tutorial_dataset_2\")\n\nHowever, there should still be an error indicating that the dataset cannot pivot between long and wide formats. \nThe ability to pivot is important for 2 reasons:{#dataset_pivot} \n\nDatabase users may prefer to display data in wide format to readily compare the values of multiple traits collected on the same individual (or population or species). \nThe pivot test groups together 13 variables that are meant to uniquely identify each row of data (dataset_id, trait_name, observation_id, source_id, taxon_name, entity_type, life_stage, basis_of_record, value_type, population_id, individual_id, temporal_id, method_id, entity_context_id, original_name). An inability to pivot indicates either: \n\nA variable present in the data.csv file to distinguish between unique observations has not been mapped into the metadata file. (Most likely a context property, column with locations, individual_id or source_id) \nDuplicate values exist within the data.csv file and have been read in multiple times. \n\n\nThe error in this dataset is a common one:\n\n\nThe three numeric traits (leaf_mass_per_area, leaf_mass_per_area and leaf_dry_mass) are all population-level measurements, while plant_growth_form is mapped in as having entity_type: species, meaning it is considered a species-level measurement. This means if the same species occurs at multiple sites, its growth form value is read in twice. However, because it is designated as entity_type: species that traits.build workflow does not connect the value to a location, since the species has the same growth form regardless of location. \nTwo options are to:\n\nRecategorise plant_growth_form as having entity_type: population.\nOnly read in a single instance of plant_growth_form per species.\n\n\nEither allows the dataset to pivot, but if the taxon truly displays only a single growth form across all populations it is much better to read in plant growth form once per species. Otherwise the database becomes longer without capturing additional information. This dataset has only a few instance of duplication, but imagine the dataset that has 500 rows of data for the same tree species - suddenly 500 instances of that species being a tree are read into the database.\nThe solution is to modify the existing custom_R_code, adding one of the customised functions from the file R/custom_R_code.R, replace_duplicates_with_NA: \n\ncustom_R_code: '\n  data %&gt;%\n    mutate(\n      across(c(\"TRAIT Leaf Dry Mass UNITS g\"), ~na_if(.x,0))\n    ) %&gt;%\n    group_by(name_original) %&gt;% #column with taxon name\n      across(c(\"TRAIT Growth Form CATEGORICAL EP epiphyte (mistletoe) F fern G grass H herb S shrub T tree V vine\"), replace_duplicates_with_NA) %&gt;%\n    ungroup()\n'\n\nRerun the tests and everything should now pass: \n\ndataset_test(\"tutorial_dataset_2\")\n\nThen rebuild the database and look at the output in the traits table for one of the taxa that previously had duplicate plant_growth_form entries: \n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\ntraits.build_database$traits %&gt;%\n  filter(dataset_id == \"tutorial_dataset_2\") %&gt;% \n  filter(taxon_name == \"Actinotus minor\") %&gt;% View()\n\n  dataset_id     taxon_name      observation_id trait_name         value            unit  entity_type location_id\n  &lt;chr&gt;          &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      \n1 tutorial_dataset_2 Actinotus minor 010            leaf_area          18.8             mm2   population  02         \n2 tutorial_dataset_2 Actinotus minor 010            leaf_dry_mass      7                mg    population  02         \n3 tutorial_dataset_2 Actinotus minor 010            leaf_mass_per_area 344.827586206897 g/m2  population  02         \n4 tutorial_dataset_2 Actinotus minor 011            leaf_area          75.9             mm2   population  03         \n5 tutorial_dataset_2 Actinotus minor 011            leaf_dry_mass      7                mg    population  03         \n6 tutorial_dataset_2 Actinotus minor 011            leaf_mass_per_area 89.2857142857143 g/m2  population  03         \n7 tutorial_dataset_2 Actinotus minor 012            plant_growth_form  herb             NA    species     NA      \n\nThe measurements for the three numeric traits from a single location share a common observation_id, as they are all part of an observation of a common entity (a specific population of Actinotus minor), at a single location, at a single point in time. However the row with the plant growth form measurement has a separate observation_id reflecting that this is an observation of a different entity (the taxon Actinotus minor).\n\n\nbuild dataset report\nAs a final step, build a report for the study\n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_2\", traits.build_database, overwrite = TRUE)"
  },
  {
    "objectID": "tutorial_dataset_3.html#overview",
    "href": "tutorial_dataset_3.html#overview",
    "title": "22  Tutorial 3: Adding contexts and complex units",
    "section": "22.1 Overview",
    "text": "22.1 Overview\nThis is the third of five tutorials on adding datasets to your traits.build database.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the datasets in traits.build-template. Instructions are available at Tutorial: Example compilation.\n\n\nGoals\n\nLearn how to add contexts. \nLearn some complexities with respect to units. \nLearn additional custom_R_code tricks. \n\n\n\nNew Functions Introduced\n\nmetadata_add_contexts"
  },
  {
    "objectID": "tutorial_dataset_3.html#adding-tutorial_dataset_3",
    "href": "tutorial_dataset_3.html#adding-tutorial_dataset_3",
    "title": "22  Tutorial 3: Adding contexts and complex units",
    "section": "22.2 Adding tutorial_dataset_3",
    "text": "22.2 Adding tutorial_dataset_3\n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_3 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_3 folder. \nThere is a folder raw nested within the tutorial_dataset_3 folder, that contains one file, notes.txt. \n\n\n\nsource necessary functions\n\nIf you have restarted R Studio since last adding a dataset, ensure all functions are loaded from both the traits.build package and the custom functions file:\n\n\nlibrary(traits.build)\nsource(\"R/custom_R_code.R\")\n\n\n\nUse functions to create a metadata.yml file\n\nCreate a metadata template\nTo create the metadata template, run:\n\nmetadata_create_template(\"tutorial_dataset_3\")\n\nAs with in the previous tutorials, this function leads you through a series of menus requiring user input. Ensure you select:\ndata format: wide\ntaxon_name column: 1: Species\nlocation_name column: 5: site\nindividual_id column: 1: NA\ncollection_date column: 1: NA\nEnter collection_date range in format ‘2007/2009’: 2011-02/2011-03\nDo all traits need repeat_measurements_id’s? 2: No\n\nIn this dataset, unlike the first two, the data being input is at the individual-level. Since there is only a single data row for each individual, it is not required to map in an individual_id. A column with an individual_id is required if you want to keep track of multiple rows of data for the same individual.\nNavigate to the dataset’s folder and open the metadata.yml file in Visual Studio Code, to ensure information is added to the expected sections as you work through the tutorial.\n\n\n\nPropagate source information into the metadata.yml file\nThis dataset is from a published source and therefore the source information can be added with the function metadata_add_source_doi:\n\nmetadata_add_source_doi(dataset_id = \"tutorial_dataset_3\", doi = \"10.1007/s11104-013-1725-x\")\n\nconfirm:\n\nthe authors’ names are formatted as first name last name or first initial last name\n\nthe article title is in sentence case\n\nthe page numbers are filled in as a range, separated by a double dash\n\n\nYou have just added 3 doi’s that all yield perfect reference information - and indeed most references are added correctly, but some journals and doi’s for many older references are in ALL CAPS or missing page numbers, so it is worth checking.\n\n\n\nAdd location details\nAll data for this dataset was collected at a single location, specified in the data.csv file as The University of Melbourne Burnley campus. No additional details are provided. For such studies, it is best to look up the campus location and input approximate latitude/longitude coordinates.\nAs well as adding locations and location properties from a table, the function metadata_add_locations lets you add a basic location data scaffold in metadata.yml.\nFor instance, for this study:\n\n\nyou add the location names from the data.csv file\n\nthe function automatically adds blank fields for latitude, longitude, and description\n\nvalues for these fields must then be filled in manually\n\n\n\ndata &lt;- read_csv(\"data/tutorial_dataset_3.csv\")\n\nmetadata_add_locations(\"tutorial_dataset_3\", data)\n\nYou select the location name, but not any location properties, as none are provided in the data.csv file or another tabular format.\nlocation_name: 4: site\nlocation_property columns: just press enter\n\nThis creates the following scaffold in methdata.yml: \n\n  The University of Melbourne Burnley campus:\n    latitude (deg): na_character\n    longitude (deg): na_character\n    description: na_character\n\n\nmetadata_add_locations automatically selects the unique values in the location name column. \nif no columns with location properties are specified, the function just adds the three core location properties. \nthe values for these location properties are available in the notes file. \n\n\n\n\nAdd traits\nTo select columns in the data.csv file that include trait data, run:\n\nmetadata_add_traits(dataset_id = \"tutorial_dataset_3\")\n\nSelect columns 5 6 7 8 9 10, as these contain trait data.\n\n\n\nAdd contexts\nA context is any piece of ancillary information that helps explain why a certain trait value was measured. \nIn traits.build, some contexts are mapped in as part of the default metadata structure, including the location (& location properties), a general sense of organism age (life_stage), basis_of_record, and the general methods for each trait.\n\nHowever most contexts are pieces of information that are essential to record for a specific dataset, but not recorded for most other datasets. The context field therefore allows any context property to be added manually.\n\nContext properties are divided into 5 categories:\n\n\nmethod contexts: Context properties that capture differences in method between measurements of the same trait. For plants, canopy position and leaf age are two common method contexts.\n\ntemporal contexts: Context properties that capture explicit time-related differences between groups of measurements. This is separate from collection_date, as an explicit meaning should accompany each temporal context property and the distinct values may span a range of collection dates. For plants sampling season (dry versus wet) is a commonly mapped in temporal context.\n\nentity contexts: This context property category pertains to individual-level measurements, and documents features of the individual that explicitly distinguish it from other individuals that are measured. In addition to features like the sex of an individual, it is the location to document individual-level co-variates that are not themselves traits, but are information required to interpret other trait values.\n\ntreatment contexts: Any experimental treatment that has been applied to groups of individuals.\n\nplot contexts: Any variation within a documented location, where different individuals experience know differences in growing/living conditions or growing/living history. For plants, this context category is frequently used to map in slope position or fire history.\n\n\nContext properties are most frequently included in the data.csv file as columns of values. Occasionally, separate columns of trait values might represent measurements with different context property values, a topic for a later tutorial.\nContext properties that are columns in the data file, can be added with the function metadata_add_contexts:\n\nmetadata_add_contexts(\"tutorial_dataset_3\")\n\nThis leads to a user-prompt to select the relevent columns:\nIndicate all columns that contain additional contextual data for tutorial_dataset_3 (by number separated by space; e.g. ‘1 2 4’):\n\n1: Species\n2: Treatment\n3: Replicate\n4: site\n5: life_form\n6: WP leaf (Mpa) predawn\n7: WP leaf (Mpa) midday\n8: LMA kg/m2\n9: Stomatal density Upper surface\n10: Stomatal density Lower surface\n\nSelect column 2 which is the only column with a context property:\nSelection: 2\n\nAdditional user prompts ask for details about the context property category and values:\nWhat category does context Treatment fit in? (by number separated by space; e.g. ‘1 2 4’):\n\n1: treatment_context\n2: plot_context\n3: temporal_context\n4: method_context\n5: entity_context\n\nThis is a treatment context, so select 1:\nSelection: 1\n\nThe following values exist for this context: Drought, Watered.\n\nAre replacement values required? (y/n) y\n\nAlthough the trait values Drought and Watered are probably sufficiently descriptive, for other drought-treatment studies we’ve used drought and well-watered, so prefer to align the context property values with these.\nAre descriptions required? (y/n) y\n\nThe free-form description field let’s you add details about the exact meaning of drought vs well-watered for this study.\nIn the metadata.yml file, there will now be a scaffold for the contexts:\n\ncontexts:\n- context_property: unknown\n  category: treatment\n  var_in: Treatment\n  values:\n  - find: Drought\n    value: unknown\n    description: unknown\n  - find: Watered\n    value: unknown\n    description: unknown\n\nIn addition to filling in the preferred context property values and descriptions, you must also assign a name to the context_property. This is a free-form field, but as with location_property it is best to ensure you align context_propery names throughout the database. In the AusTraits plant trait database, this context_property is always called drought treatment.\nThe finished context section will be:\n\ncontexts:\n- context_property: drought treatment\n  category: treatment\n  var_in: Treatment\n  values:\n  - find: Drought\n    value: drought\n    description: The plants were watered with 20% of the water used by well-watered\n      plants (determined gravimetrically) in the 3-4 days preceding each watering\n      event).\n  - find: Watered\n    value: well-watered\n    description: The plants were watered to pot capacity at (2 L per pot).\n\n\n\n\n\nManual filling in of metadata\nThe components of this dataset that can be propagated with functions are not complete, and the remaining unknown fields must now be filled in manually.\n\n\nthe contributors section\n\ndescription, basis_of_record, life_stage, sampling_strategy, original_file, and notes under the dataset section\n\ndetails for each trait, including unit_in, trait_name, entity_type, value_type, basis_of_record, replicates and methods\n\n\n\nAdding contributors\nThe file data/tutorial_dataset_3/raw/tutorial_dataset_3_notes.txt indicates the main data_contributor for this study.\n\n\n\nDataset fields\nThe file data/tutorial_dataset_3/raw/tutorial_dataset_3_notes.txt indicates how to fill in the unknown dataset fields for this study.\n\n\nTrait details\nThe file data/tutorial_dataset_3/raw/tutorial_dataset_3_notes.txt indicates how to fill in the unknown trait fields for this study, but see below as well.\nRemember, the trait_name must match a trait concept within the traits dictionary. For this example:\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn in dataset\ntrait concept\nunits_in\nentity_type\nvalue_type\nbasis_of_value\nreplicates\n\n\n\n\nlife_form\nlife_form\n.na\nspecies\nmode\nexpert_score\n.na\n\n\nWP leaf (Mpa) predawn\nwater_potential_predawn\nneg_MPa\nindividual\nraw\nmeasurement\n1\n\n\nWP leaf (Mpa) midday\nwater_potential_midday\nneg_MPa\nindividual\nraw\nmeasurement\n1\n\n\nLMA kg/m2\nleaf_mass_per_area\nkg/m2\nindividual\nraw\nmeasurement\n1\n\n\nStomatal density Upper surface\nleaf_stomatal_density_adaxial\n‘{count}/mm2’\nindividual\nraw\nmeasurement\n1\n\n\nStomatal density Lower surface\nleaf_stomatal_density_abaxial\n‘{count}/mm2’\nindividual\nraw\nmeasurement\n1\n\n\n\nWith the units, note:\n\nIn the data.csv file, all water potential values are positive, indicating the data contributor mapped in the “negative” of the true water potential values (which are always below zero). A negative sign at the beginning of the units field is not recognised and therefore the convention is to use the prefix neg_ to indicate the values input are the negative of the true values.\nstomatal density is a “count density”, a number of stomata per unit area. The actual UCUM standard for this is simply 1/mm2 , but for clarity we use {count}/mm2 . The word count is in curly brackets, since it is a “note” rather than a true unit.\nIf the unit begin with a curly bracket, the unit needs to be placed in single quotes\n\n\n\n\nTesting, error fixes, and report building\nAt this point, run the dataset tests and rebuild the dataset:\n\ndataset_test(\"tutorial_dataset_3\")\n\nbuild_setup_pipeline()\n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\nThe dataset test should yield an error that one water_potential_predawn value does not convert to numeric, indicating a placeholder-character is being used in place of an NA {note: this error wasn’t triggering as this vignette was being written)\nLooking at the excluded_data table indicates there is a “*” in one column, so one adds:\n\n  custom_R_code: '\n    data %&gt;%\n      mutate(      \n        across(c(\"WP leaf (Mpa) predawn\"), ~na_if(.x,\"*\"))\n      )\n  '\n\nHowever, now you’ll get the error: Caused by error in na_if(): ! Can’t convert y  to match type of x .\nThis indicates a mismatch between column types, necessitating that you change the column to character:\n\n  custom_R_code: '\n    data %&gt;%\n      mutate(\n        across(c(\"WP leaf (Mpa) predawn\"), ~as.character(.x)), \n        across(c(\"WP leaf (Mpa) predawn\"), ~na_if(.x,\"*\"))\n      )\n  '\n\nAt this point, rerunning the tests and rebuilding the database should not generate any errors or excluded values, so you can build and review the report.\nAs a final step, build a report for the study\n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_3\", traits.build_database, overwrite = TRUE)"
  },
  {
    "objectID": "tutorial_dataset_4.html#overview",
    "href": "tutorial_dataset_4.html#overview",
    "title": "23  Tutorial 4: Additional complexities",
    "section": "23.1 Overview",
    "text": "23.1 Overview\nThis is the fourth tutorial on adding datasets to your traits.build database.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the example datasets in traits.build-template. Instructions are available at Tutorial: Example compilation.\n\n\nGoals\n\nLearn how to generate location names using custom_R_code\nLearn how to add multiple sources to the reference section. \nLearn how to add map source_ID into the metadata file. \nLearn how to map metadata from columns. \nLearn how to exclude data. \n\n\n\nNew Functions Introduced\n\nnone."
  },
  {
    "objectID": "tutorial_dataset_4.html#adding-tutorial_dataset_4",
    "href": "tutorial_dataset_4.html#adding-tutorial_dataset_4",
    "title": "23  Tutorial 4: Additional complexities",
    "section": "23.2 Adding tutorial_dataset_4",
    "text": "23.2 Adding tutorial_dataset_4\nThis dataset is a subset of data from Togashi_2015 in AusTraits. The data are a compilation of many datasets, each with their own references information.\n\nFor such datasets, there are two options: 1) sourcing the data from the original publication, and adding it to the database as its own datset; 2) entering the dataset as part of a broader compilation. For this study, in AusTraits, a combination of both approaches was used. For original sources that included a broader range of traits and similar (or better) resolution, the original source was used. However, in this compilation there were several studies where Togashi had already individually contacted the authors of the source publications, as the data appendix for this publication often had better resolution than the original papers. \nThis tutorial focuses on adding a single dataset derived from many original studies. \nBefore you begin creating the metadata file, take a look at the data.csv file - you’ll notice that the columns present are quite different from the datasets added so far. There are columns for latitude & longitude, but not location name column. There are columns for the original source. These are columns for entity_type, basis_of_record, and value_type, three metadata fields that, for the previous tutorial datasets, were entered in the metadata file as a fixed value. And the only trait column log_LA.SA is in a non-standard format.\nWith a few small tricks this dataset can also be added seamlessly.\n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_4 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_4 folder. \nThere is a folder raw nested within the tutorial_dataset_4 folder, that contains one file, tutorial_dataset_4_notes.txt. \n\n\n\nsource necessary functions\n\nIf you have restarted R Studio since last adding a dataset, ensure all functions are loaded from both the traits.build package and the custom functions file:\n\n\nlibrary(traits.build)\nsource(\"R/custom_R_code.R\")\n\n\n\nCreate the metadata.yml file\n\nNote, that because, for this dataset, there are a number of variables that cannot simply be adding using the metadata_add_... functions, unlike in previous tutorials, you’ll now mix the three steps that were previously separated: \n\n\nUse metadata_add... functions where possible. \nMutate new columns using custom_R_code. \nManually map newly creted column names in the appropriate part of the metadata.yml file. \n\n\nCreate a metadata template\nTo create the metadata template, run:\n\nmetadata_create_template(\"tutorial_dataset_4\")\n\nAs with in the previous tutorials, this function leads you through a series of menus requiring user input. Ensure you select:\ndata format: wide\ntaxon_name column: 1: Species\nlocation_name column: 1: NA\nindividual_id column: 1: NA\ncollection_date column: 1: NA\nEnter collection_date range in format ‘2007/2009’: 1996/2015\nDo all traits need repeat_measurements_id’s? 2: No\n\nNotes:\n\n\nAs you noted before, there is no location column to map in automatically, so that must be added later. You enter NA for now.\n\nSince this is a compilation, there is nowhere in the manuscript that indicates the collection dates for each study. The best estimate is to list the range of publication dates of the sources: 1996–2015.\n\n\nNavigate to the dataset’s folder and open the metadata.yml file in Visual Studio Code, to ensure information is added to the expected sections as you work through the tutorial.\n\n\n\nPropagate source information into the metadata.yml file\nEntering the reference information for the manuscript from which the data appendix was sourced is straightforward, as this is a single publication:\n\nmetadata_add_source_doi(dataset_id = \"tutorial_dataset_4\", doi = \"10.1002/ece3.1344\")\n\nHowever, you also want to acknowledge the original data sources, those documented in the column References.\n\nread_csv(\"data/tutorial_dataset_4/data.csv\") %&gt;% distinct(References)\n\n# A tibble: 11 × 1\nReferences             \n   &lt;chr&gt;                  \n 1 Barrett et al. 1996    \n 2 Benyon et al. 1999     \n 3 Bleby et al.2009       \n 4 Brodribb and Felid 2000\n 5 Brooksbank et al. 2011 \n 6 Canham et al. 2009     \n 7 Carter and White 2009  \n 8 Cernusak et al. 2006   \n 9 Choat et al. 2005      \n10 Drake and Franks 2003  \n11 Drake et al. 2011      \n\nYou would now need to look up each of these references in the reference section of the manuscript and use Google Scholar (or another reference resource) to look up the doi for each reference.\n\nTo add additional references, you need to add an argument to the metadata_add_source_doi function:\n\n\nmetadata_add_source_doi(dataset_id = \"tutorial_dataset_4\", doi = \"10.1071/bt9960249\", type = \"secondary\")\n\nIf you now look at the metadata file you’ll see that a second reference has been added. However, you need to manually change the key field to be the author_year of this reference, Barrett_1996. In addition, you need to change the field secondary_01 to original_01. Per the traits.build schema, a secondary reference is when there have been two related publications out of a single dataset, while the term original is used if the data were collected as part of a previous dataset (generally by different authors) and added to traits.build as part of a compilation.\n\nWhen you add Benyon_1999 (doi - “10.1016/s0378-3774(98)00080-8”), you’ll need to change the field secondary_01 to original_02, etc.\nNote that AusTraits will build fine without adding all 11 original sources; it is up to you how many you want to add as a test.\n\n\n\nMap source_id into metadata file\n\nsource_id is a metadata field that is used relatively infrequently. In the AusTraits trait database only 1/20 studies require the mapping of a source_id and therefore as a default the field does not get added to the metadata template. \nInstead, you have to manually add it to the files’ dataset section, generally directly below location_name. \nSimply add a line source_id: source_id. Make sure the indents line up with the fields above/below. \nWhen the AusTraits team initially added this dataset, the curator manually added the source_id column. Otherwise such a column could be mutated from the reference column using custom_R_code or added manually in Excel. \n\n\n\n\nAdd location details\nYour protocol for adding locations diverges from the past 3 tutorials, because in this dataset you don’t yet have location names. The data file instead includes the latitude and longitude of each site, information you can use to “create” location names; the actual name doesn’t matter, just that each latitude/longitude combination has a unique location name.\n\nAlthough you could edit the data.csv file directly (and sometimes we do), you could alternatively create a location_name column through custom_R_code:\n\n\n  custom_R_code: '\n    data %&gt;% \n      mutate(\n        location_name = paste0(\"lat_\",Lat,\"_long_\",Long)\n      )\n'\n\nIn this case you’re using custom_R_code to generate many unique location names as a column in the data table as it is first read into the R workflow. \nNote: While, for this example you are generating many unique location names, there are many datasets where all data have been collected at a single location, and therefore the submitted dataset doesn’t include a location_name column. For all of those you simply add code like mutate(location_name = \"Broken Hill\") into custom_R_code. \nYou next want to create a table of location names and location properties (i.e latitude & longitude):\n\n\nlocation_table &lt;- \n  metadata_check_custom_R_code(\"tutorial_dataset_4\") %&gt;%\n  select(location_name, Lat, Long) %&gt;%\n  rename(`latitude (deg)` = Lat, `longitude (deg)` = Long) %&gt;%\n  distinct()\n\nmetadata_add_locations(\"tutorial_dataset_4\", location_table)\n\nNotes: \n\nRemember the function metadata_check_custom_R_code reads in the data.csv file, applies custom_R_code manipulations, and outputs the updated data table. This is very useful if you want to check your custom_R_code is performing as expected or if you want to perform further manipulations to the output. \nThere are many other ways to create a table of location names and properties. You could create a standalone table using R (or Excel), but this solution generates no additional files to store.\n\n\n\n\n\nMap location name into metadata file\n\nBecause the column location_name did not exist when you created the metadata table, you filled in location_name = NA during the initial user prompts.\nYou now need to manually fill the newly mutated location name column into the metadata file. Under the dataset section you’ll find the field location_name: unknown. This needs to be replaced with location_name: location_name.\n\n\n\n\n\nAdd traits\nThere is a single trait for this study, Huber value, which is the sapwood area to leaf area ratio. However, in the data.csv file it is documented as log_LA.SA and another line of custom R code must be added:\n\n\n  custom_R_code: '\n    data %&gt;% \n      mutate(\n        location_name = paste0(\"lat_\",Lat,\"_long_\",Long),\n        LA.SA = 10^(log_LA.SA)\n      )\n'\n\nYou could take the inverse of LA.SA, but this can also be accomplished through unit conversions.\nYou can then run:\n\nmetadata_add_traits(\"tutorial_dataset_4\")\n\nSelect column 13, your newly created column of trait data.\nAs with previous datasets, the following section has been added to metadata.yml:\n\n- var_in: LA.SA\n  unit_in: unknown\n  trait_name: unknown\n  entity_type: unknown\n  value_type: unknown\n  basis_of_value: unknown\n  replicates: unknown\n  methods: unknown\n\nThis trait has several “non-standard” values:\n\n\nunit_in: The units for the input column are leaf area/sapwood area, a dimensionless “area ratio”. Meanwhile, Huber value is reported as sapwood area/leaf area, the inverse dimensionless “area ratio”. \n\nThe UCUM standard to which traits.build conforms specifies that “dimensionless” is only accepted for the very few traits that are truly dimensionless, not traits where units top and bottom simply cancel out. You need to specify that it is a ratio of area/area (or mass/mass, count/count, etc.) \nLooking in the trait dictionary, you’ll see that the units are specified as: mm2{sapwood}/mm2{leaf}, specifically to be explicit about which area is the denominator vs numerator. You therefore specify that the units_in are mm2{leaf}/mm2{sapwood}. \nSince it is dimensionless, you could, of course specify any area units on top and bottom as long as they are identical, but I know mm2{leaf}/mm2{sapwood} is already in the unit conversions file. \n\nentity_type, value_type: Because this study is a compilation of many sources, the entity_type and value_type are not consistent across all measurements. Instead the data curator had to go back to many of the original sources and document which were population-level versus individual-level measurements, and correspondingly which were means vs raw values. For such circumstances (which also can occur within a single study), you can map a column name as the value.\n\nreplicates: For most of the studies the number of replicates comprising the trait values is unknown.\n\nTaking all this into account, you’re left with:\n\n- var_in: LA.SA\n  unit_in: mm2{leaf}/mm2{sapwood}\n  trait_name: huber_value\n  entity_type: entity\n  value_type: value\n  basis_of_value: measurement\n  replicates: unknown\n  methods: Leaf area was measured using a desktop scanner for all leaves on a sampled\n    branch. After bark removal, the branch cross-sectional area was measured with\n    a digital caliper at two points near the cut (methodology that includes the non-conductive\n    part of the sapwood). The pith cross-sectional area was measured and subtracted\n    from the branch cross-sectional area. For compiled and contributed data, measurements\n    on branches were made as described above, although with a variable number of branches\n    sampled per tree. In some studies where the branch diameter was too small (&lt;10\n    mm), the pith was considered part of the sapwood. For whole trees, cross-sectional\n    sapwood area was measured at 1.3 m above the ground from bored cores or harvesting\n    the tree. Leaf area in whole trees was in most cases measured from harvested trees.\n\n\n\n\nAdd contexts\nIf you consider the columns within the data.csv file you’ll note that Sample documents a method context and needs to be added as a context property. In addition, within this dataset, Height (plant height) is not a trait, but a covariate; some datasets documented this, because they thought it might influence the Huber value of the plant. It is therefore a context.\n\nmetadata_add_contexts(\"tutorial_dataset_4\")\n\nFollowing the user prompts select:\n5: Sample\n6: Height\n\nYou are then led sequentially through the user prompts for each of the context properties:\nSample\n4: method_context\n\nThis context is a method context, because it specifies a difference in methodology that might influence that trait value. \nThe following values exist for this context: trunk sample branch sample\nAre replacement values required? (y/n) n\nAre descriptions required? (y/n) y\n\nThe answers to the next two questions are up to the dataset curator, but at AusTraits we decided that trunk sample and branch sample were sufficiently explicit context property values, but that it would be helpful to add a description. \nTherefore, we filled in context_property: wood sample type and added descriptions for the context property values:\n\n\n- context_property: wood sample type\n  category: method_context\n  var_in: Sample\n  values:\n  - value: trunk sample\n    description: Measurements taken on the trunk of the tree.\n  - value: branch sample\n    description: Measeurements taken on a branch from the tree.\n\nHeight\nPlant height is an entity context. It is a feature of the entity (an individual) that might influence the trait value.\n\n5: entity_context\n\nAgain, the dataset curator may choose what information to document within the metadata file. For continuous traits, like plant height, the general consensus is that the values are self-explanatory, so you’d select:\n\nThe following values exist for this context: 6.7, 4.8, 6.9\nAre replacement values required? (y/n) n\nAre descriptions required? (y/n) n\n\nFilling in that the context property is tree height (m), the metadata file would simply be:\n\n\n- context_property: tree height (m)\n  category: entity_context\n  var_in: Height\n\n\n\n\nTesting, error fixes, and report building\nAt this point, run the dataset tests, rebuild the dataset, and check for excluded data:\n\ndataset_test(\"tutorial_dataset_4\")\n\nbuild_setup_pipeline()\n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\ntraits.build_database$excluded_data %&gt;% filter(dataset_id == \"tutorial_dataset_4\") %&gt;%  View()\n\nThere should be no errors and no excluded data, so go ahead and build a report for the study:\n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_4\", traits.build_database, overwrite = TRUE)\n\nOverall, this report isn’t very informative since it is the first Huber value dataset in the new database.\n\nBut let me draw your attention to the list of taxa at the bottom. Because the tutorials are, for now, ignoring taxon alignments (traits.build in a state of flux with regard to this), the tutorials have ignored this section.  But let me draw your attention to the list of taxa at the bottom. Because the tutorials are, for now, ignoring taxon alignments (traits.build in a state of flux with regard to this), the tutorials have ignored this section. \nHowever, note the unknown taxa names unk sp. 1 and unk sp. 2. Although the AusTraits database accepts names resolved to genus and family, data collected on a truly unknown taxon is useless and should be excluded. Being a terrestrial vascular plant database, we also exclude mosses and lichens that are sometimes in datasets. The curators for other databases aligned to the traits.build workflow will have their own standards for values to explicitly disallow, based on taxonomy (or some other variable).\nNear the bottom of the metadata file is a section for excluding data.\n\nIt currently reads:\n\n\nexclude_observations: .na\n\nChange this to:\n\nexclude_observations: \n- variable: taxon_name\n  find: unk sp. 1, unk sp. 2\n  reason: omitting completely unknown taxa (E Wenk, 2023.09.20)\n\nNotes: \n\nOther variable names can also be used here. Perhaps there is a particular context value or location that is known to have problematic data. In AusTraits this field is almost exclusively used to exclude specific taxa, but the metadata section is designed to have broader applications.\n\nIf you now rebuild the database, you’ll see that the measurements associated with these two data are now in the excluded_data table."
  },
  {
    "objectID": "tutorial_dataset_5.html#overview",
    "href": "tutorial_dataset_5.html#overview",
    "title": "24  Tutorial 5: Multiple columns for a trait",
    "section": "24.1 Overview",
    "text": "24.1 Overview\nThis is the fifth tutorial on adding datasets to your traits.build database.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the example datasets in traits.build-template. Instructions are available at Tutorial: Example compilation.\n\nIt is also recommended that you first work through some of the earlier tutorials, as many steps for adding datasets to a traits.build database are only thoroughly described in the early tutorials.\n\nGoals\n\nLearn how to map context properties into the metadata traits section \nLearn how to add measurement remarks \n\n\n\nNew Functions Introduced\n\nnone."
  },
  {
    "objectID": "tutorial_dataset_5.html#adding-tutorial_dataset_5",
    "href": "tutorial_dataset_5.html#adding-tutorial_dataset_5",
    "title": "24  Tutorial 5: Multiple columns for a trait",
    "section": "24.2 Adding tutorial_dataset_5",
    "text": "24.2 Adding tutorial_dataset_5\nThis dataset is a subset of data from Geange_2017 in AusTraits.\nThis tutorial focuses on how to input a dataset where there are multiple columns for the same trait, with each column indicating measurements made under different context conditions. \nBefore you begin creating the metadata file, take a look at the data.csv file. Note that there are two columns for each photosynthesis and conductance. For this dataset these represent repeat measurements made on the same individuals under different experimental treatments. For other studies, these may be multiple columns if the same trait was measured using separate methods. \n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_5 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_5 folder. \nThere is a folder raw nested within the tutorial_dataset_5 folder, that contains one file, tutorial_dataset_5_notes.txt. \n\n\n\nsource necessary functions\n\nIf you have restarted R Studio since last adding a dataset, ensure all functions are loaded from both the traits.build package and the custom functions file:\n\n\nlibrary(traits.build)\nsource(\"R/custom_R_code.R\")\n\n\n\n\nCreate a metadata.yml file\n\nCreate a metadata template\nTo create the metadata template, run:\n\nmetadata_create_template(\"tutorial_dataset_5\")\n\nAs with in the previous tutorials, this function leads you through a series of menus requiring user input. Ensure you select:\ndata format: wide\ntaxon_name column: 1: species_name\nlocation_name column: 1: NA\nindividual_id column: 1: NA\ncollection_date column: 21: date\nDo all traits need repeat_measurements_id’s? 2: No\n\nNotes:\n\n\nThere is no location column to map in automatically, so that must be added later. You enter NA for now.\n\nThe column Abbrev! appears to be a unique identifier for each individual that can be mapped in to identify individual plants. Mapping in an individual_id column is essential if multiple rows include measurements for the same individual, as might happen if individuals are measured repeatedly across time. For this dataset, each row includes measurements on a separate individual, so it isn’t required to map individual_id. Moreover, if you were to look closely at the values in the column Abbrev! you would notice there are 3 instances of duplication. Were you to map in individual_id: Abbrev! you would end up with an error - as occurred initially when this dataset was added to AusTraits.\n\n\nNavigate to the dataset’s folder and open the metadata.yml file in Visual Studio Code, to ensure information is added to the expected sections as you work through the tutorial.\n\n\n\nPropagate source information into the metadata.yml file\nUse the function metadata_add_source_doi to add the source.\n\nThe reference doi is 10.1186/s40665-017-0033-8.\n\n\n\nAdd measurement remarks\nThere is a free-form comments column called measurement_remarks that can be mapped in at the dataset level (i.e. for all measurements) or under specific traits. \nThis column is not used to generate any of the identifiers and therefore cannot be used as a location to document information that is a context property, source, location, method, etc. However, there can be minor notes that have been documented about specific observations or trait measurements that should be retained in the traits.build output and if this information if available in a column it can be mapped into measurement remarks.\n\nFor instance, in this dataset, the column Mother documents the maternal lineage of each individual. This could be recorded as an official context property, or, alternatively could simply be added as a measurement remark, first mutating a column:\n\n\n  custom_R_code: '\n    data %&gt;% \n      mutate(\n        measurement_remarks = paste0(\"maternal lineage \", Mother)\n      )\n'\n\nand then adding measurement_remarks: measurement_remarks to the dataset section of the metadata file, below life_stage\n\n\n\nAdd location details\nThere isn’t a location name specified in the data.csv file, so use custom_R_code to mutate a new column, location_name.\n\n\n  custom_R_code: '\n    data %&gt;% \n      mutate(\n        measurement_remarks = paste0(\"maternal lineage \", Mother),\n        location = \"Australian National University glasshouse\"\n      )\n'\n\nAnd then specify this column as the source of location_name in the dataset section of the metadata file.\n\nAnd manually add the location details to the location section of the metadata file\n\n\n  Australian National University glasshouse:\n    latitude (deg): -35.283\n    longitude (deg): 149.1167\n    precipitation, MAP (mm): 622\n    description: Australian National University glasshouses\n\n\n\nAdd traits\nTo select columns in the data.csv file that include trait data, run:\n\nmetadata_add_traits(dataset_id = \"tutorial_dataset_5\")\n\nSelect columns 13 14 15 16 17 18 19, as these contain trait data.\n\nThen fill in the details for each trait column in the traits section of the metadata file.\n\nRemember, the trait_name must match a trait concept within the traits dictionary. For this example:\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn in dataset\ntrait concept\nunits_in\nentity_type\nvalue_type\nbasis_of_value\nreplicates\n\n\n\n\nPhoto\nleaf_photosynthetic_rate_per_area_saturated\numol{CO2}/m2/s\nindividual\nraw\nmeasurement\n1\n\n\nCond\nleaf_stomatal_conductance_per_area_at_Asat\nmol{H2O}/m2/s\nindividual\nraw\nmeasurement\n1\n\n\nPhoto_D\nleaf_photosynthetic_rate_per_area_saturated\numol{CO2}/m2/s\nindividual\nraw\nmeasurement\n1\n\n\nCond_D\nleaf_stomatal_conductance_per_area_at_Asat\nmol{H2O}/m2/s\nindividual\nraw\nmeasurement\n1\n\n\narea_mm2\nleaf_area\nmm2\nindividual\nraw\nmeasurement\n1\n\n\nSLA_cm_g:4\nleaf_mass_per_area\ncm2/g\nindividual\nraw\nmeasurement\n1\n\n\n%N:1\nleaf_N_per_dry_mass\n‘%’\nindividual\nraw\nmeasurement\n1\n\n\n\n\n\nAdd contexts\n\nContexts from columns\nThere are two columns in the data.csv file that specify contexts, Elevation (seed provenance) and Treatment (drought treatment).\nTo add these contexts to the metadata file, run:\n\nmetadata_add_contexts(dataset_id = \"tutorial_dataset_5\")\n\nSelect columns 6 7 as these contain context properties\nThe category for both of these is treatment_context.\nAnd as the values for both are abbreviations, it is recommended to replace the values for both context properties with proper terms and descriptions.\nTherefore, the metadata template will now have the following section:\n\ncontexts:\n- context_property: unknown\n  category: treatment\n  var_in: Elevation\n  values:\n  - find: LoElev\n    value: unknown\n    description: unknown\n  - find: HiElev\n    value: unknown\n    description: unknown\n- context_property: unknown\n  category: treatment\n  var_in: Treatment\n  values:\n  - find: LoWat\n    value: unknown\n    description: unknown\n  - find: HiWat\n    value: unknown\n    description: unknown\n\nWhich will be filled in as:\n\n- context_property: seed provenance\n  category: treatment\n  var_in: Elevation\n  values:\n  - find: LoElev\n    value: low elevation\n    description: Seeds sourced from low elevation populations.\n  - find: HiElev\n    value: high elevation\n    description: Seeds sourced from hight elevation populations.\n- context_property: drought treatment\n  category: treatment\n  var_in: Treatment\n  values:\n  - find: LoWat\n    value: low water\n    description: Plants assigned to low water treatment.\n  - find: HiWat\n    value: high water\n    description: Plants assigned to high water treatment.\n\n\n\nContexts manually added\nAs background, at the point in the traits.build workflow where the trait metadata is read in, the trait data has been converted to long format, with each trait measurement is its own row. This allows columns such as methods, entity_type, and units to be added, which are inherently unique to a specific trait. It also means that context properties can now be added, with different values assigned to different traits. \nFor this study, in addition to the two contexts that are documented as columns, there is a context property that is documented across columns, the time from last watering to gas exchange measurements. For photosynthesis and conductance, the columns Photo and Cond document measurements made just after a watering cycle, while the columns Photo_D and Cond_D document measurements made at the very end of a watering cycle. \nFor such situations, you add a line to the traits section of the metadata for each of these traits.\nFor instance, for the column Photo, you would add: \n\n  replicates: 1\n  time_since_watering: start of watering cycle\n  methods: Gas exchange was measured using...\n\nThis creates a new column, time_since_watering and for the trait column Photo, the value is start of watering cycle. \nYou add an identical line to the trait column Cond, while for the trait columns Photo_D and Cond_D instead insert a line time_since_watering: end of watering cycle. \nFor the other traits, no lines are added, as these the context property time_since_watering doesn’t apply to them. \nSince the trait metadata is read in long after the custom_R_code code is executed, this context property cannot be read in using a function. Instead it must be manually added as a context_property.\n\n- context_property: time since watering\n  category: temporal\n  var_in: time_since_watering\n  values:\n  - value: start of watering cycle\n    description: Measurements made on the morning following a watering event when the plants were at their least water-limited.\n  - value: end of watering cycle\n    description: Measurements made on the final day of a watering cycle when the plants were at the driest point in the cycle.\n\n\n\n\nAdding contributors\nThe file data/tutorial_dataset_5/raw/tutorial_dataset_5_notes.txt indicates the main data_contributor for this study. \n\n\nDataset fields\nThe file data/tutorial_dataset_5/raw/tutorial_dataset_5_notes.txt indicates how to fill in the unknown dataset fields for this study.\n\n\n\nTesting, error fixes, and report building\nAt this point, run the dataset tests, rebuild the dataset, and check for excluded data:\n\ndataset_test(\"tutorial_dataset_5\")\n\nbuild_setup_pipeline(method=\"remake\")\n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\ntraits.build_database$excluded_data %&gt;% filter(dataset_id == \"tutorial_dataset_5\") %&gt;%  View()\n\nThere should be no errors. \nThere are a handful of excluded values, including both negative photosynthetic rates and negative conductance rates and two instances where leaf_area = 0. The leaf_area = 0 values need to be removed using custom_R_code. \n\nmutate(across(c(\"area_mm2\"), ~na_if(.x,0)))\n\nThen remake the database and again check the excluded data table.\nIf the only excluded values remaining are the negative gas exchange rates, build a report for the study:\n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_5\", traits.build_database, overwrite = TRUE)"
  },
  {
    "objectID": "tutorial_dataset_6.html#overview",
    "href": "tutorial_dataset_6.html#overview",
    "title": "25  Tutorial 6: Data with repeat measurements",
    "section": "25.1 Overview",
    "text": "25.1 Overview\nThis is the sixth tutorial on adding datasets to your traits.build database.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the example datasets in traits.build-template. Instructions are available at Tutorial: Example compilation.\n\nIt is also recommended that you first work through some of the earlier tutorials, as many steps for adding datasets to a traits.build database are only thoroughly described in the early tutorials.\n\nGoals\n\nLearn how to add repeat measurement id’s \nLearn how to add individual_id’s \n\n\n\nNew Functions Introduced\n\nnone."
  },
  {
    "objectID": "tutorial_dataset_6.html#adding-tutorial_dataset_6",
    "href": "tutorial_dataset_6.html#adding-tutorial_dataset_6",
    "title": "25  Tutorial 6: Data with repeat measurements",
    "section": "25.2 Adding tutorial_dataset_6",
    "text": "25.2 Adding tutorial_dataset_6\nThis dataset is data submitted as part of Cernusak_2011 in AusTraits. AusTraits itself does not include the raw A-ci curve data that is being added for this tutorial.\nThis tutorial focuses on how to input a dataset where a single trait measurement consists of a series of time-ordered measurements and the repeat measurements must clearly be identified as being part of the same the same observation. \nBefore you begin creating the metadata file, take a look at the data.csv file. If you are familiar with the output of an IRGA (instrument to measure gas exchange) you will note that many columns of essential metadata have been removed - for simplicity of this tutorial \n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_6 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_6 folder. \nThere is a folder raw nested within the tutorial_dataset_6 folder, that contains two files, locations.csv and tutorial_dataset_6_notes.txt. \n\n\n\nsource necessary functions\n\nIf you have restarted R Studio since last adding a dataset, ensure all functions are loaded from both the traits.build package and the custom functions file:\n\n\nlibrary(traits.build)\nsource(\"R/custom_R_code.R\")\n\n\n\n\nCreate a metadata.yml file\n\nCreate a metadata template\nTo create the metadata template, run:\n\nmetadata_create_template(\"tutorial_dataset_6\")\n\nAs with in the previous tutorials, this function leads you through a series of menus requiring user input. Ensure you select:\ndata format: wide\ntaxon_name column: 2: Species\nlocation_name column: 2: Site\nindividual_id column: 1: NA\ncollection_date column: 6: Date\nDo all traits need repeat_measurements_id’s? 1: Yes\n\nNotes:\n\n\nThere currently isn’t an individual_id column, but this is required for repeat_measurements_id’s to properly generate. An individual_id column will need to be added via custom_R_code.\n\nThis is the first tutorial that includes repeat_measurement_id’s. repeat_measurement_id’s are sequential integer identifiers assigned to a sequence of measurements on a single trait that together represent a single observation (and are assigned a single observation_id by the traits.build pipeline. The assumption is that these are measurements that document points on a response curve. Although the exact time of each measurement will of course be different for point on the curve, time is not a temporal context and must be identical for all measurements within a single curve.\n\nFor this dataset - and probably for most datasets that document response curve data - all traits being added will be repeat measurements. However, if some columns of trait data are not part of the response curve data, one can alternatively map repeat_measurments_id: TRUE for individual traits in the traits section of metadata.yml.\nA word of warning for datasets where the output data includes a time stamp. Ensure that there is a separate collection_date column that is a date not a time, as all measurements that comprise a single response curve must have the same collection_date. Otherwise, the traits.build pipeline will assign them each separate observation_id’s.\nNavigate to the dataset’s folder and open the metadata.yml file in Visual Studio Code, to ensure information is added to the expected sections as you work through the tutorial.\n\n\nPropagate source information into the metadata.yml file\nUse the function metadata_add_source_doi to add the source.\n\nThe reference doi is 10.1016/j.agrformet.2011.01.006.\n\n\n\nAdd individual_id\nIn order for repeat_measurements_id’s to properly generate, it is essential to identify which sequence of rows represent a single individual. For this dataset, the columns Site, Species, and Leaf number jointly identify individuals and therefore a new column must be mutated in custom_R_code, then specified as the source of individual_id in the dataset section of metadata.yml: \n\n  custom_R_code: '\n    data %&gt;% \n      mutate(\n        individual_id = paste(Site, Species, `Leaf number`, sep = \"_\")\n      )\n  '\n\nand then add individual_id: individual_id to the dataset section of the metadata file, below location_name.\n\n\n\nAdd location details\nThere is a file in the raw folder with location details: \n\nlocations &lt;- read_csv(\"data/tutorial_dataset_6/raw/locations.csv\")\n\nmetadata_add_locations(\"tutorial_dataset_6\", locations)\n\nAt the user prompts: \nlocation name: 1\ncolumns with location properties: 1 2 3 4 5 6\n\n\n\nAdd traits\nTo select columns in the data.csv file that include trait data, run:\n\nmetadata_add_traits(dataset_id = \"tutorial_dataset_6\")\n\nSelect columns 13 14 15, as these contain trait data.\n\nThen fill in the details for each trait column in the traits section of the metadata file.\n\nRemember, the trait_name must match a trait concept within the traits dictionary. For this example:\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn in dataset\ntrait concept\nunits_in\nentity_type\nvalue_type\nbasis_of_value\nreplicates\n\n\n\n\nPhotosynthesis (umol m-2 s-1)\nleaf_photosynthetic_rate_per_area_saturated\numol{CO2}/m2/s\nindividual\nraw\nmeasurement\n1\n\n\nConductance to H2O (mol m-2 s-1)\nleaf_stomatal_conductance_per_area_at_Asat\nmol{H2O}/m2/s\nindividual\nraw\nmeasurement\n1\n\n\nCi (umol mol-1)\nleaf_intercellular_CO2_concentration_at_Asat\numol{CO2}/mol\nindividual\nraw\nmeasurement\n1\n\n\n\n\n\nAdd contexts\nThere are no required contexts for this dataset. One could add the column Canopy of understory as a method_context, but as there is only a single value reported (“canopy”) this isn’t essential.\n\n\nAdding contributors\nThe file data/tutorial_dataset_6/raw/tutorial_dataset_6_notes.txt indicates the main data_contributor for this study.\n\n\n\nDataset fields\nThe file data/tutorial_dataset_6/raw/tutorial_dataset_6_notes.txt indicates how to fill in the unknown dataset fields for this study.\n\n\n\nTesting, error fixes, and report building\nAt this point, run the dataset tests, rebuild the dataset, and check for excluded data:\n\ndataset_test(\"tutorial_dataset_6\")\n\nbuild_setup_pipeline(method=\"remake\")\n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\ntraits.build_database$excluded_data %&gt;% filter(dataset_id == \"tutorial_dataset_6\") %&gt;%  View()\n\nThere should be no errors. However there are many excluded data values - entirely negative photosynthetic rates. The definition of leaf_photosynthetic_rate_per_area_saturated requires photosynthetic rates to be positive, so these are valid excluded values and simply remain in the excluded data table. So go ahead and build a report for the study:\n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_6\", traits.build_database, overwrite = TRUE)"
  },
  {
    "objectID": "tutorial_dataset_7.html#overview",
    "href": "tutorial_dataset_7.html#overview",
    "title": "26  Tutorial 7: Adding long format dataset",
    "section": "26.1 Overview",
    "text": "26.1 Overview\nThis is the seventh tutorial on adding datasets to your traits.build database.\nBefore you begin this tutorial, ensure you have installed traits.build, cloned the traits.build-template repository, and have successfully build a database from the example datasets in traits.build-template. Instructions are available at Tutorial: Example compilation.\n\nIt is also recommended that you first work through some of the earlier tutorials, as many steps for adding datasets to a traits.build database are only thoroughly described in the early tutorials.\n\nGoals\n\nLearn how to add a long dataset \nLearn how to add units from a column \n\n\n\nNew Functions Introduced\n\nnone."
  },
  {
    "objectID": "tutorial_dataset_7.html#adding-tutorial_dataset_7",
    "href": "tutorial_dataset_7.html#adding-tutorial_dataset_7",
    "title": "26  Tutorial 7: Adding long format dataset",
    "section": "26.2 Adding tutorial_dataset_7",
    "text": "26.2 Adding tutorial_dataset_7\nThis dataset is a subset of data from ABRS_1981 in AusTraits. These are data from the original Flora of Australia volumes (Australian Biological Resources Study) and are therefore all species-level trait values.\nThis tutorial focuses on how to input a dataset in long format, where there is a single column with all trait values and a column specifying the trait documented in each row of the data file. \n\nEnsure the dataset folder contains the correct data files\nIn the traits.build-template repository, there is a folder titled tutorial_dataset_7 within the data folder. \n\nEnsure that this folder exists on your computer. \nThe file data.csv exists within the tutorial_dataset_7 folder. \nThere is a folder raw nested within the tutorial_dataset_7 folder, that contains two files, locations.csv and tutorial_dataset_7_notes.txt. \n\n\n\nsource necessary functions\n\nIf you have restarted R Studio since last adding a dataset, ensure all functions are loaded from both the traits.build package and the custom functions file:\n\n\nlibrary(traits.build)\nsource(\"R/custom_R_code.R\")\n\n\n\n\nCreate a metadata.yml file\n\nCreate a metadata template\nTo create the metadata template, run:\n\nmetadata_create_template(\"tutorial_dataset_7\")\n\nAs with previous datasets, the first question asks whether this is a long or wide dataset. You now select long:\nAs with in the previous tutorials, this function leads you through a series of menus requiring user input. Ensure you select:\ndata format: long\n\nThe remaining prompts are now slightly different, since you have to identify columns for trait_name and value:\nSelect column for taxon_name 1: species_name\nSelect column for trait_name 2: trait\nSelect column for value 4: value\nlocation_name column: 1: NA\nindividual_id column: 1: NA\ncollection_date column: 1: NA\nEnter collection_date range in format ‘2007/2009’: unknown/1981\nDo all traits need repeat_measurements_id’s? 2: No\n\nNotes: \n\nAll long-format datasets require an identifier to group rows of data referring to the same entity. If neither a location_name nor an individual_id is provided (as is the case for all flora-derived datasets), the taxon_name becomes the identifier that is used to unite measurements into a single observation.\n\nNavigate to the dataset’s folder and open the metadata.yml file in Visual Studio Code, to ensure information is added to the expected sections as you work through the tutorial.\n\n\nPropagate source information into the metadata.yml file\nSince this dataset is not from a published study with a doi, the source information needs to be manually added:\n\n\n  bibtype: Online\n  year: 1981\n  author: '{Australian Biological Resources Study}'\n  title: Flora of Australia, Australian Biological Resources Study, Canberra.\n  publisher: Department of Climate Change, Energy, the Environment and Water, Canberra.\n  url: http://www.ausflora.org.au\n\nThere are other bibtype’s you will encounter as well, including Unpublished, Book, Misc, Thesis, InBook (for chapters), Report and TechReport. For each there are different required and optional fields (per BibTex’s rules). See the complete guide to adding datasets for examples of each.\n\n\nAdd traits\nTo select columns in the data.csv file that include trait data, run:\n\nmetadata_add_traits(dataset_id = \"tutorial_dataset_7\")\n\nFor long datasets, this function outputs a list of unique values within the trait names column:\nIndicate all columns you wish to keep as distinct traits in tutorial_dataset_7 (by number separated by space; e.g. ‘1 2 4’): 1: leaf length maximum 2: leaf type 3: seed length maximum 4: seed length minimum\nSelect columns 1 2 3 4, you want to include all four traits.\n\nThen fill in the details for each trait column in the traits section of the metadata file.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrait\ntrait concept\nunits_in\nentity_type\nvalue_type\nbasis_of_value\nreplicates\n\n\n\n\nleaf length maximum\nleaf_length\nunits\nspecies\nmaximum\nmeasurement\n.na\n\n\nleaf type\nleaf_compoundness\n.na\nspecies\nmode\nexpert_score\n.na\n\n\nseed length maximum\nseed_length\nunits\nspecies\nmaximum\nmeasurement\n.na\n\n\nseed length minimum\nseed_length\nunits\nspecies\nminimum\nmeasurement\n.na\n\n\n\nNotes:\n\nYou may have noticed in the data.csv file that there is also a column units. For many long datasets there is a fixed unit for each trait, just as is standardly the case for wide datasets. In such cases fixed units values are mapped into the traits section of the metadata file, just as occurs with most wide datasets. In this dataset there is a column documenting the units, as different tax have leaf length and seed length reported in different units. The column for units can be mapped in at the trait level, as indicated here, or, for a long dataset, it could be mapped in a single time in the dataset section of the metadata, units_in: units and then you’d delete the line referring to units_in from each of the traits.\nThere are two different trait names that refer to seed length, seed length maximum and seed length minimum. It is not a problem that these both map to the trait concept seed_length as they are different value types. \nBecause these are species-level trait values, even the numeric traits do not have a replicate count. The range of values should represent all individuals of the species. \n\n\n\nAdding contributors\nThe file data/tutorial_dataset_7/raw/tutorial_dataset_7_notes.txt indicates the main data_contributor for this study.\n\n\n\nDataset fields\nThe file data/tutorial_dataset_7/raw/tutorial_dataset_7_notes.txt indicates how to fill in the unknown dataset fields for this study. \n\n\n\nTesting, error fixes, and report building\nAt this point, run the dataset tests, rebuild the dataset, and check for excluded data:\n\ndataset_test(\"tutorial_dataset_7\")\n\nbuild_setup_pipeline(method=\"remake\")\n\ntraits.build_database &lt;- remake::make(\"austraits\")\n\ntraits.build_database$excluded_data %&gt;% filter(dataset_id == \"tutorial_dataset_7\") %&gt;%  View()\n\nThe excluded data includes four rows of data with the error Unsupported trait value for the trait leaf_compoundness. The term article does not describe a leaf’s compoundness. As articles are always simple leaves you can add a substitution: \n\nmetadata_add_substitution(dataset_id = \"tutorial_dataset_7\", trait_name = \"leaf_compoundness\", find = \"articles\", replace = \"simple\")\n\nThen remake the database and again check excluded data to ensure the substitution has worked as intended. \n\ntraits.build_database$build_info$version &lt;- \"4.0.0\"  # a fix because the function was built around specific AusTraits versions\ndataset_report(\"tutorial_dataset_7\", traits.build_database, overwrite = TRUE)"
  },
  {
    "objectID": "using_data.html#output-formats",
    "href": "using_data.html#output-formats",
    "title": "27  Using a compilation",
    "section": "27.1 Output formats",
    "text": "27.1 Output formats"
  },
  {
    "objectID": "using_data.html#how-to-access-and-use-the-data",
    "href": "using_data.html#how-to-access-and-use-the-data",
    "title": "27  Using a compilation",
    "section": "27.2 How to access and use the data",
    "text": "27.2 How to access and use the data"
  },
  {
    "objectID": "austraits_database.html#austraits-data-records",
    "href": "austraits_database.html#austraits-data-records",
    "title": "28  AusTraits, Australia’s Plant Trait Database",
    "section": "28.1 AusTraits data records",
    "text": "28.1 AusTraits data records\nAs of October, 2023, AusTraits has:\n\n370+ datasets \n250+ contributors \n1,800,000+ data records \n500+ traits \n30,000 Australian plant taxa"
  },
  {
    "objectID": "austraits_package.html#getting-started",
    "href": "austraits_package.html#getting-started",
    "title": "29  The austraits package",
    "section": "29.1 Getting started",
    "text": "29.1 Getting started\naustraits is still under development. To install the current version from GitHub:\n\n#install.packages(\"remotes\")\nremotes::install_github(\"traitecoevo/austraits\", dependencies = TRUE, upgrade = \"ask\")\n\n# Load the austraits package\nlibrary(austraits)\n\n\nLoading AusTraits database\nBy default, load_austraits will download AusTraits to a specified path e.g. data/austraits and will reload it from this location in the future. You can set update = TRUE so the austrait versions are downloaded fresh from Zenodo. Note that load_austraits will happily accept a DOI of a particular version.\n\naustraits &lt;- load_austraits(version = \"3.0.2\", path = \"data/austraits\")\n\nYou can check out different versions and their associated DOI of AusTraits by using:\n\nget_versions(path = \"data/austraits\")\n\nThe AusTraits object is a very long list with various of elements. If you are not familiar with working with lists in R, we recommend having a quick look at this tutorial. To learn more about the structure of austraits, check out the structure of the database.\n\naustraits\n\n#&gt; This is version 0.0.0.900 of austraits!\n#&gt;  \n#&gt; This object contains a total of 13695 records for 1255 taxa and 65 traits.\n#&gt; \n#&gt; This object is a 'list' with the following components:\n#&gt; \n#&gt;  - `traits`: A table containing measurements of plant traits.\n#&gt; - `sites`: A table containing observations of site characteristics associated with information in `traits`. Cross referencing between the two dataframes is possible using combinations of the variables `dataset_id`, `site_name`.\n#&gt; - `contexts`: A table containing observations of contextual characteristics associated with information in `traits`. Cross referencing between the two dataframes is possible using combinations of the variables `dataset_id`, `context_name`.\n#&gt; - `methods`: A table containing details on methods with which data were collected, including time frame and source.\n#&gt; - `excluded_data`: A table of data that did not pass quality test and so were excluded from the master dataset.\n#&gt; - `taxa`: A table containing details on taxa associated with information in `traits`. This information has been sourced from the APC (Australian Plant Census) and APNI (Australian Plant Names Index) and is released under a CC-BY3 license.\n#&gt; - `definitions`: A copy of the definitions for all tables and terms. Information included here was used to process data and generate any documentation for the study.\n#&gt; - `sources`: Bibtex entries for all primary and secondary sources in the compilation.\n#&gt; - `contributors`: A table of people contributing to each study.\n#&gt; - `taxonomic_updates`: A table of all taxonomic changes implemented in the construction of AusTraits. Changes are determined by comapring against the APC (Australian Plant Census) and APNI (Australian Plant Names Index).\n#&gt; - `build_info`: A description of the computing environment used to create this version of the dataset, including version number, git commit and R session_info.\n#&gt; \n#&gt; To access a component, try using the $ e.g. austraits$traits"
  },
  {
    "objectID": "austraits_package.html#descriptive-summaries-of-traits-and-taxa",
    "href": "austraits_package.html#descriptive-summaries-of-traits-and-taxa",
    "title": "29  The austraits package",
    "section": "29.2 Descriptive summaries of traits and taxa",
    "text": "29.2 Descriptive summaries of traits and taxa\nAusTraits contains 65 plant traits. Check out definitions of the traits to learn more about how each trait is defined.\nHave a look at what trait or taxa we have with:\n\nsummarise_austraits(austraits, \"trait_name\") %&gt;% head()\n\n#&gt;            trait_name n_records n_dataset n_taxa percent_total\n#&gt;   bark_C_per_dry_mass       170         1     17       0.01240\n#&gt;   bark_N_per_dry_mass       170         1     17       0.01240\n#&gt;         bark_delta13C       170         1     17       0.01240\n#&gt;         bark_delta15N       170         1     17       0.01240\n#&gt;        bark_thickness       187         1     17       0.01370\n#&gt;  branch_mass_fraction        45         1     45       0.00329\n\nsummarise_austraits(austraits, var =  \"family\") %&gt;% head()\n\n#&gt;         family n_records n_dataset n_taxa percent_total\n#&gt;    Acanthaceae         1         1      1      0.000073\n#&gt;     Akaniaceae         3         1      1      0.000219\n#&gt;  Anacardiaceae       462         3      7      0.033700\n#&gt;     Annonaceae        15         2      5      0.001100\n#&gt;       Apiaceae        17         1      2      0.001240\n#&gt;    Apocynaceae        32         3      9      0.002340\n\nsummarise_austraits(austraits, \"genus\") %&gt;% head()\n\n#&gt;       genus n_records n_dataset n_taxa percent_total\n#&gt;      Acacia      1642         5     79      0.120000\n#&gt;   Aceratium         5         1      3      0.000365\n#&gt;      Ackama        10         1      2      0.000730\n#&gt;   Acradenia         3         1      2      0.000219\n#&gt;  Acronychia        21         3      7      0.001530\n#&gt;   Actinotus        10         1      1      0.000730\n\n\nInterested in a specific trait? Try lookup_trait\n\nlookup_trait(austraits, \"leaf\") %&gt;% head()\n\n#&gt; [1] \"leaf_angle\"              \"leaf_area\"              \n#&gt; [3] \"leaf_compoundness\"       \"leaf_N_per_dry_mass\"    \n#&gt; [5] \"specific_leaf_area\"      \"leaf_cell_wall_fraction\""
  },
  {
    "objectID": "austraits_package.html#extracting-data",
    "href": "austraits_package.html#extracting-data",
    "title": "29  The austraits package",
    "section": "29.3 Extracting data",
    "text": "29.3 Extracting data\nIn most cases, users would like to extract a subset of austraits for their own research purposes.extract_dataset subsets a particular study, whereas extract_traitsubsets by certain traits. Note that the other tables and elements of the AusTraits data are extracted too, not just the main trait table. See ?extract_dataset and ?extract_trait for more details\n\nExtracting by study\nFiltering one particular study and assigning it to an object\n\nsubset_data &lt;- extract_dataset(austraits, \"Falster_2005_2\")\n\nsubset_data$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 12\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;\n#&gt; 1 Falster_200… Acacia lo… Myall_La… &lt;NA&gt;         Falster_2005_… fire_resp… fire…\n#&gt; 2 Falster_200… Acacia lo… Myall_La… &lt;NA&gt;         Falster_2005_… huber_val… 0.00…\n#&gt; 3 Falster_200… Acacia lo… Myall_La… &lt;NA&gt;         Falster_2005_… huber_val… 0.00…\n#&gt; 4 Falster_200… Acacia lo… Myall_La… &lt;NA&gt;         Falster_2005_… huber_val… 0.00…\n#&gt; 5 Falster_200… Acacia lo… Myall_La… &lt;NA&gt;         Falster_2005_… huber_val… 0.00…\n#&gt; 6 Falster_200… Acacia lo… Myall_La… &lt;NA&gt;         Falster_2005_… leaf_area  1761 \n#&gt; # ℹ 5 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;\n\n\nFiltering multiple studies by two different lead authors and assigning it to an object\n\nsubset_multi_studies &lt;- extract_dataset(austraits, \n                                        dataset_id = c(\"Thompson_2001\",\"Ilic_2000\"))\n \nsubset_multi_studies$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 12\n#&gt;   dataset_id taxon_name   site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;\n#&gt; 1 Ilic_2000  Acacia acra… &lt;NA&gt;      &lt;NA&gt;         Ilic_2000_0001 wood_dens… 0.904\n#&gt; 2 Ilic_2000  Acacia acum… &lt;NA&gt;      &lt;NA&gt;         Ilic_2000_0002 wood_dens… 0.895\n#&gt; 3 Ilic_2000  Acacia acum… &lt;NA&gt;      &lt;NA&gt;         Ilic_2000_0003 wood_dens… 1.008\n#&gt; 4 Ilic_2000  Acacia adsu… &lt;NA&gt;      &lt;NA&gt;         Ilic_2000_0004 wood_dens… 0.887\n#&gt; 5 Ilic_2000  Acacia ampl… &lt;NA&gt;      &lt;NA&gt;         Ilic_2000_0005 wood_dens… 0.568\n#&gt; 6 Ilic_2000  Acacia aneu… &lt;NA&gt;      &lt;NA&gt;         Ilic_2000_0006 wood_dens… 1.035\n#&gt; # ℹ 5 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;\n\n\nFiltering multiple studies by same lead author (e.g. Falster) and assigning it to an object.\n\n# First, we need to identify all studies with an id that includes \"Falster\"\n\n( dataset_ids &lt;- austraits$methods$dataset_id %&gt;% unique() %&gt;% subset(., grepl(\"Falster\",.))) \n\n#&gt; [1] \"Falster_2003\"   \"Falster_2005_1\" \"Falster_2005_2\"\n\n# Then we extract\ndata_falster_studies &lt;- extract_dataset(austraits, dataset_ids)\n\ndata_falster_studies$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 12\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle 66.1 \n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp… simp…\n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle 71.7 \n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp… simp…\n#&gt; # ℹ 5 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;\n\n\n\n\nExtracting by taxonomic level\n\n# By family \nproteaceae &lt;- extract_taxa(austraits, family = \"Proteaceae\")\n# Checking that only taxa in Proteaceae have been extracted\nproteaceae$taxa$family %&gt;% unique()\n\n#&gt; [1] \"Proteaceae\"\n\n# By genus \nacacia &lt;- extract_taxa(austraits, genus = \"Acacia\")\n# Checking that only taxa in Acacia have been extracted\nacacia$traits$taxon_name %&gt;% unique() %&gt;% head()\n\n#&gt; [1] \"Acacia myrtifolia\" \"Acacia suaveolens\" \"Acacia floribunda\"\n#&gt; [4] \"Acacia celsa\"      \"Acacia longifolia\" \"Acacia terminalis\"\n\n\n\n\nExtracting by trait\nFiltering one particular trait and assigning it to an object\n\ndata_wood_dens &lt;- extract_trait(austraits, \"wood_density\")\n\nhead(data_wood_dens$traits)\n\n#&gt; # A tibble: 6 × 12\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_200… Acacia ce… Atherton  &lt;NA&gt;         Falster_2005_… wood_dens… 0.498\n#&gt; 2 Falster_200… Acronychi… Atherton  &lt;NA&gt;         Falster_2005_… wood_dens… 0.525\n#&gt; 3 Falster_200… Alphitoni… Atherton  &lt;NA&gt;         Falster_2005_… wood_dens… 0.413\n#&gt; 4 Falster_200… Glochidio… Atherton  &lt;NA&gt;         Falster_2005_… wood_dens… 0.566\n#&gt; 5 Falster_200… Homalanth… Atherton  &lt;NA&gt;         Falster_2005_… wood_dens… 0.319\n#&gt; 6 Falster_200… Melicope … Atherton  &lt;NA&gt;         Falster_2005_… wood_dens… 0.346\n#&gt; # ℹ 5 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;\n\n\nCombining lookup_trait with extract_trait to obtain all traits with ‘leaf’ in the trait name and assigning it to an object. Note we use the . notation to pass on the lookup_trait results to extract_trait\n\ndata_leaf &lt;- lookup_trait(austraits, \"leaf\") %&gt;% extract_trait(austraits, .) \n\nhead(data_leaf$traits)\n\n#&gt; # A tibble: 6 × 12\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  66.1\n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  71.7\n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; # ℹ 5 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;"
  },
  {
    "objectID": "austraits_package.html#join-data-from-other-tables-and-elements",
    "href": "austraits_package.html#join-data-from-other-tables-and-elements",
    "title": "29  The austraits package",
    "section": "29.4 Join data from other tables and elements",
    "text": "29.4 Join data from other tables and elements\nOnce users have extracted the data they want, they may want to merge other study details into the main traits dataframe for their analyses. For example, users may require taxonomic information for a phylogenetic analysis. This is where the join_ functions come in. There are five join_ functions in total, each designed to append specific information from other tables and elements in the austraits object. Their suffixes refer to the type of information that is joined, e.g. join_taxonomy appends taxonomic information to the traits dataframe. See ?join_all for more details.\n\n# Join taxonomic information \n(data_leaf %&gt;% join_taxonomy)$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 16\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  66.1\n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  71.7\n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; # ℹ 9 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;,\n#&gt; #   taxonRank &lt;chr&gt;, acceptedNameUsageID &lt;chr&gt;\n\n# Join methodological information \n(data_leaf %&gt;% join_methods)$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 16\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  66.1\n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  71.7\n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; # ℹ 9 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;, methods &lt;chr&gt;,\n#&gt; #   year_collected_start &lt;chr&gt;, year_collected_end &lt;chr&gt;, collection_type &lt;chr&gt;\n\n# Join site based information \n(data_leaf %&gt;% join_sites)$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 14\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  66.1\n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  71.7\n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; # ℹ 7 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;, `latitude (deg)` &lt;chr&gt;,\n#&gt; #   `longitude (deg)` &lt;chr&gt;\n\n# Join context information \n(data_leaf %&gt;% join_contexts)$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 18\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  66.1\n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  71.7\n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; # ℹ 11 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;, type &lt;chr&gt;, description &lt;chr&gt;,\n#&gt; #   leaf_parts &lt;chr&gt;, plant_age &lt;chr&gt;, `plant age` &lt;chr&gt;, season &lt;chr&gt;\n\n# Alternatively users can join *all* information \n(data_leaf %&gt;% join_all)$traits %&gt;% head()\n\n#&gt; # A tibble: 6 × 22\n#&gt;   dataset_id   taxon_name site_name context_name observation_id trait_name value\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  66.1\n#&gt; 2 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt; 3 Falster_2003 Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; 4 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle  71.7\n#&gt; 5 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt; 6 Falster_2003 Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp…  NA  \n#&gt; # ℹ 15 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;, `latitude (deg)` &lt;chr&gt;,\n#&gt; #   `longitude (deg)` &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, taxonRank &lt;chr&gt;,\n#&gt; #   acceptedNameUsageID &lt;chr&gt;, methods &lt;chr&gt;, year_collected_start &lt;chr&gt;,\n#&gt; #   year_collected_end &lt;chr&gt;, collection_type &lt;chr&gt;"
  },
  {
    "objectID": "austraits_package.html#visualising-data-by-site",
    "href": "austraits_package.html#visualising-data-by-site",
    "title": "29  The austraits package",
    "section": "29.5 Visualising data by site",
    "text": "29.5 Visualising data by site\nplot_site_locations graphically summarises where trait data was collected from and how much data is available. The legend refers to the number of neighbouring points: the warmer the colour, the more data that is available. This function only works for studies that are geo-referenced. Users must first use join_sites to append latitude and longitude information into the trait dataframe before plotting\n\ndata_wood_dens &lt;- data_wood_dens %&gt;% join_sites()\nplot_site_locations(data_wood_dens$traits)"
  },
  {
    "objectID": "austraits_package.html#visualising-data-distribution-and-variance",
    "href": "austraits_package.html#visualising-data-distribution-and-variance",
    "title": "29  The austraits package",
    "section": "29.6 Visualising data distribution and variance",
    "text": "29.6 Visualising data distribution and variance\nplot_trait_distribution creates histograms and beeswarm plots for specific traits to help users visualise the variance of the data. Users can specify whether to create separate beeswarm plots at the level of taxonomic family or for each dataset_id\n\naustraits %&gt;% plot_trait_distribution_beeswarm(\"wood_density\", \"family\")\n\n\n\n\n\n\n\naustraits %&gt;% plot_trait_distribution_beeswarm(\"wood_density\", \"dataset_id\")"
  },
  {
    "objectID": "austraits_package.html#pivotting-from-long-to-wide-format",
    "href": "austraits_package.html#pivotting-from-long-to-wide-format",
    "title": "29  The austraits package",
    "section": "29.7 Pivotting from long to wide format",
    "text": "29.7 Pivotting from long to wide format\nThe table of traits in AusTraits comes in long format, where data for all trait information are denoted by two columns called trait_name and value. You can convert this to wide format, where each trait is in a separate column, using the function trait_pivot_wider.\nThe function operates differently depending on the version of AusTraits\n\nPivot wider &lt;=3.0.2\nIn AusTraits &lt;=3.0.2, some studies will have multiple observations for some traits, e.g. huber values. This can prevent pivoting from long format to wide. There are two ways to collapse multiple observations. The first option bind_trait_values concatenates the observations into a single string, thereby retaining all information.\nAlternatively, if you don’t want to bind the trait values, you can give summarise_trait_means a go, which computes means with multiple observations. Note that this method condenses the traits table and you won’t be able to revert it back unless you load_austraits again\n\nbind_trait_values\n\ndata_wide_bound &lt;- data_falster_studies$traits %&gt;%\n  bind_trait_values() %&gt;% # Joining multiple obs with `--`\n  trait_pivot_wider() #Pivot wide\n\ndata_wide_bound$value # The trait values table\n\n#&gt; # A tibble: 103 × 17\n#&gt;    dataset_id   taxon_name      site_name context_name observation_id leaf_angle\n#&gt;    &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;     \n#&gt;  1 Falster_2003 Acacia myrtifo… Ku-ring-… &lt;NA&gt;         Falster_2003_… 66.1      \n#&gt;  2 Falster_2003 Acacia suaveol… Ku-ring-… &lt;NA&gt;         Falster_2003_… 71.7      \n#&gt;  3 Falster_2003 Angophora hisp… Ku-ring-… &lt;NA&gt;         Falster_2003_… 50.8      \n#&gt;  4 Falster_2003 Banksia margin… Ku-ring-… &lt;NA&gt;         Falster_2003_… 53.1      \n#&gt;  5 Falster_2003 Banksia oblong… Ku-ring-… &lt;NA&gt;         Falster_2003_… 45        \n#&gt;  6 Falster_2003 Boronia pinnata Ku-ring-… &lt;NA&gt;         Falster_2003_… 43.9      \n#&gt;  7 Falster_2003 Conospermum lo… Ku-ring-… &lt;NA&gt;         Falster_2003_… 72.3      \n#&gt;  8 Falster_2003 Epacris pulche… Ku-ring-… &lt;NA&gt;         Falster_2003_… 42.9      \n#&gt;  9 Falster_2003 Eriostemon aus… Ku-ring-… &lt;NA&gt;         Falster_2003_… 62.1      \n#&gt; 10 Falster_2003 Corymbia gummi… Ku-ring-… &lt;NA&gt;         Falster_2003_… 59.3      \n#&gt; # ℹ 93 more rows\n#&gt; # ℹ 11 more variables: leaf_area &lt;chr&gt;, leaf_compoundness &lt;chr&gt;,\n#&gt; #   branch_mass_fraction &lt;chr&gt;, huber_value &lt;chr&gt;, leaf_N_per_dry_mass &lt;chr&gt;,\n#&gt; #   seed_mass &lt;chr&gt;, specific_leaf_area &lt;chr&gt;, wood_density &lt;chr&gt;,\n#&gt; #   fire_response &lt;chr&gt;, plant_height &lt;chr&gt;, original_name &lt;chr&gt;\n\n# Check out the 'bounded' trait values\ndata_wide_bound$value %&gt;% \n  select(-c(1:5)) %&gt;% #Excluding values that are not traits so we can see which columns contains bounded values\n  filter_all(.vars_predicate = any_vars(str_detect(., \"--\"))) \n\n#&gt; # A tibble: 55 × 12\n#&gt;    leaf_angle leaf_area leaf_compoundness branch_mass_fraction huber_value      \n#&gt;    &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;             &lt;chr&gt;                &lt;chr&gt;            \n#&gt;  1 &lt;NA&gt;       2786      &lt;NA&gt;              0.67                 0.00033590863285…\n#&gt;  2 &lt;NA&gt;       14302     &lt;NA&gt;              0.48                 0.00013227513227…\n#&gt;  3 &lt;NA&gt;       6820      &lt;NA&gt;              0.42                 0.00023041474654…\n#&gt;  4 &lt;NA&gt;       3209      &lt;NA&gt;              0.41                 0.00050175614651…\n#&gt;  5 &lt;NA&gt;       10682     &lt;NA&gt;              0.47                 0.00046816479400…\n#&gt;  6 &lt;NA&gt;       6955      &lt;NA&gt;              0.58                 0.00023803856224…\n#&gt;  7 &lt;NA&gt;       5228      &lt;NA&gt;              0.58                 0.00059311981020…\n#&gt;  8 &lt;NA&gt;       6806      &lt;NA&gt;              0.42                 0.00016644474034…\n#&gt;  9 &lt;NA&gt;       11157     &lt;NA&gt;              0                    0.00020092425155…\n#&gt; 10 &lt;NA&gt;       3401      &lt;NA&gt;              0.34                 0.00023142791020…\n#&gt; # ℹ 45 more rows\n#&gt; # ℹ 7 more variables: leaf_N_per_dry_mass &lt;chr&gt;, seed_mass &lt;chr&gt;,\n#&gt; #   specific_leaf_area &lt;chr&gt;, wood_density &lt;chr&gt;, fire_response &lt;chr&gt;,\n#&gt; #   plant_height &lt;chr&gt;, original_name &lt;chr&gt;\n\n\nIf you would like to revert the bounded trait values, you have to use trait_pivot_longer first, then call separate_trait_values:\n\ndata_wide_bound %&gt;% \n  trait_pivot_longer() %&gt;% \n  separate_trait_values(., austraits$definitions)\n\n#&gt; # A tibble: 691 × 12\n#&gt;    dataset_id  taxon_name site_name context_name observation_id trait_name value\n#&gt;    &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;\n#&gt;  1 Falster_20… Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle 66.1 \n#&gt;  2 Falster_20… Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  319  \n#&gt;  3 Falster_20… Acacia my… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp… simp…\n#&gt;  4 Falster_20… Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle 71.7 \n#&gt;  5 Falster_20… Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  562  \n#&gt;  6 Falster_20… Acacia su… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp… simp…\n#&gt;  7 Falster_20… Angophora… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle 50.8 \n#&gt;  8 Falster_20… Angophora… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_area  1590 \n#&gt;  9 Falster_20… Angophora… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_comp… simp…\n#&gt; 10 Falster_20… Banksia m… Ku-ring-… &lt;NA&gt;         Falster_2003_… leaf_angle 53.1 \n#&gt; # ℹ 681 more rows\n#&gt; # ℹ 5 more variables: unit &lt;chr&gt;, date &lt;chr&gt;, value_type &lt;fct&gt;,\n#&gt; #   replicates &lt;chr&gt;, original_name &lt;chr&gt;\n\n\n\n\nsummarise_trait_means\n\ndata_wide_summarised &lt;- data_falster_studies$traits %&gt;%\n  summarise_trait_means() %&gt;% \n  trait_pivot_wider()\n\ndata_wide_summarised$value %&gt;% head()\n\n#&gt; # A tibble: 6 × 17\n#&gt;   dataset_id   taxon_name       site_name context_name observation_id leaf_angle\n#&gt;   &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1 Falster_2003 Acacia myrtifol… Ku-ring-… &lt;NA&gt;         Falster_2003_…       66.1\n#&gt; 2 Falster_2003 Acacia suaveole… Ku-ring-… &lt;NA&gt;         Falster_2003_…       71.7\n#&gt; 3 Falster_2003 Angophora hispi… Ku-ring-… &lt;NA&gt;         Falster_2003_…       50.8\n#&gt; 4 Falster_2003 Banksia margina… Ku-ring-… &lt;NA&gt;         Falster_2003_…       53.1\n#&gt; 5 Falster_2003 Banksia oblongi… Ku-ring-… &lt;NA&gt;         Falster_2003_…       45  \n#&gt; 6 Falster_2003 Boronia pinnata  Ku-ring-… &lt;NA&gt;         Falster_2003_…       43.9\n#&gt; # ℹ 11 more variables: leaf_area &lt;dbl&gt;, leaf_compoundness &lt;dbl&gt;,\n#&gt; #   branch_mass_fraction &lt;dbl&gt;, leaf_N_per_dry_mass &lt;dbl&gt;, seed_mass &lt;dbl&gt;,\n#&gt; #   specific_leaf_area &lt;dbl&gt;, wood_density &lt;dbl&gt;, huber_value &lt;dbl&gt;,\n#&gt; #   fire_response &lt;dbl&gt;, plant_height &lt;dbl&gt;, original_name &lt;chr&gt;"
  },
  {
    "objectID": "help.html#before-you-post",
    "href": "help.html#before-you-post",
    "title": "30  Getting help",
    "section": "30.1 Before you post",
    "text": "30.1 Before you post\n\nCode of conduct\nPlease note that the traits.build project adheres to a Contributor Code of Conduct, as specified in austraits.build. By contributing to this project you agree to abide by its terms.\n\n\nSearch existing issues\nPlease check if your question already has an answer. You can search the [GitHub Repositories:\n\ntraits.build-book\ntraits.build\naustraits.build\n\n\n\nTry troubleshooting\nFor specific errors or other issues, please read this chapter’s section on troubleshooting. Please try to work through the steps yourself before posting a question."
  },
  {
    "objectID": "help.html#troubleshooting",
    "href": "help.html#troubleshooting",
    "title": "30  Getting help",
    "section": "30.2 Troubleshooting",
    "text": "30.2 Troubleshooting\nIt is okay to reach out if you are struggling to solve a specific problem in a specific project: an error message, a part of the code you are not sure how to write, or any experience with traits.build that is incorrect, unwelcome, unexpected, or confusing. However, please follow the guidelines below and take an active role in the troubleshooting process.\n\nUpdate your R packages\nIf the error is a bug in traits.build or austraits, it is possible the bug has already been fixed in a newer version. Before posting, please try again with the latest CRAN release of traits.build (or austraits), then again with the GitHub development version if needed. Please see http://traitecoevo.github.io/traits.build/#installation for installation instructions.\n\n\nAttribute the error\nThe traits.build package itself is often not usually the cause of problems that arise in traits.build pipelines. Most issues come from the user-defined R code, or data files that the pipeline calls, as well as other R packages on your system. So before you post a question, please attempt to troubleshoot and figure out if traits.build is actually the source of the trouble, or if the error comes from another package or your own code. The tips in the debugging chapter may help. If the culprit turns out to be a non-traits.build issue, then please ask your question in a non-traits.build forum and write the question accordingly.\n\n\nAttribute the error\n\n\nWrite a reprex\nTo set up the discussion for success, please provide the complete context of the problem, including a reprex. The purpose of a reprex, or reproducible example1, is to eliminate the knowledge gaps, misunderstandings, and hidden assumptions where bugs hide. A reprex is a sample of complete, self-contained, runnable code that fully emulates and reproduces the problem. The code should look clean and readable, be as short and concise as possible, run in as few seconds as possible, and contain only the details most relevant to troubleshooting. You can embed the code inline in your question, or you can upload it to a public repository and post the link. Regardless, please expect that anyone trying to help will read all the code and run the enclosed _targets.R file on their own private computer. This process is hands-on and empirical, so please make it as quick and easy as possible for the people who volunteer their valuable time and energy to answer questions.\nThe following posts explain how to write a good reprex.\n\nhttps://stackoverflow.com/help/minimal-reproducible-example\nhttps://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example\nhttps://reprex.tidyverse.org/\nhttps://www.tidyverse.org/blog/2017/12/workflow-vs-script/"
  },
  {
    "objectID": "help.html#sec-contact",
    "href": "help.html#sec-contact",
    "title": "30  Getting help",
    "section": "30.3 Contact",
    "text": "30.3 Contact\nThere are many ways to reach out.\n\nMaintainer\nTo contact the maintainer directly, please post to the relevant public GitHub Discussions page of the package.2 Examples:\n\ntraits.build: https://github.com/ropensci/targets/discussions\naustraits: https://github.com/ropensci/tarchetypes/discussions\n\nGitHub makes it easy to search for and link to public discussions. Not only does this help users solve their own problems, it also helps the maintainer avoid repetition. So please use discussions instead of private emails, instant messages, or mentions on social media."
  },
  {
    "objectID": "help.html#acknowledgements-and-copyright",
    "href": "help.html#acknowledgements-and-copyright",
    "title": "30  Getting help",
    "section": "30.4 Acknowledgements and copyright",
    "text": "30.4 Acknowledgements and copyright\nThis page was adapted from a corresponding file for the targets package, with text by Will Landau. The original file is available at https://github.com/ropensci-books/targets/blob/main/help.qmd. Text adapted from that page is under copy right specified in that package https://github.com/ropensci-books/targets/blob/main/LICENSE.md."
  },
  {
    "objectID": "help.html#footnotes",
    "href": "help.html#footnotes",
    "title": "30  Getting help",
    "section": "",
    "text": "Also known as a minimal reproducible example or minimal working example.↩︎\nYou may need to create a free GitHub account, but the process is straightforward.↩︎"
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "31  Debugging",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "csv.html",
    "href": "csv.html",
    "title": "Appendix A — CSV files",
    "section": "",
    "text": "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. This is a comma format for storing tables of data in a simple text file. You can edit it in Excel or in a text editor. For more, see here."
  },
  {
    "objectID": "yaml.html",
    "href": "yaml.html",
    "title": "Appendix B — Yaml files",
    "section": "",
    "text": "The yml file extension (pronounced “YAML”) is a type structured data file, that is both human and machine readable. You can edit it in any text editor, or also in Rstudio. Generally, yml is used in situations where a table is not suitable because of variable lengths and or nested structures. It has the advantage over a spreadsheet in that the nested “headers” can have variable numbers of categories. The data under each of the hierarchical headings are easily extracted by R."
  }
]